{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9f9c61fd50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "# which makes intuitive sense.\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.io import read_image\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch is using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_fname_space(path):\n",
    "    for filename in os.listdir(path):\n",
    "        my_source = path + \"/\" + filename\n",
    "        my_dest = path + \"/\" + filename.strip().replace(\" \", \"\")\n",
    "        os.rename(my_source, my_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr_real = '../../data/CIFAK/train/REAL'\n",
    "# remove_fname_space(path_tr_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_tr_real = os.listdir(path_tr_real)\n",
    "fname_tr_real.sort()\n",
    "labels_lst=[1]*len(fname_tr_real)\n",
    "# print(len(labels_lst))\n",
    "# print(len(fname_tr_real))\n",
    "tr_dict={'Image_name':fname_tr_real,'True?':labels_lst}\n",
    "tr_df=pd.DataFrame(tr_dict)\n",
    "tr_df.to_csv(path_tr_real+\"/tr_annotation.csv\")\n",
    "\n",
    "fpath_tr_real = []\n",
    "for i, file in enumerate(fname_tr_real):\n",
    "    fpath_tr_real.append(path_tr_real+\"/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = cv2.imread(fpath_tr_real[0]).shape[0]\n",
    "\n",
    "horizontal_slides= 6\n",
    "total_img_slices = horizontal_slides**2\n",
    "num_loader_images = 16\n",
    "\n",
    "slice_width = img_size//horizontal_slides\n",
    "slice_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # img: is a tensor of the shape (Color_Channels x Rows (Hight) x Columns (Width))\n",
    "        #\n",
    "        # Make a slice every \"slice_width\" as we are moving across dimension 1 (as we are moving\n",
    "        # vertically across rows)\n",
    "        img = img.unfold(1, self.slice_width, self.slice_width)\n",
    "        # Make a slice every slice_width as we are moving across dimension 2,\n",
    "        # Note that previous operation has added new dimension at the beginning\n",
    "        # refers to no. of vertical slices, hence 2 here still refers to the rows.\n",
    "        img = img.unfold(2, self.slice_width, self.slice_width)\n",
    "        return img\n",
    "\n",
    "    def plot(self, img):\n",
    "        img = self.slice(img).permute(1, 2, 0, 3, 4)\n",
    "        print(img.size())\n",
    "\n",
    "        fig = plt.figure(figsize=(2, 2))\n",
    "        grid = ImageGrid(fig, 111, nrows_ncols=(\n",
    "            img.size(0), img.size(1)), axes_pad=0.1)\n",
    "\n",
    "        for i, ax in enumerate(grid):\n",
    "            i_b4 = str(np.base_repr(i, img.size(0))).zfill(2)\n",
    "            row = int(i_b4[0])\n",
    "            column = int(i_b4[1])\n",
    "            patch = img[row][column].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(patch)\n",
    "            ax.axis('off')\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.slice(img)\n",
    "        channels = img.size(0)\n",
    "\n",
    "        return img.reshape(-1, self.slice_width * self.slice_width * channels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by slicing each image [3,32,23] ==> [16,3,8,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, images, annotations_file, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = read_image(self.images[index])\n",
    "\n",
    "        # img.to(device)\n",
    "\n",
    "        labels = self.labels.iloc[index, 2]\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((32, 32)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "tr_annotation_file = path_tr_real+\"/tr_annotation.csv\"\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(fpath_tr_real, tr_annotation_file, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=num_loader_images,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)  # increase number of processor cores loading the data and getting it ready for model training inside the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 31.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARKUlEQVR4nO2dWY8kSVaFzffYMjNyj6rqqu7sYgqJgR5BIxBCI/gVSGhe+D/8A/4J88AqwRMSGoae7lHDdG25VOUSS8bm4QvPZucgfHiBqz7fm1+Ze3h43nTZiXvtWNS2beuEMEb8f30DQvxvUOIKkyhxhUmUuMIkSlxhEiWuMIkSV5hEiStMosQVJkm7DvzJX34HsSzD07Ne4n9AFuHF4hpDCRkW4fWTJPOOqx1eP4rwYkmD/6NRhEXDJMJ7i6LGvy9XwZgmwfsoowJiVYc6ZZLg/adpDjFW9NztdviZVRUcNzAmTcmzjjHWNHhuXeMzC+8tikgeEP7qL/Y7jdMbV5hEiStMosQVJlHiCpN0Fmd5nkEszjDv0yAWJSggoo7/LrVjk35fuMQZua8WxU1MxBOD3VoU++dGDr9THOP1Y4exPMZPiEkspG1RdLVEKCUxEZy5f/1+gULPEaHXNPj8qwaFqYvwPuLE/0z2fLoKNobeuMIkSlxhEiWuMIkSV5ikuzjr44Q+IYInzYKKSYoTd1bZakgVyxHREgX/azG5lmvZeTguIkImapmI8L8DE0D4LZ2LSEUpTfH6YaGMVaeaHYqiqCUVMFp18//MVbXtdH1WEUuZCKXC1z+XXYt9T+f2SAzRG1eYRIkrTKLEFSbpPMdNSUcX/7U+7AoipyVk3kvmTuT3e7h+TH/EJvMwOndlHWOskBDMaUkrG4xxzmWkiyyOMJa4cA4NQ1yEdRb6eGIy/05j/zN3Nekgczjvbcm18gRThhWU1suVd7ycz2HMavWIJ7qnJIbojStMosQVJlHiCpMocYVJOouzLGM/FpOuoFCckTEtEUAR65Bi6iMQWUzo0QIEKXDwTjD2meGJHQScc65gBZSWLA0KYklKuu6IIEzYze5IR13Q0dXWaxjT7FYQqyvSkUbuoyEdY5v51Dte3t3DmNWaibMfkxiiN64wiRJXmESJK0yixBUm6SzORgUKEuplHnRStaQ7LE5ZpxZeqiHiDBqKWEWP3FdElB5fOsKWwwTnUR2J5xWkUytmWjJ4kKwDi/k9NLsSYts1iqxdELu7/QBj1gusbO1KrKb1iJdGSrrDquDcjFxrn1TmuqI3rjCJEleYRIkrTKLEFSbpLM7a1QxiDfE9gGUuRNRlES4DihPs24tiHNcE6qYhlShW2XKsYkXbGtnlAkXFjPGIUk1JRYl6AIYfSs4rtxuIPc6mnWLbpV+hmhJxtlou8MYqFH9u2IfQgPg0xIFYHZBXZM78HTqiN64wiRJXmESJK0yixBUm6SzO5tfvMMjMzoIr5gP8iHg4xNigB7EkQyEQBR9QxHgtVtHjK9NINY2O9MVYxMQZc1YgLYZpiiI0D1oFywrPK2dY2Zp/vMHYPbYPlquld7zXw3vIiFAKzfKcc25E/k4FqZztSr8lsiEu6ExDd0VvXGESJa4wiRJXmKTzHLeZ3UFsU+ISkHCjnIrMca8fccnGxQ9eQmyxw8lqGxQqJmefwpj1Bn/AT1PcASclptBs15qwqJISs+qyxB/rY7KEKOvjvH0dzAdvbq5gzMPtR4g5Yrw8JN/pYOz7ccU1zjd7zNaiJt+JeIztyPMOiygZ6ZTLaXthN/TGFSZR4gqTKHGFSZS4wiSdxdlRH4cuGpxc93r+uGvyI/nr1/+JN0LW+o9PziBWBoLtZoVdU+sV6wQjZm2kFaws8XqhzxtbvsKMi6MEBeGoTwomQfFiNcVOvJh0ahVsS1pSCNltw3PJzjzk/qlXBBF/UYbCq6r8WFsTAUeKEl3RG1eYRIkrTKLEFSZR4gqTdN91p0FxkDVYZeoHFZLpB+wqm968hdj9EIXM6QHuwBJWo8rZLd5sjaKrKFAUZURo5AkRKcFyoYRUlDYbFHV1i9efL1F4bbe+58B2hd4IbNvUjHRqlWSJ0mblC99+jyyJIh4NzO09zfHcnIiz8Nk2XffB7YjeuMIkSlxhEiWuMIkSV5ikszhbLR4g1pKtNdcLXwisiXg6HZOlOzUKkg35zPHRsXfMvBHWaxSN9QqXtGxIC+OOiJSD/YF3vDfexzGkXbFu8PFOSVVsu/U9DfoZfqfBAMVrQcaxd9F+3xdKq8cljKlJi2RJtpVKiVFdkpJlV4FIZ22N3HSwG3rjCpMocYVJlLjCJEpcYZLO4my7wXX9DufbbjYLzNNabGd7efEJxKbEEfvtd99ALE9/0zuuiABqyb69OTHVS8heu1WNwmUXVJ4WDRrEsTVnrsV7+0C2TQrPPTo6gjEZETItaXVkFbai54unXU7W8pEWxpRU/jLmyUBiofBqwEreuaZVW6P4nqHEFSZR4gqTKHGFSTqLs/UaRQsxDHfrYH/W/QOsqhyfjCFWNrjm7A1xzl4sfGOSKMKKUkKEWJpjZYtV/uotmpXs2kCFkn2s1mSbpo8fUYjtSqxQhS2AmyWKloXDe2W0xLQvfB5pjn8TsnzQJSkxwiOW6jUxP6wqX/gyoxUm2LqiN64wiRJXmESJK0zSeY673OAc1+3I9qHBdHB8NIYx9/c4d3WkGBD6GTjn3ENgvjc+OIYxC7LzzHyKRYOHOzSSq0l32MVnz73jV7/xGYx5ej7Gz3xAT4knpycQC02Vl0uc72+WOF8e7mGXGpuXLgPd8fEevzdpDnMR+QNkBdEUGX5mFFSnonBfWedcmnZOP0BvXGESJa4wiRJXmESJK0zS3ZE8QtFSEi+BgyPfC6EosIXsF7/8JcT29kcQq2v80f3y6o1//O49jAmXjTjnXEFES69PXLL38D7Gh35sOCJLVUjH1a7CosT+GMVN2En1/v01jFlviGgcfg6xosDiy2Mg9i7f/QfeK9khqCLdW2zrWlbQ6AdLmQ4OD2HM/j6Ky67ojStMosQVJlHiCpMocYVJOouztMCJ+u0DeiY8rv0KT02M8ZZkmc54Hw3unj97CrGvvvnaOy7XKOAmkwnEGtJ9Np2ib8PxMVbiBnu+sFtXKEp/9RaN/OIMn9nby19BLBSTVzdvYMz1NQq2716jyB0MUFwen557x02NVcQ1MdqbzdADYrPF5x0KMeecOznxK4T95BzGlE7iTHzPUOIKkyhxhUmUuMIk3StnxABtRSb0i/nUO15vUcgM+wOIHR6il0BO2t6Wc19YzB6mMObq6hJiF5+9gFiPOHPPZyjY/vqnP/WO37x+3elaMRGmJ6co/i4uLrzj4RBNAZngbMleuKwCtln5YviAVAePiJFfdvEMYqwVsYt5HVumU65R/HVFb1xhEiWuMIkSV5hEiStM0lmcnRydQixLsUWvDCorrNIyPjiA2G//1u9A7I74KtTBXr4bspfvxUts9/vzP/sJxFyMouIXX/07xL4J2jA/XOF9XXx6AbF90v54NMb2vlevXnnHrPrF9goOvQucc26zxvbHsH3w8RG9I3K2DRSJMZhnQmjkJ18FIZwSVxhFiStM0n2OO8Y57idP0KB5NPS7vMod8eeqcG5zfIAFiHKF5z6f+J/Zlnit7SPO8/71X34GscnkDGKDHH/8Pz/2v/v2xUsYMybLUKoS53WzKRZtri79LrvBAMew+SCb4243+JnbjT+OmT/vyHN8bLGjjhlYh1u6OofFEbb9LIt1RW9cYRIlrjCJEleYRIkrTNJZnH24RqO0JMEf8Hs9v/NrR8TZihQN3r7GH7uZ+DgMRNzRF9ht9e2330LsH/7uHyH25Ze/CzHmyfBvP/u5d3x8jEL1yRkK1b0B2fo1xkf+ySf+uaw7zDkUTxvia/HwgN1t4dKa5RINDJlQYs+CiUTWkRYWTJgglOmd+N6hxBUmUeIKkyhxhUk6z457KU7U2dKdMtg+lE3AE2KmVm1QxLEVIWeHvhhjlZzf/xGKrpsPVxCbnKDIur9Hr4g42JJmcoKCcNRDb4GY7BlbVShkZg/+EhbmZ9DWRBTV+N0f7tG5PAqE0Yvnz//HMc45F5M/QM3+KGQXolDYJRG5PsmpruiNK0yixBUmUeIKkyhxhUk6i7OXn+PSFCaMwkn5YIAeCqzixqpkbLlHWJFh12ImbB9vcesmNq4ovoDYn/7Jj71jJjjfv0PxVxR4/RUx6QtF0PyxmzjLCryPIsd3URO0GF6+R1+ImlTEGrZciMSYOAu3h0pjrMyxLaS6ojeuMIkSV5hEiStMosQVJum+l+8MK0ppTrYOSnyvhYZsmRTV+P9CWyQHpNUu9QXPiLQAMnFzRvYUTjLSahdjNacN7oNVgZ58ib4QbYPjSrLeLjT3m5H7T0nFarCHwndBTPs2wf7Ec2IUGJEqVpHi848zTJk0wlgViL1qS9bHVSi+u6I3rjCJEleYRIkrTNJ5jjubol/Wiiwd2R/5vlctmZulbBkH+TE9atC4+ODI9x27vcEiyPEZdn31cryP2wdcjlSRokoaeGgNeugJVtX4LMYHJxCbBcbXzjn35ibYUSfG7316gr4TqyX6HrQt2RWn53/3ZoTPv26xsLDb4vXrHc7Rh330Otvf8/9ODdl+drHUHFd8z1DiCpMocYVJlLjCJJ3FWV4QkZWhsfNk4i9rYebAzFj49hYLHLfE2Hl47xccBgMUSrM5Xot5BLCOtIgIo9AYOcvxe+cpntc6YhC3we1g68YXdgdke9heH5//43IKsdUKn+1o5Bcqnj0nQo8sw1otiTke6dirKhRxd3f+fTCjwy679fx36I0rTKLEFSZR4gqTKHGFSTqLs2fPnkCMmbOFIujqCpe0hGLBOed++MM/htj7y7cQ+/rrr73jy6t3MIZN+vfINqBs3HyO4in8nvM5dm8xM7gfffF7EBuRjrc48q/f76OgLcgGOMxFfEVcxJePvvB69QPc6jSJsfIXod50ux2+6zYbInKDStzwgGzDeoQisSt64wqTKHGFSZS4wiRKXGGSzuLs5gZ9CVjVaj73KyZfffVzGDOZTCA2PsTtltg2RHd3d94xq4jtk8rTixcvINaSVr7Ly2uI9ftFcIx+CZeXlxD7p39GF/QDsh3sduOLp/kjLr85O0Uh8+IzFFmTc9xydVP6wmuxuIMxj0sUnLMZCtX5At3MSyLOstzPjX4PUy2sGP466I0rTKLEFSZR4gqTKHGFSTqLswWZlLOticL/hX4fq2vTKU76//Zv/p585gJi4RZJrGLFKmJMaDQNMXBzeO5h4ILekooSa8s8PcY1Zw1xJN+s/Gf7cI/iqSDb3sbRU4idn6Fbepr7Anb5iM9iNEDBWZJtvR4e8N6qHRFnmf+ZC7LW7v4Bn1lX9MYVJlHiCpMocYVJlLjCJJ3F2YCIrKbBCX24J2xE/jdasi8ta5HMc2x/DKtpeU5M2Ihj+HyGa7HYOqseMfsIjfzYXrjLJV5rb0j2MSbnFoV//ZcXn8OY8SG2BZZbFE/X11jB29v3n20cEwO9Pn5vdzyGEBPDJORGI78SuiXVtbAK+uugN64wiRJXmESJK0zSeY57eYMeB6MRzrvyYHeY2SPO6fb2sHtr8hS36RyRH8qvr/3uLTY3ZvOw2WwKsekc56XFFgsEb975n1k36C0wGo4h9u4dzjefPcHOuPOJX6gYj/G5Dofo5bAmc3RWIMgCw78TsqVrqE2cc64mO/0w2Llh115RYBcfy4Ou6I0rTKLEFSZR4gqTKHGFSTqLs4o4UdcVtkkdnftC4+T4rNP1dyXZfpMYpZ2f+/4OLWnVYtuwHh7ikpaTE3QuZ4QFgvEYl9/80R/8IcSqEkXcpy+woysPhMvdPS6TWjOzvBoLHGwJzmrtn7u3x5ZJYTGDG9xhIaEsyY46W//cJEZjCFYo6oreuMIkSlxhEiWuMIkSV5gkapm6EeL/OXrjCpMocYVJlLjCJEpcYRIlrjCJEleYRIkrTKLEFSZR4gqT/BeNboQbJhnOygAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_img, tr_img_t, tr_labels = next(iter(Im_tr_loader))\n",
    "\n",
    "img_sample = tr_img[0]\n",
    "print(img_sample.size())\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img_sample.permute(1, 2, 0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6, 3, 5, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 72 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slice=SliceImage(slice_width)\n",
    "slice.plot(img_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_img_slices = 2\n",
    "# num_loader_patches = 4\n",
    "# flattened_size = 8\n",
    "# hidden_size = 5\n",
    "\n",
    "# inp = torch.rand(num_loader_patches, num_img_slices, flattened_size)\n",
    "# print(\"image: \", inp.size())\n",
    "# print(\"\")\n",
    "\n",
    "# projection = nn.Linear(flattened_size, hidden_size)\n",
    "# res = projection(inp)\n",
    "\n",
    "# print(\"image embedding: \", res.size())\n",
    "\n",
    "# class_token = nn.Parameter(torch.rand(1, hidden_size)\n",
    "#                            ).repeat(res.size(0), 1, 1)\n",
    "# print(\"cls embedding: \", class_token.size())\n",
    "# print(\"\")\n",
    "\n",
    "# res = torch.concat([class_token, res], dim=1)\n",
    "# print(\"cls embedding + image embedding: \", res.size())\n",
    "\n",
    "# position = nn.Parameter(torch.rand(\n",
    "#     1, num_img_slices + 1, hidden_size)).repeat(res.size(0), 1, 1)\n",
    "# print(\"Position embedding: \", position.size())\n",
    "# print(\"\")\n",
    "# final=res+position\n",
    "# print(\"cls embedding + image embedding + Position embedding: \", final.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1, slice_embed_size))\n",
    "        self.pos_to_embed = nn.Parameter(torch.rand(1, img_slices + 1, slice_embed_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        # \n",
    "        img_embedding = self.embed(flattened_img)\n",
    "\n",
    "        cls_embedding = self.cls_to_embed.repeat(img_embedding.size(0), 1, 1)\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "\n",
    "        position_embedding = self.pos_to_embed.repeat(\n",
    "            img_embedding.size(0), 1, 1)\n",
    "\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        return self.dropout(img_and_pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):  \n",
    "  \n",
    "    def __init__(self, size): # size is hidden size \n",
    "        super(AttentionHead, self).__init__()  \n",
    "  \n",
    "        self.query = nn.Linear(size, size)  \n",
    "        self.key = nn.Linear(size, size)  \n",
    "        self.value = nn.Linear(size, size)  \n",
    "  \n",
    "    def forward(self, input_tensor):  \n",
    "        q, k, v = self.query(input_tensor), self.key(input_tensor), self.value(input_tensor)  \n",
    "  \n",
    "        scale = q.size(1) ** 0.5  \n",
    "        scores = torch.bmm(q, k.transpose(1, 2)) / scale  \n",
    "  \n",
    "        scores = F.softmax(scores, dim=-1)  \n",
    "   \n",
    "        output = torch.bmm(scores, v)  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(size)\n",
    "                                   for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(size * num_heads, size)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        s = [head(input_tensor) for head in self.heads]\n",
    "        s = torch.cat(s, dim=-1)\n",
    "\n",
    "        output = self.linear(s)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  \n",
    "  \n",
    "    def __init__(self, size: int, num_heads: int, dropout: float = 0.1):  \n",
    "        super().__init__()  \n",
    "  \n",
    "        self.attention = MultiHeadAttention(size, num_heads)  \n",
    "        self.feed_forward = nn.Sequential(  \n",
    "            nn.Linear(size, 4 * size),  \n",
    "            nn.Dropout(dropout),  \n",
    "            nn.GELU(),  \n",
    "            nn.Linear(4 * size, size),  \n",
    "            nn.Dropout(dropout)  \n",
    "        )  \n",
    "        self.norm_attention = nn.LayerNorm(size)  \n",
    "        self.norm_feed_forward = nn.LayerNorm(size)  \n",
    "  \n",
    "    def forward(self, input_tensor):  \n",
    "        attn = input_tensor + self.attention(self.norm_attention(input_tensor))  \n",
    "        output = attn + self.feed_forward(self.norm_feed_forward(attn))  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1,\n",
    "                 lr=1e-4, min_lr=4e-5,\n",
    "                 weight_decay=0.1,\n",
    "                 epochs=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, num_heads, dropout=enc_dropout) for _ in range(num_encoders)],)\n",
    "        self.mlp_head = nn.Linear(slice_embed_size, num_classes)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        output = self.mlp_head(attn[:, 0, :])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1= torch.tensor([[[1,2],[3,4],[5,6]]])\n",
    "# print(a1.size())\n",
    "# a2= torch.tensor([[[10,20],[30,40]]])\n",
    "# torch.bmm(a1,a2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
