{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from dataset_process import dataset_to_df, search_df\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/CIFAK'\n",
    "relative_paths = [\"/train/REAL\", \"/train/FAKE\", \"/test/REAL\", \"/test/FAKE\"]\n",
    "paths_classes = [\"REAL\", \"FAKE\", \"REAL\", \"FAKE\"]\n",
    "\n",
    "# path = '../../data/meso_data'\n",
    "# relative_paths= [\"/Real\", \"/DeepFake\"]\n",
    "# paths_classes=['REAL',\"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all, df_train, df_val, df_test, classes_stats = dataset_to_df(\n",
    "    path, relative_paths, paths_classes, 0.8, 0.19, 0.01)\n",
    "\n",
    "classes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First and Last Elements in the Whole dataset\")\n",
    "df_all.iloc[[0,-1]]\n",
    "# print(df_all.iloc[[0,-1]].to_markdown(headers='keys', tablefmt='psql'))\n",
    "# print(\"\")\n",
    "# print(\"First and Last Elements in the Training dataset\")\n",
    "# print(df_train.iloc[[0,-1]].to_markdown(headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_horizontal_slices= 4\n",
    "images_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = read_image(df_all.iloc[0, 0]).size()\n",
    "\n",
    "slice_width = img_shape[1]//Img_horizontal_slices\n",
    "total_img_slices = Img_horizontal_slices**2\n",
    "\n",
    "print(f\"slice_width: {slice_width} pixels\")\n",
    "print(\"\")\n",
    "print(\"Image shape: \", img_shape)\n",
    "print(\n",
    "    f\"Image will be divided into: {Img_horizontal_slices} x {Img_horizontal_slices} = {total_img_slices} slices each with shape {(img_shape[0],slice_width,slice_width)}\")\n",
    "print(\n",
    "    f\"Target Shape of the final flattened image: {total_img_slices} x {img_shape[0]*slice_width**2} \")\n",
    "print(\"\")\n",
    "print(f\"Feed ({images_batch}) Images to the Dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # c: color channels\n",
    "\n",
    "        # h: desired slice height (pixels)\n",
    "        # w: desired slice width (pixels)\n",
    "\n",
    "        # row: No. of vertical slices\n",
    "        # col: No. of horizontal slices\n",
    "\n",
    "        # b: Batch of Images\n",
    "\n",
    "        sliced_Img = rearrange(\n",
    "            img, 'c (row h) (col w) -> row col c h w', h=slice_width, w=slice_width)\n",
    "\n",
    "        return sliced_Img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sliced_img = self.slice(img)\n",
    "        sliced_flattened_img = rearrange(\n",
    "            sliced_img, 'row col c h w -> (row col) (c h w)')\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((64, 64)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        \n",
    "        self.annotation = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 0])\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 3], dtype=torch.float64)\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(df_train, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "Im_val_dataset = Images_Dataset(df_val, data_transform)\n",
    "\n",
    "Im_val_loader = DataLoader(dataset=Im_val_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original Image and the slices Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(Img_train_loader, index):\n",
    "\n",
    "    img, img_t, label = Img_train_loader.dataset[index]\n",
    "\n",
    "    channels, img_size,_ = img.size()\n",
    "\n",
    "    img_slices, slice_flatsize = img_t.size()\n",
    "\n",
    "    imgs_per_batch = Im_tr_loader.batch_size\n",
    "\n",
    "    slice_width = int(np.sqrt(slice_flatsize/channels))\n",
    "\n",
    "    return img, img_t, label, channels, img_size, slice_width, img_slices, slice_flatsize, imgs_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img)\n",
    "    print(\"Sliced Image Shape: \", sliced_img.shape)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(8, 4))\n",
    "    subfigs = fig.subfigures(3, 1, height_ratios=[1., 1.5, 1.], hspace=0.05, squeeze='True')\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    axs0.imshow(rearrange(img, \"c h w -> h w c\"))\n",
    "    axs0.axis('off')\n",
    "    # subfigs[0].suptitle('Input Image', fontsize=10)\n",
    "\n",
    "    grid1 = ImageGrid(subfigs[1], 111, nrows_ncols=(\n",
    "        sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid1):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = rearrange(sliced_img[row][column], \"c h w -> h w c\").numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[1].suptitle('Slices', fontsize=10)\n",
    "\n",
    "    grid2 = ImageGrid(subfigs[2], 111, nrows_ncols=(\n",
    "        1, sliced_img.size(0)*sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid2):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[2].suptitle('Position', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_aaa=torch.tensor([[[0.501,0.601],[0.502,0.702]],[[0.511,0.611],[0.512,0.612]],[[0.521,0.621],[0.522,0.622]]])\n",
    "# cls_emb=torch.tensor([[[1,1]]])\n",
    "# cls_emb=cls_emb.repeat(3,1,1)\n",
    "# imgs_bbb=torch.concat([imgs_aaa, cls_emb], dim=1)\n",
    "# # pos=torch.tensor([[[0.3,0.3],[0.3,0.3],[0.3,0.3]],[[0.3,0.3],[0.3,0.3],[0.3,0.3]],[[0.3,0.3],[0.3,0.3],[0.3,0.3]]])\n",
    "# pos=torch.tensor([[[0.3,0.3],[0.3,0.3],[0.3,0.3]]])\n",
    "# imgs_ccc=imgs_bbb+pos\n",
    "\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Input image with cls: \\n\",imgs_bbb)\n",
    "# print(imgs_bbb.size())\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Position size: \",pos.size())\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Input image with cls + positional embedding shape: \\n\",imgs_ccc)\n",
    "# print(imgs_ccc.size())\n",
    "\n",
    "# input = torch.randn(3, 2,2)\n",
    "# print(input)\n",
    "# m = nn.Softmax(dim=2)\n",
    "# m(input)\n",
    "# print(imgs_aaa)\n",
    "# torch.transpose(imgs_aaa,1,2)\n",
    "\n",
    "# aa=torch.randn((3,4))\n",
    "# aaa=aa.repeat(2,1,1)\n",
    "# print(\"aa: \",aa,\"\\n\",aa.size(),\"\\n\")\n",
    "# print(\"aaa: \",aaa,\"\\n\",aaa.size(),\"\\n\")\n",
    "# bb=aa.unsqueeze(0)\n",
    "# bbb=bb.repeat(2,1,1)\n",
    "# print(\"bb: \",bb,\"\\n\",bb.size(),\"\\n\")\n",
    "# print(\"bbb: \",bbb,\"\\n\",bbb.size(),\"\\n\")\n",
    "# aaa==bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sample_img_batch,_ = next(iter(Im_tr_loader))\n",
    "\n",
    "print(f\"Batch of Images Shape: {sample_img_batch.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, sample_img_trans, sample_img_label, ch, img_size, slice_width, slices, slice_flatsize, batch_size = img_data(\n",
    "    Im_tr_loader, 2)\n",
    "print(f\"Input Image Shape: {sample_img.size()}\")\n",
    "img_plot(sample_img, slice_width)\n",
    "print(f\"Flattened Image Shape: {sample_img_trans.size()}\")\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"slice_flatsize\",slice_flatsize)\n",
    "slice_embed = slice_flatsize*4\n",
    "print(\"slice_embed\",slice_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the VIT Model : Embedding &rarr; Transformer Encoder &rarr; MLP_Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use FC layer to change the image flattened slice to embedding    \n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "\n",
    "        # cls: classification token is added to each image as an additional slice(patch),\n",
    "        # it has the same embedding size as any slice in the image.\n",
    "        # here I consider a single image, the batch size will be considered under the forward method\n",
    "        # note that it will be trainable that's why I added (requires grad)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1, slice_embed_size),requires_grad=True)\n",
    "\n",
    "        # positional embedding: is added to each slice in the image (including the cls token),\n",
    "        # it identifies the position of the slice (patch) inside the image.\n",
    "        # here I consider a single image, the batch size will be considered under the forward method\n",
    "        # note that it will be trainable that's why I added (requires grad)\n",
    "        self.pos_to_embed = nn.Parameter(torch.rand(img_slices + 1, slice_embed_size), requires_grad=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_imgs):\n",
    "\n",
    "        # b: is the number of images in the batch\n",
    "        batch_size = flattened_imgs.size(0)\n",
    "\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        #\n",
    "        img_embedding = self.img_to_embed(flattened_imgs)\n",
    "\n",
    "        # repeat cls for all the images in the batch\n",
    "        cls_embedding = self.cls_to_embed.repeat(batch_size, 1, 1)\n",
    "\n",
    "        # we can repeat position for all the images in the batch, but it's not necessary\n",
    "        # position_embedding = self.pos_to_embed.repeat(batch_size, 1, 1)\n",
    "        position_embedding = self.pos_to_embed\n",
    "\n",
    "        # Append the cls embedding to the beginning of the image slices\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        \n",
    "        embedding = self.dropout(img_and_pos_embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = ImageEmbedding(slice_input_size=slice_flatsize,\n",
    "                                 slice_embed_size=slice_embed,\n",
    "                                 img_slices=slices,\n",
    "                                 dropout_ratio=0.2)\n",
    "\n",
    "embedding_output = embedding_layer(sample_img_batch)\n",
    "\n",
    "print(\"Sample batch shape:  \", sample_img_batch.size())\n",
    "print(\"Label of a sample image: \",sample_img_label)\n",
    "print(\"\")\n",
    "print(\"Output Shape: \", embedding_output.size())\n",
    "print(\"Output Gradient Function: \",embedding_output.grad_fn)\n",
    "\n",
    "summary(model=embedding_layer,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "red { color: red }\n",
    "yellow { color: yellow }\n",
    "</style>\n",
    "\n",
    "##### **<yellow> Query: </yellow>** The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "\n",
    "##### **<yellow> Keys: </yellow>** For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "\n",
    "##### **<yellow> Values: </yellow>** For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "\n",
    "##### **<yellow> Score function: </yellow>** To rate which elements we want to pay attention to, we need to specify a score function . The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "##### A word asks the same question to all words in the sequence using the ‘query’ vector. Similarly, it provides the same answer to all words using the ‘key’ vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{Softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right) V\n",
    "$$\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, block_size, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.proj = nn.Linear(d_in, d_out)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(block_size, block_size), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv.unbind(0)\n",
    "\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(queries, keys, values, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA_layer = MHA(d_in=embedding_output.size(2),\n",
    "#                                  d_out=embedding_output.size(2),\n",
    "#                                  num_heads=12,\n",
    "#                                  block_size=embedding_output.size(1),\n",
    "#                                  dropout=0.0\n",
    "#                                  )\n",
    "\n",
    "# MHA_output = MHA_layer(embedding_output)\n",
    "\n",
    "# print(\"Input Shape:  \", embedding_output.size())\n",
    "# print(\"Output Shape: \", MHA_output.size())\n",
    "\n",
    "# summary(model=MHA_layer,\n",
    "#         input_size=embedding_output.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, context_len, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.0,\n",
    "            bias=False,\n",
    "            add_bias_kv=False,\n",
    "            batch_first=True)\n",
    "\n",
    "        # self.attention = MHA(\n",
    "        #     d_in=embed_size,\n",
    "        #     d_out=embed_size,\n",
    "        #     block_size=context_len,\n",
    "        #     dropout=0.0,\n",
    "        #     num_heads=num_heads,\n",
    "        #     qkv_bias=False\n",
    "        # )\n",
    "        \n",
    "        # Below I selected the hidden size of the feed forward = 4 * embed size\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, input):\n",
    "        normalized_input = self.norm1(input)\n",
    "\n",
    "        attn = self.attention(normalized_input)\n",
    "\n",
    "        attn = input + attn\n",
    "\n",
    "        normalized_attn = self.norm2(attn)\n",
    "        \n",
    "        output = attn + self.feed_forward(normalized_attn)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = Encoder(embed_size=slice_flatsize, num_heads=12, dropout=0.1)\n",
    "# encoder_output = encoder_layer(embedding_output)\n",
    "\n",
    "# print(\"Output Shape: \", encoder_output.size())\n",
    "\n",
    "# summary(model=encoder_layer,\n",
    "#         input_size=embedding_output.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders=1,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, img_slices, num_heads, dropout=enc_dropout) for _ in range(num_encoders)],)\n",
    "\n",
    "        self.mlp_head =  nn.Sequential(\n",
    "            nn.Linear(slice_embed_size, num_classes),\n",
    "            nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        # output = torch.round(self.sigmoid(self.mlp_head(attn[:, 0, :])))\n",
    "        output = self.mlp_head(attn[:, 0])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model Hyper-parameters Hereunder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = VIT(slice_input_size=slice_flatsize,\n",
    "                slice_embed_size=slice_embed,\n",
    "                img_slices=slices,\n",
    "                num_classes=1,\n",
    "                num_heads=12,\n",
    "                num_encoders=2,\n",
    "                emb_dropout=0.1,\n",
    "                enc_dropout=0.1)\n",
    "\n",
    "vit_output = vit_model(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape: \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=vit_model,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_epochs = 50         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "vit_optimizer = Adam(vit_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "criterion = CrossEntropyLoss() # returns the mean loss for the batch\n",
    "# scheduler = lr_scheduler.LinearLR(vit_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path=\"../weights/vit4\"\n",
    "\n",
    "# saved_model = vit_model()\n",
    "# saved_model.load_state_dict(torch.load(model_state_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_history = [1000]\n",
    "tr_accuracy_history = [0]\n",
    "val_loss_history = [1000]\n",
    "val_accuracy_history = [0]\n",
    "\n",
    "vit_model.to(device)\n",
    "\n",
    "for epoch in range(vit_epochs):\n",
    "\n",
    "    # Training Loop\n",
    "    vit_model.train(True)\n",
    "\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_tr_accuracy = 0\n",
    "    accumulated_val_loss = 0\n",
    "    accumulated_val_accuracy = 0\n",
    "\n",
    "    loop = tqdm(enumerate(Im_tr_loader), total=len(Im_tr_loader))\n",
    "    for batch, (_, inputs, targets) in loop:\n",
    "\n",
    "        # Put inputs and labels in device cuda\n",
    "        tr_images = inputs.to(device)\n",
    "        sample_img_labels = targets.to(device)\n",
    "\n",
    "        # Forward Path\n",
    "        tr_outputs = vit_model(tr_images).squeeze(1)\n",
    "\n",
    "        loss = criterion(tr_outputs,sample_img_labels)\n",
    "\n",
    "        accumulated_tr_loss += loss.detach().cpu().item()#loss.item()\n",
    "        accumulated_tr_accuracy += (sample_img_labels == tr_outputs).sum().item()/sample_img_labels.size(0)\n",
    "        \n",
    "        # avg_tr_loss = accumulated_tr_loss / (batch+1)\n",
    "        # avg_tr_accuracy = 100*accumulated_tr_accuracy / (batch+1)\n",
    "        avg_tr_loss = accumulated_tr_loss / len(Im_tr_loader)\n",
    "        avg_tr_accuracy = 100*accumulated_tr_accuracy / len(Im_tr_loader)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        vit_optimizer.zero_grad()\n",
    "\n",
    "        # Backward path (Gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer(Adam) Step\n",
    "        vit_optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{vit_epochs}]\")\n",
    "        loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "    \n",
    "    tr_loss_history.append(avg_tr_loss)\n",
    "    tr_accuracy_history.append(avg_tr_accuracy)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "# Validation Loop\n",
    "    vit_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch, (_, inputs, targets) in enumerate(Im_val_loader):\n",
    "\n",
    "            # Put inputs and labels in device cuda\n",
    "            val_images = inputs.to(device)\n",
    "            val_labels = targets.to(device)\n",
    "\n",
    "            val_outputs = vit_model(val_images).squeeze(1)\n",
    "\n",
    "            loss = criterion(val_outputs,val_labels)\n",
    "\n",
    "            accumulated_val_loss += loss.detach().cpu().item() #loss.item()\n",
    "            accumulated_val_accuracy += (val_labels == val_outputs).sum().item()/val_labels.size(0)\n",
    "\n",
    "            # avg_val_loss = accumulated_val_loss / (batch+1)\n",
    "            # avg_val_accuracy = 100*accumulated_val_accuracy / (batch+1)\n",
    "            avg_val_loss = accumulated_val_loss / len(Im_val_loader)\n",
    "            avg_val_accuracy = 100*accumulated_val_accuracy / len(Im_val_loader)\n",
    "\n",
    "        \n",
    "        best_val_loss=min(val_loss_history)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            torch.save(vit_model.state_dict(), model_state_path+f'_epoch_{epoch}')\n",
    "\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_accuracy_history.append(avg_val_accuracy)\n",
    "\n",
    "        print(f'Validation loss: {avg_val_loss:.2f} | Validation accuracy: {avg_val_accuracy:.2f}')\n",
    "        print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in vit_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", vit_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(tr_loss_history )), tr_loss_history , label='Train Loss')\n",
    "axs[1].plot(range(len(tr_accuracy_history)), tr_accuracy_history, label='Validation Loss')\n",
    "axs[0].plot(range(len(val_loss_history)), val_loss_history, label='Train Accuracy')\n",
    "axs[1].plot(range(len(val_accuracy_history)), val_accuracy_history, label='Validation Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
