{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5ffbdebd50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "# which makes intuitive sense.\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.io import read_image\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch is using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_fname_space(path):\n",
    "    for filename in os.listdir(path):\n",
    "        my_source = path + \"/\" + filename\n",
    "        my_dest = path + \"/\" + filename.strip().replace(\" \", \"\")\n",
    "        os.rename(my_source, my_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr_real = '../../data/CIFAK/train/REAL'\n",
    "# remove_fname_space(path_tr_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_tr_real = os.listdir(path_tr_real)\n",
    "fname_tr_real.sort()\n",
    "labels_lst=[1]*len(fname_tr_real)\n",
    "# print(len(labels_lst))\n",
    "# print(len(fname_tr_real))\n",
    "tr_dict={'Image_name':fname_tr_real,'True?':labels_lst}\n",
    "tr_df=pd.DataFrame(tr_dict)\n",
    "tr_df.to_csv(path_tr_real+\"/tr_annotation.csv\")\n",
    "\n",
    "fpath_tr_real = []\n",
    "for i, file in enumerate(fname_tr_real):\n",
    "    fpath_tr_real.append(path_tr_real+\"/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imread(fpath_tr_real[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width=8):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # img: is a tensor of the shape (Color_Channels x Rows (Hight) x Columns (Width))\n",
    "        #\n",
    "        # Make a slice every \"slice_width\" as we are moving across dimension 1 (as we are moving\n",
    "        # vertically across rows)\n",
    "        img = img.unfold(1, self.slice_width, self.slice_width)\n",
    "        # Make a slice every slice_width as we are moving across dimension 2,\n",
    "        # Note that previous operation has added new dimension at the beginning\n",
    "        # refers to no. of vertical slices, hence 2 here still refers to the rows.\n",
    "        img = img.unfold(2, self.slice_width, self.slice_width)\n",
    "        return img\n",
    "\n",
    "    def plot(self, img):\n",
    "        img = self.slice(img).permute(1, 2, 0, 3, 4)\n",
    "\n",
    "        fig = plt.figure(figsize=(2, 2))\n",
    "        grid = ImageGrid(fig, 111, nrows_ncols=(\n",
    "            img.size(0), img.size(1)), axes_pad=0.1)\n",
    "        print(grid)\n",
    "\n",
    "        for i, ax in enumerate(grid):\n",
    "            i_b4 = str(np.base_repr(i, 4)).zfill(2)\n",
    "            row = int(i_b4[0])\n",
    "            column = int(i_b4[1])\n",
    "            patch = img[row][column].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(patch)\n",
    "            ax.axis('off')\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.slice(img)\n",
    "        channels = img.size(0)\n",
    "\n",
    "        return img.reshape(-1, self.slice_width * self.slice_width * channels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((32, 32)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(8)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, images, annotations_file, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = read_image(self.images[index])\n",
    "        \n",
    "        # img.to(device)\n",
    "\n",
    "        labels = self.labels.iloc[index,2]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "tr_annotation_file= path_tr_real+\"/tr_annotation.csv\"\n",
    "Im_tr_dataset = Images_Dataset(fpath_tr_real,tr_annotation_file, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                       batch_size=64,\n",
    "                       # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                       drop_last=True,\n",
    "                       shuffle=True,\n",
    "                       num_workers=4)  # increase number of processor cores loading the data and getting it ready for model training inside the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im_dataset[12]\n",
    "# plt.imshow(Im_dataset[12].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by slicing each image [3,32,23] ==> [16,3,8,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 192])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(Im_tr_loader))\n",
    "train_features[0].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
