{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "# which makes intuitive sense.\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam , lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "# start= torch.cuda.Event(enable_timing=True)\n",
    "# end=torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr = '../../data/CIFAK/train'\n",
    "path_val = '../../data/CIFAK/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Spaces from the Dataset folder to simplify reading from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_fname_space(path):\n",
    "#     for filename in os.listdir(path):\n",
    "#         my_source = path + \"/\" + filename\n",
    "#         my_dest = path + \"/\" + filename.strip().replace(\" \", \"\")\n",
    "#         os.rename(my_source, my_dest)\n",
    "\n",
    "\n",
    "# remove_fname_space(path_tr + \"/REAL\")\n",
    "# remove_fname_space(path_tr + \"/FAKE\")\n",
    "# remove_fname_space(path_val + \"/REAL\")\n",
    "# remove_fname_space(path_val + \"/FAKE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_data(img_path, folder_name, img_class):\n",
    "    fname = os.listdir(img_path + \"/\"+folder_name)\n",
    "    fname.sort()\n",
    "    fpath = [img_path + \"/\"+folder_name+\"/\" + f for f in fname]\n",
    "    # print(fpath[0])\n",
    "    labels = [img_class]*len(fname)\n",
    "    return fpath, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_tr_real, labels_tr_real = dataset_data(path_tr, \"REAL\", 1.0)\n",
    "fpath_tr_fake, labels_tr_fake = dataset_data(path_tr, \"FAKE\", 0.0)\n",
    "fpath_tr = fpath_tr_real + fpath_tr_fake\n",
    "labels_tr = labels_tr_real+labels_tr_fake\n",
    "tr_dict = {'Image_path': fpath_tr, 'True?': labels_tr}\n",
    "tr_df = pd.DataFrame(tr_dict)\n",
    "tr_df.to_csv(path_tr+\"/tr_annotation.csv\")\n",
    "\n",
    "\n",
    "fpath_val_real, labels_val_real = dataset_data(path_val, \"REAL\", 1.0)\n",
    "fpath_val_fake, labels_val_fake = dataset_data(path_val, \"FAKE\", 0.0)\n",
    "fpath_val = fpath_val_real + fpath_val_fake\n",
    "labels_val = labels_val_real+labels_val_fake\n",
    "val_dict = {'Image_path': fpath_val, 'True?': labels_val}\n",
    "val_df = pd.DataFrame(val_dict)\n",
    "val_df.to_csv(path_val+\"/val_annotation.csv\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total Number of:          Real         Fake         Total\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Training Samples:        \",len(fpath_tr_real),\"     \",len(fpath_tr_fake),\"       \",len(fpath_tr))\n",
    "print(\"Validation Samples:      \",len(fpath_val_real),\"     \",len(fpath_val_fake),\"       \",len(fpath_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tr_df\")\n",
    "tr_df.iloc[[0,1,99998,99999]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val_df\")\n",
    "val_df.iloc[[0,1,19998,19999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_horizontal_slices= 4\n",
    "images_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = cv2.imread(fpath_tr[0]).shape\n",
    "print(\"Image Shape: \",img_shape)\n",
    "\n",
    "slice_width = img_shape[0]//Img_horizontal_slices\n",
    "print(\"Image Slice Shape: \",(slice_width,slice_width))\n",
    "\n",
    "total_img_slices = Img_horizontal_slices**2\n",
    "print(\"Total Number of Slices per Image: \",total_img_slices)\n",
    "print(\"Total Number of Images per Loader: \",images_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # img: is a tensor of the shape (Color_Channels x Rows (Hight) x Columns (Width))\n",
    "        #\n",
    "        # Make a slice every \"slice_width\" as we are moving across dimension 1 (as we are moving\n",
    "        # vertically across rows)\n",
    "        img = img.unfold(1, self.slice_width, self.slice_width)\n",
    "        # Make a slice every slice_width as we are moving across dimension 2,\n",
    "        # Note that previous operation has added new dimension at the beginning\n",
    "        # refers to no. of vertical slices, hence 2 here still refers to the rows.\n",
    "        img = img.unfold(2, self.slice_width, self.slice_width)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.slice(img)\n",
    "        channels = img.size(0)\n",
    "\n",
    "        return img.reshape(-1, self.slice_width * self.slice_width * channels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((32, 32)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None):\n",
    "        \n",
    "        self.annotation = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 1])\n",
    "        # img.to(device)\n",
    "        # labels = torch.tensor(self.annotation.iloc[index, 2]).unsqueeze(1)\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 2])\n",
    "\n",
    "        # print(type(labels))\n",
    "        # print(labels.size())\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "tr_annotation_file = path_tr+\"/tr_annotation.csv\"\n",
    "val_annotation_file = path_val+\"/val_annotation.csv\"\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(tr_annotation_file, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "\n",
    "Im_val_dataset = Images_Dataset(val_annotation_file, data_transform)\n",
    "\n",
    "Im_val_loader = DataLoader(dataset=Im_val_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original Image and the slices Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(Img_train_loader, index):\n",
    "\n",
    "    date_sample=Img_train_loader.dataset[index]\n",
    "\n",
    "    img = date_sample[0]\n",
    "    img_t = date_sample[1]\n",
    "    label = date_sample[2]\n",
    "\n",
    "    channels = img.size(0)\n",
    "    img_size = img.size(1)\n",
    "\n",
    "    total_img_slices = img_t.size(0)\n",
    "    slice_flat = img_t.size(1)\n",
    "\n",
    "    imgs_per_batch = Im_tr_loader.batch_size\n",
    "\n",
    "    slice_width = int(np.sqrt(slice_flat/channels))\n",
    "\n",
    "    return img, channels, img_size, slice_width, total_img_slices, slice_flat, imgs_per_batch, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img).permute(1, 2, 0, 3, 4)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(4, 4))\n",
    "    subfigs = fig.subfigures(2, 1,height_ratios=[1., 1.])\n",
    "    \n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    axs0.imshow(img.permute(1, 2, 0))\n",
    "    axs0.axis('off')\n",
    "\n",
    "    grid = ImageGrid(subfigs[1], 111, nrows_ncols=(\n",
    "        sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, ch, img_size, slice_width, slices, slice_flat, batch_size, label = img_data(\n",
    "    Im_tr_loader, 2)\n",
    "img_plot(img, slice_width)\n",
    "# torch.tensor().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the VIT Model : Embedding &rarr; Transformer Encoder &rarr; MLP_Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1, slice_embed_size))\n",
    "        self.pos_to_embed = nn.Parameter(\n",
    "            torch.rand(1, img_slices + 1, slice_embed_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_imgs):\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        #\n",
    "        img_embedding = self.img_to_embed(flattened_imgs)\n",
    "\n",
    "        cls_embedding = self.cls_to_embed.repeat(img_embedding.size(0), 1, 1)\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "\n",
    "        position_embedding = self.pos_to_embed.repeat(\n",
    "            img_embedding.size(0), 1, 1)\n",
    "\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        embedding = self.dropout(img_and_pos_embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tr_img_t, _ = next(iter(Im_tr_loader))\n",
    "\n",
    "slice_embed = slice_flat\n",
    "\n",
    "embedding_layer = ImageEmbedding(slice_input_size=slice_flat,\n",
    "                                 slice_embed_size=slice_embed,\n",
    "                                 img_slices=slices,\n",
    "                                 dropout_ratio=0.2)\n",
    "\n",
    "embedding_output = embedding_layer(tr_img_t)\n",
    "\n",
    "print(\"Input Shape:  \", tr_img_t.size())\n",
    "print(\"Output Shape: \", embedding_output.size())\n",
    "# print(embedding_output)\n",
    "\n",
    "summary(model=embedding_layer,\n",
    "        input_size=tr_img_t.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_attention = nn.LayerNorm(size)\n",
    "        self.query = nn.Linear(size, size)\n",
    "        self.key = nn.Linear(size, size)\n",
    "        self.value = nn.Linear(size, size)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.norm_feed_forward = nn.LayerNorm(size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(size, 4 * size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * size, size),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        attn_input = self.norm_attention(input_tensor)\n",
    "        query = self.query(attn_input)\n",
    "        key = self.key(attn_input)\n",
    "        value = self.value(attn_input)\n",
    "        attn_output, _ = self.attention(query=query, key=key, value=value)\n",
    "\n",
    "        attn_plus_norm = input_tensor + attn_output\n",
    "\n",
    "        mlp_input = self.norm_feed_forward(attn_plus_norm)\n",
    "        output = attn_plus_norm + self.feed_forward(mlp_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = Encoder(size=slice_flat, num_heads=12, dropout=0.1)\n",
    "encoder_output = encoder_layer(embedding_output)\n",
    "\n",
    "print(\"Output Shape: \", encoder_output.size())\n",
    "\n",
    "summary(model=encoder_layer,\n",
    "        input_size=embedding_output.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders=1,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1,\n",
    "                 lr=1e-4, min_lr=4e-5,\n",
    "                 weight_decay=0.1,\n",
    "                 epochs=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "        \n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, num_heads, dropout=enc_dropout) for _ in range(num_encoders)],)\n",
    "        \n",
    "        self.mlp_head = nn.Linear(slice_embed_size, num_classes)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        output = torch.round(self.sigmoid(self.mlp_head(attn[:, 0, :])))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_embed = slice_flat*4\n",
    "\n",
    "vit_model = VIT(slice_input_size=slice_flat,\n",
    "                slice_embed_size=slice_embed,\n",
    "                img_slices=slices,\n",
    "                num_classes=1,\n",
    "                num_heads=12,\n",
    "                num_encoders=2,\n",
    "                emb_dropout=0.1,\n",
    "                enc_dropout=0.1,\n",
    "                lr=1e-4, min_lr=4e-5,\n",
    "                weight_decay=0.1,\n",
    "                epochs=200)\n",
    "\n",
    "vit_output = vit_model(tr_img_t)\n",
    "\n",
    "print(\"Input Shape: \", tr_img_t.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=vit_model,\n",
    "        input_size=tr_img_t.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_epochs = 30         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "vit_optimizer = Adam(vit_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "criterion = CrossEntropyLoss() # returns the mean loss for the batch\n",
    "scheduler = lr_scheduler.LinearLR(vit_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model.to(device)\n",
    "\n",
    "for epoch in range(vit_epochs):\n",
    "\n",
    "    # Training Loop\n",
    "    vit_model.train()\n",
    "\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_tr_accuracy = 0\n",
    "    accumulated_val_loss = 0\n",
    "    accumulated_val_accuracy = 0\n",
    "\n",
    "    tr_loss_history = []\n",
    "    tr_accuracy_history = []\n",
    "    val_loss_history = []\n",
    "    val_accuracy_history = []\n",
    "\n",
    "    loop = tqdm(enumerate(Im_tr_loader), total=len(Im_tr_loader))\n",
    "    for batch, (_, inputs, targets) in loop:\n",
    "\n",
    "        # Put inputs and labels in device cuda\n",
    "        tr_images = inputs.to(device)\n",
    "        tr_labels = targets.to(device)\n",
    "\n",
    "        # Forward Path\n",
    "        tr_outputs = vit_model(tr_images).squeeze(1)\n",
    "\n",
    "        loss = criterion(tr_outputs,tr_labels)\n",
    "\n",
    "        accumulated_tr_loss += loss.item()\n",
    "        accumulated_tr_accuracy += (tr_labels == tr_outputs).sum().item()/tr_labels.size(0)\n",
    "        \n",
    "        avg_tr_loss = accumulated_tr_loss / (batch+1)\n",
    "        avg_tr_accuracy = 100*accumulated_tr_accuracy / (batch+1)\n",
    "\n",
    "        # Backward path (Gradient)\n",
    "        vit_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer(Adam) Step\n",
    "        vit_optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{vit_epochs}]\")\n",
    "        loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "    \n",
    "    tr_loss_history.append(avg_tr_loss)\n",
    "    tr_accuracy_history.append(avg_tr_accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# Validation Loop\n",
    "    vit_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch, (_, inputs, targets) in enumerate(Im_val_loader):\n",
    "\n",
    "            # Put inputs and labels in device cuda\n",
    "            val_images = inputs.to(device)\n",
    "            val_labels = targets.to(device)\n",
    "\n",
    "            val_outputs = vit_model(val_images).squeeze(1)\n",
    "\n",
    "            loss = criterion(val_outputs,val_labels)\n",
    "\n",
    "            accumulated_val_loss += loss.item()\n",
    "            accumulated_val_accuracy += (val_labels == val_outputs).sum().item()/val_labels.size(0)\n",
    "\n",
    "            avg_val_loss = accumulated_val_loss / (batch+1)\n",
    "            avg_val_accuracy = 100*accumulated_val_accuracy / (batch+1)\n",
    "\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_accuracy_history.append(avg_val_accuracy)\n",
    "\n",
    "        print(f'Validation loss: {avg_val_loss:.2f} | Validation accuracy: {avg_val_accuracy:.2f}')\n",
    "        print(\"-------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(tr_loss_history )), tr_loss_history , label='Train Loss')\n",
    "axs[1].plot(range(len(tr_accuracy_history)), tr_accuracy_history, label='Validation Loss')\n",
    "axs[0].plot(range(len(val_loss_history)), val_loss_history, label='Train Accuracy')\n",
    "axs[1].plot(range(len(val_accuracy_history)), val_accuracy_history, label='Validation Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
