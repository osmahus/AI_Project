{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from dataset_process import dataset_to_df, search_df\n",
    "from engine import train_fn , eval_fn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import albumentations as alb\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from typing import Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.1+cu121\n",
      "GPU Card: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Torch is using device: cuda:0\n",
      "CPU Count: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/CIFAK'\n",
    "relative_paths = [\"/train/REAL\", \"/train/FAKE\", \"/test/REAL\", \"/test/FAKE\"]\n",
    "paths_classes = [\"REAL\", \"FAKE\", \"REAL\", \"FAKE\"]\n",
    "\n",
    "# path = '../../data/meso_data'\n",
    "# relative_paths= [\"/Real\", \"/DeepFake\"]\n",
    "# paths_classes=['REAL',\"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REAL</th>\n",
       "      <th>FAKE</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>48000</td>\n",
       "      <td>48000</td>\n",
       "      <td>96000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>11400</td>\n",
       "      <td>11400</td>\n",
       "      <td>22800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row_Total</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             REAL   FAKE   Total\n",
       "Training    48000  48000   96000\n",
       "Validation  11400  11400   22800\n",
       "Testing       600    600    1200\n",
       "Row_Total   60000  60000  120000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all, df_train, df_val, df_test, classes_stats = dataset_to_df(\n",
    "    path, relative_paths, paths_classes, 0.8, 0.19, 0.01)\n",
    "\n",
    "classes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First and Last Elements in the Whole dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>Image_size</th>\n",
       "      <th>Class</th>\n",
       "      <th>Class_Codes</th>\n",
       "      <th>Class_Onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/CIFAK//train/REAL/0000(10).jpg</td>\n",
       "      <td>(3, 32, 32)</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>../../data/CIFAK//test/FAKE/999.jpg</td>\n",
       "      <td>(3, 32, 32)</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Image_path   Image_size Class  \\\n",
       "0       ../../data/CIFAK//train/REAL/0000(10).jpg  (3, 32, 32)  REAL   \n",
       "119999        ../../data/CIFAK//test/FAKE/999.jpg  (3, 32, 32)  FAKE   \n",
       "\n",
       "        Class_Codes Class_Onehot  \n",
       "0                 1       [1, 0]  \n",
       "119999            0       [0, 1]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First and Last Elements in the Whole dataset\")\n",
    "df_all.iloc[[0,-1]]\n",
    "# print(df_all.iloc[[0,-1]].to_markdown(headers='keys', tablefmt='psql'))\n",
    "# print(\"\")\n",
    "# print(\"First and Last Elements in the Training dataset\")\n",
    "# print(df_train.iloc[[0,-1]].to_markdown(headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_horizontal_slices= 4\n",
    "images_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice_width: 8 pixels\n",
      "\n",
      "Image shape:  torch.Size([3, 32, 32])\n",
      "Image will be divided into: 4 x 4 = 16 slices each with shape (3, 8, 8)\n",
      "Target Shape of the final flattened image: 16 x 192 \n",
      "\n",
      "Feed (16) Images to the Dataloader\n"
     ]
    }
   ],
   "source": [
    "img_shape = read_image(df_all.iloc[0, 0]).size()\n",
    "\n",
    "slice_width = img_shape[1]//Img_horizontal_slices\n",
    "total_img_slices = Img_horizontal_slices**2\n",
    "\n",
    "print(f\"slice_width: {slice_width} pixels\")\n",
    "print(\"\")\n",
    "print(\"Image shape: \", img_shape)\n",
    "print(\n",
    "    f\"Image will be divided into: {Img_horizontal_slices} x {Img_horizontal_slices} = {total_img_slices} slices each with shape {(img_shape[0],slice_width,slice_width)}\")\n",
    "print(\n",
    "    f\"Target Shape of the final flattened image: {total_img_slices} x {img_shape[0]*slice_width**2} \")\n",
    "print(\"\")\n",
    "print(f\"Feed ({images_batch}) Images to the Dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # c: color channels\n",
    "\n",
    "        # h: desired slice height (pixels)\n",
    "        # w: desired slice width (pixels)\n",
    "\n",
    "        # row: No. of vertical slices\n",
    "        # col: No. of horizontal slices\n",
    "\n",
    "        # b: Batch of Images\n",
    "\n",
    "        sliced_Img = rearrange(\n",
    "            img, 'c (row h) (col w) -> row col c h w', h=slice_width, w=slice_width)\n",
    "\n",
    "        return sliced_Img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sliced_img = self.slice(img)\n",
    "        sliced_flattened_img = rearrange(\n",
    "            sliced_img, 'row col c h w -> (row col) (c h w)')\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618]\n",
    "norm_std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((64, 64)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=norm_mean, std=norm_std),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = alb.Compose(\n",
    "    [\n",
    "        To\n",
    "        alb.Resize(config.image_height, config.image_width, always_apply=True),\n",
    "        alb.Normalize(norm_mean , norm_std, always_apply=True)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        \n",
    "        self.annotation = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 0])\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 4], dtype=torch.float64)\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(df_train, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "Im_val_dataset = Images_Dataset(df_val, data_transform)\n",
    "\n",
    "Im_val_loader = DataLoader(dataset=Im_val_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original Image and the slices Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(Img_train_loader, index):\n",
    "\n",
    "    img, img_t, label = Img_train_loader.dataset[index]\n",
    "\n",
    "    channels, img_size,_ = img.size()\n",
    "\n",
    "    img_slices, slice_flatsize = img_t.size()\n",
    "\n",
    "    imgs_per_batch = Im_tr_loader.batch_size\n",
    "\n",
    "    slice_width = int(np.sqrt(slice_flatsize/channels))\n",
    "\n",
    "    return img, img_t, label, channels, img_size, slice_width, img_slices, slice_flatsize, imgs_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img)\n",
    "    print(\"Sliced Image Shape: \", sliced_img.shape)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(8, 4))\n",
    "    subfigs = fig.subfigures(3, 1, height_ratios=[1., 1.5, 1.], hspace=0.05, squeeze='True')\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    axs0.imshow(rearrange(img, \"c h w -> h w c\"))\n",
    "    axs0.axis('off')\n",
    "    # subfigs[0].suptitle('Input Image', fontsize=10)\n",
    "\n",
    "    grid1 = ImageGrid(subfigs[1], 111, nrows_ncols=(\n",
    "        sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid1):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = rearrange(sliced_img[row][column], \"c h w -> h w c\").numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[1].suptitle('Slices', fontsize=10)\n",
    "\n",
    "    grid2 = ImageGrid(subfigs[2], 111, nrows_ncols=(\n",
    "        1, sliced_img.size(0)*sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid2):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[2].suptitle('Position', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Images Shape: torch.Size([16, 16, 192])\n"
     ]
    }
   ],
   "source": [
    "_, sample_img_batch,_ = next(iter(Im_tr_loader))\n",
    "\n",
    "print(f\"Batch of Images Shape: {sample_img_batch.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape: torch.Size([3, 32, 32])\n",
      "Sliced Image Shape:  torch.Size([4, 4, 3, 8, 8])\n",
      "Flattened Image Shape: torch.Size([16, 192])\n",
      "\n",
      "slice_flatsize 192\n",
      "slice_embed 768\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 65 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img, sample_img_trans, sample_img_label, ch, img_size, slice_width, slices, slice_flatsize, batch_size = img_data(\n",
    "    Im_tr_loader, 2)\n",
    "print(f\"Input Image Shape: {sample_img.size()}\")\n",
    "img_plot(sample_img, slice_width)\n",
    "print(f\"Flattened Image Shape: {sample_img_trans.size()}\")\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"slice_flatsize\",slice_flatsize)\n",
    "slice_embed = slice_flatsize*4\n",
    "print(\"slice_embed\",slice_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the VIT Model : Embedding &rarr; Transformer Encoder &rarr; MLP_Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use FC layer to change the image flattened slice to embedding    \n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "\n",
    "        # cls: classification token is added to each image as an additional slice(patch),\n",
    "        # it has the same embedding size as any slice in the image.\n",
    "        # here I consider a single image, the batch size will be considered under the forward method\n",
    "        # note that it will be trainable that's why I added (requires grad)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1, slice_embed_size),requires_grad=True)\n",
    "\n",
    "        # positional embedding: is added to each slice in the image (including the cls token),\n",
    "        # it identifies the position of the slice (patch) inside the image.\n",
    "        # here I consider a single image, the batch size will be considered under the forward method\n",
    "        # note that it will be trainable that's why I added (requires grad)\n",
    "        self.pos_to_embed = nn.Parameter(torch.rand(img_slices + 1, slice_embed_size), requires_grad=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_imgs):\n",
    "\n",
    "        # b: is the number of images in the batch\n",
    "        batch_size = flattened_imgs.size(0)\n",
    "\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        #\n",
    "        img_embedding = self.img_to_embed(flattened_imgs)\n",
    "\n",
    "        # repeat cls for all the images in the batch\n",
    "        cls_embedding = self.cls_to_embed.repeat(batch_size, 1, 1)\n",
    "\n",
    "        # we can repeat position for all the images in the batch, but it's not necessary\n",
    "        # position_embedding = self.pos_to_embed.repeat(batch_size, 1, 1)\n",
    "        position_embedding = self.pos_to_embed\n",
    "\n",
    "        # Append the cls embedding to the beginning of the image slices\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        \n",
    "        embedding = self.dropout(img_and_pos_embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape:   torch.Size([16, 16, 192])\n",
      "Label of a sample image:  tensor([1., 0.], dtype=torch.float64)\n",
      "\n",
      "Output Shape:  torch.Size([16, 17, 768])\n",
      "Output Gradient Function:  <MulBackward0 object at 0x7fbd2e44fa30>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ImageEmbedding (ImageEmbedding)          [16, 16, 192]        [16, 17, 768]        13,824               True\n",
       "├─Linear (img_to_embed)                  [16, 16, 192]        [16, 16, 768]        148,224              True\n",
       "├─Dropout (dropout)                      [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "========================================================================================================================\n",
       "Total params: 162,048\n",
       "Trainable params: 162,048\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 2.37\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 1.57\n",
       "Params size (MB): 0.59\n",
       "Estimated Total Size (MB): 2.36\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = ImageEmbedding(slice_input_size=slice_flatsize,\n",
    "                                 slice_embed_size=slice_embed,\n",
    "                                 img_slices=slices,\n",
    "                                 dropout_ratio=0.2)\n",
    "\n",
    "embedding_output = embedding_layer(sample_img_batch)\n",
    "\n",
    "print(\"Sample batch shape:  \", sample_img_batch.size())\n",
    "print(\"Label of a sample image: \",sample_img_label)\n",
    "print(\"\")\n",
    "print(\"Output Shape: \", embedding_output.size())\n",
    "print(\"Output Gradient Function: \",embedding_output.grad_fn)\n",
    "\n",
    "summary(model=embedding_layer,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "red { color: red }\n",
    "yellow { color: yellow }\n",
    "</style>\n",
    "\n",
    "##### **<yellow> Query: </yellow>** The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "\n",
    "##### **<yellow> Keys: </yellow>** For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "\n",
    "##### **<yellow> Values: </yellow>** For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "\n",
    "##### **<yellow> Score function: </yellow>** To rate which elements we want to pay attention to, we need to specify a score function . The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "##### A word asks the same question to all words in the sequence using the ‘query’ vector. Similarly, it provides the same answer to all words using the ‘key’ vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{Softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right) V\n",
    "$$\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_of_head: int = 12, Dropout=0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_of_head = num_of_head\n",
    "        self.depth = self.embed_dim // self.num_of_head\n",
    "        self.Wq = nn.Linear(in_features=embed_dim, out_features=embed_dim)\n",
    "        self.Wk = nn.Linear(in_features=embed_dim, out_features=embed_dim)\n",
    "        self.Wv = nn.Linear(in_features=embed_dim, out_features=embed_dim)\n",
    "        # self.dropout=nn.Dropout(p=Dropout)\n",
    "        self.dropout = Dropout\n",
    "        self.linear = nn.Linear(in_features=embed_dim, out_features=embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        Q = rearrange(q, 'b s (h d) -> b h s d',\n",
    "                      h=self.num_of_head, d=self.depth)\n",
    "        K = rearrange(k, 'b s (h d) -> b h s d',\n",
    "                      h=self.num_of_head, d=self.depth)\n",
    "        V = rearrange(v, 'b s (h d) -> b h s d',\n",
    "                      h=self.num_of_head, d=self.depth)\n",
    "\n",
    "        use_dropout = 0.0 if not self.training else self.dropout\n",
    "\n",
    "        attention = F.scaled_dot_product_attention(\n",
    "            Q, K, V, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
    "        # # Q(b h s d) . KT(b h d s) ==> b h s s\n",
    "        # # qk = torch.matmul(q, k.transpose(-2, -1)) / (self.depth**0.5)\n",
    "        # qk = torch.matmul(q, k.transpose(-2, -1)) / (self.embed_dim**0.5)\n",
    "\n",
    "        # #  b h s s ==> b h s s\n",
    "        # weight = F.softmax(qk, dim=1)\n",
    "        # # weight_dropout=self.dropout(weight)\n",
    "\n",
    "        # #weight(b h s s).V(b h s d) ==> attention (b h s d)\n",
    "        # attention = torch.matmul(weight, v)\n",
    "        # # attention = torch.matmul(weight_dropout, v)\n",
    "\n",
    "        attention = rearrange(attention, 'b h s d -> b s (h d)')\n",
    "        attention = self.linear(attention)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA_layer = MHA(d_in=embedding_output.size(2),\n",
    "#                                  d_out=embedding_output.size(2),\n",
    "#                                  num_heads=12,\n",
    "#                                  block_size=embedding_output.size(1),\n",
    "#                                  dropout=0.0\n",
    "#                                  )\n",
    "\n",
    "# MHA_output = MHA_layer(embedding_output)\n",
    "\n",
    "# print(\"Input Shape:  \", embedding_output.size())\n",
    "# print(\"Output Shape: \", MHA_output.size())\n",
    "\n",
    "# summary(model=MHA_layer,\n",
    "#         input_size=embedding_output.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.attention = MHA(embed_dim=embed_size,\n",
    "                             num_of_head=num_heads,\n",
    "                             Dropout=dropout)\n",
    "\n",
    "        # Below I selected the hidden size of the feed forward = 4 * embed size\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, input):\n",
    "        normalized_input = self.norm1(input)\n",
    "\n",
    "        attn = self.attention(normalized_input)\n",
    "\n",
    "        attn_and_input = input + attn\n",
    "\n",
    "        normalized_attn = self.norm2(attn_and_input)\n",
    "\n",
    "        output = attn + self.feed_forward(normalized_attn)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = Encoder(embed_size=slice_flatsize, num_heads=12, dropout=0.1)\n",
    "# encoder_output = encoder_layer(embedding_output)\n",
    "\n",
    "# print(\"Output Shape: \", encoder_output.size())\n",
    "\n",
    "# summary(model=encoder_layer,\n",
    "#         input_size=embedding_output.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders=1,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, num_heads, dropout=enc_dropout) for _ in range(num_encoders)])\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(slice_embed_size),\n",
    "            nn.Linear(slice_embed_size, num_classes))\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        output = self.mlp_head(attn[:, 0])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model Hyper-parameters Hereunder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "VIT (VIT)                                     [16, 16, 192]        [16, 2]              --                   True\n",
       "├─ImageEmbedding (embedding)                  [16, 16, 192]        [16, 17, 768]        13,824               True\n",
       "│    └─Linear (img_to_embed)                  [16, 16, 192]        [16, 16, 768]        148,224              True\n",
       "│    └─Dropout (dropout)                      [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "├─Sequential (encoders)                       [16, 17, 768]        [16, 17, 768]        --                   True\n",
       "│    └─Encoder (0)                            [16, 17, 768]        [16, 17, 768]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                 [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─MHA (attention)                   [16, 17, 768]        [16, 17, 768]        2,362,368            True\n",
       "│    │    └─LayerNorm (norm2)                 [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─Sequential (feed_forward)         [16, 17, 768]        [16, 17, 768]        4,723,968            True\n",
       "│    └─Encoder (1)                            [16, 17, 768]        [16, 17, 768]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                 [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─MHA (attention)                   [16, 17, 768]        [16, 17, 768]        2,362,368            True\n",
       "│    │    └─LayerNorm (norm2)                 [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─Sequential (feed_forward)         [16, 17, 768]        [16, 17, 768]        4,723,968            True\n",
       "├─Sequential (mlp_head)                       [16, 768]            [16, 2]              --                   True\n",
       "│    └─LayerNorm (0)                          [16, 768]            [16, 768]            1,536                True\n",
       "│    └─Linear (1)                             [16, 768]            [16, 2]              1,538                True\n",
       "=============================================================================================================================\n",
       "Total params: 14,343,938\n",
       "Trainable params: 14,343,938\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 229.28\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 41.78\n",
       "Params size (MB): 57.32\n",
       "Estimated Total Size (MB): 99.30\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model = VIT(slice_input_size=slice_flatsize,\n",
    "                slice_embed_size=slice_embed,\n",
    "                img_slices=slices,\n",
    "                num_classes=2,\n",
    "                num_heads=12,\n",
    "                num_encoders=2,\n",
    "                emb_dropout=0.1,\n",
    "                enc_dropout=0.1)\n",
    "\n",
    "vit_output = vit_model(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape: \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=vit_model,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_epochs = 200         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "optimizer = Adam(vit_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "loss_criterion = CrossEntropyLoss() # returns the mean loss for the batch\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path=\"../weights/vit5/\"\n",
    "\n",
    "# saved_model = vit_model()\n",
    "# saved_model.load_state_dict(torch.load(model_state_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]: 100%|██████████| 6000/6000 [01:02<00:00, 95.26it/s, acc=50.1, loss=nan]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: nan | Validation accuracy: 50.00\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/200]:  17%|█▋        | 1015/6000 [00:10<00:52, 94.13it/s, acc=50.3, loss=nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Optimizer(Adam) Step\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch [\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvit_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mavg_tr_loss, acc\u001b[38;5;241m=\u001b[39mavg_tr_accuracy)\n\u001b[1;32m     52\u001b[0m tr_loss_history\u001b[38;5;241m.\u001b[39mappend(avg_tr_loss)\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:1394\u001b[0m, in \u001b[0;36mtqdm.set_description\u001b[0;34m(self, desc, refresh)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc \u001b[38;5;241m=\u001b[39m desc \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[0;32m-> 1394\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[1;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:452\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m--> 452\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     fp_flush()\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/ipykernel/iostream.py:694\u001b[0m, in \u001b[0;36mOutStream.write\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/ipykernel/iostream.py:590\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/ipykernel/iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     f()\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/zmq/sugar/socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    689\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    690\u001b[0m             data,\n\u001b[1;32m    691\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    692\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    693\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    694\u001b[0m         )\n\u001b[1;32m    695\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_loss_history = [1000]\n",
    "tr_accuracy_history = [0]\n",
    "val_loss_history = [1000]\n",
    "val_accuracy_history = [0]\n",
    "\n",
    "vit_model.to(device)\n",
    "\n",
    "for epoch in range(vit_epochs):\n",
    "\n",
    "    # Training Loop\n",
    "    vit_model.train(True)\n",
    "\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_tr_accuracy = 0\n",
    "    accumulated_val_loss = 0\n",
    "    accumulated_val_accuracy = 0\n",
    "\n",
    "    loop = tqdm(enumerate(Im_tr_loader), total=len(Im_tr_loader))\n",
    "    for batch, (_, inputs, targets) in loop:\n",
    "\n",
    "        # Put inputs and labels in device cuda\n",
    "        tr_images = inputs.to(device)\n",
    "        sample_img_labels = targets.to(device)\n",
    "\n",
    "        # Forward Path\n",
    "        tr_outputs = vit_model(tr_images)#.squeeze(1)\n",
    "        # print(sample_img_labels.size())\n",
    "        # print(tr_outputs.size())\n",
    "\n",
    "        loss = loss_criterion(tr_outputs,sample_img_labels)\n",
    "\n",
    "        accumulated_tr_loss += loss.detach().cpu().item()#loss.item()\n",
    "        accumulated_tr_accuracy += (sample_img_labels.argmax(dim=-1) == tr_outputs.argmax(dim=-1)).sum().item()/sample_img_labels.size(0)\n",
    "        \n",
    "        avg_tr_loss = accumulated_tr_loss / (batch+1)\n",
    "        avg_tr_accuracy = 100*accumulated_tr_accuracy / (batch+1)\n",
    "        # avg_tr_loss = accumulated_tr_loss / len(Im_tr_loader)\n",
    "        # avg_tr_accuracy = 100*accumulated_tr_accuracy / len(Im_tr_loader)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward path (Gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer(Adam) Step\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{vit_epochs}]\")\n",
    "        loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "    \n",
    "    tr_loss_history.append(avg_tr_loss)\n",
    "    tr_accuracy_history.append(avg_tr_accuracy)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "# Validation Loop\n",
    "    vit_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch, (_, inputs, targets) in enumerate(Im_val_loader):\n",
    "\n",
    "            # Put inputs and labels in device cuda\n",
    "            val_images = inputs.to(device)\n",
    "            val_labels = targets.to(device)\n",
    "\n",
    "            val_outputs = vit_model(val_images)#.squeeze(1)\n",
    "\n",
    "            loss = loss_criterion(val_outputs,val_labels)\n",
    "\n",
    "            accumulated_val_loss += loss.detach().cpu().item() #loss.item()\n",
    "            accumulated_val_accuracy += (val_labels.argmax(dim=-1) == val_outputs.argmax(dim=-1)).sum().item()/val_labels.size(0)\n",
    "\n",
    "            avg_val_loss = accumulated_val_loss / (batch+1)\n",
    "            avg_val_accuracy = 100*accumulated_val_accuracy / (batch+1)\n",
    "            # avg_val_loss = accumulated_val_loss / len(Im_val_loader)\n",
    "            # avg_val_accuracy = 100*accumulated_val_accuracy / len(Im_val_loader)\n",
    "\n",
    "        \n",
    "        best_val_loss=min(val_loss_history)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            torch.save(vit_model.state_dict(), model_state_path+f'epoch_{epoch}')\n",
    "\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_accuracy_history.append(avg_val_accuracy)\n",
    "\n",
    "        print(f'Validation loss: {avg_val_loss:.2f} | Validation accuracy: {avg_val_accuracy:.2f}')\n",
    "        print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in vit_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", vit_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(tr_loss_history )), tr_loss_history , label='Train Loss')\n",
    "axs[0].plot(range(len(tr_accuracy_history)), tr_accuracy_history, label='Validation Loss')\n",
    "axs[1].plot(range(len(val_loss_history)), val_loss_history, label='Train Accuracy')\n",
    "axs[1].plot(range(len(val_accuracy_history)), val_accuracy_history, label='Validation Accuracy')\n",
    "axs[0].set_ylim([0,70])\n",
    "axs[1].set_ylim([0,70])\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
