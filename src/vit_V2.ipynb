{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3869683d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "# which makes intuitive sense.\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch is using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr = '../../data/CIFAK/train'\n",
    "path_tst = '../../data/CIFAK/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Spaces from the Dataset folder to simplify reading from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_fname_space(path):\n",
    "#     for filename in os.listdir(path):\n",
    "#         my_source = path + \"/\" + filename\n",
    "#         my_dest = path + \"/\" + filename.strip().replace(\" \", \"\")\n",
    "#         os.rename(my_source, my_dest)\n",
    "\n",
    "\n",
    "# remove_fname_space(path_tr + \"/REAL\")\n",
    "# remove_fname_space(path_tr + \"/FAKE\")\n",
    "# remove_fname_space(path_tst + \"/REAL\")\n",
    "# remove_fname_space(path_tst + \"/FAKE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_data(img_path, folder_name, img_class):\n",
    "    fname = os.listdir(img_path + \"/\"+folder_name)\n",
    "    fname.sort()\n",
    "    fpath = [img_path + \"/\"+folder_name+\"/\" + f for f in fname]\n",
    "    print(fpath[0])\n",
    "    labels = [img_class]*len(fname)\n",
    "    return fpath, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/CIFAK/train/REAL/0000(10).jpg\n",
      "../../data/CIFAK/train/FAKE/1000(10).jpg\n",
      "../../data/CIFAK/test/REAL/0000(10).jpg\n",
      "../../data/CIFAK/test/FAKE/0(10).jpg\n"
     ]
    }
   ],
   "source": [
    "fpath_tr_real, labels_tr_real = dataset_data(path_tr, \"REAL\", 1)\n",
    "fpath_tr_fake, labels_tr_fake = dataset_data(path_tr, \"FAKE\", 0)\n",
    "fpath_tr = fpath_tr_real + fpath_tr_fake\n",
    "labels_tr = labels_tr_real+labels_tr_fake\n",
    "tr_dict = {'Image_path': fpath_tr, 'True?': labels_tr}\n",
    "tr_df = pd.DataFrame(tr_dict)\n",
    "tr_df.to_csv(path_tr+\"/tr_annotation.csv\")\n",
    "\n",
    "fpath_tst_real, labels_tst_real = dataset_data(path_tst, \"REAL\", 1)\n",
    "fpath_tst_fake, labels_tst_fake = dataset_data(path_tst, \"FAKE\", 0)\n",
    "fpath_tst = fpath_tst_real + fpath_tst_fake\n",
    "labels_tst = labels_tst_real+labels_tst_fake\n",
    "tst_dict = {'Image_path': fpath_tst, 'True?': labels_tst}\n",
    "tst_df = pd.DataFrame(tst_dict)\n",
    "tst_df.to_csv(path_tst+\"/tst_annotation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = cv2.imread(fpath_tr[0]).shape[0]\n",
    "\n",
    "horizontal_slides= 4\n",
    "num_loader_images = 16\n",
    "\n",
    "slice_width = img_size//horizontal_slides\n",
    "total_img_slices = horizontal_slides**2\n",
    "\n",
    "slice_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # img: is a tensor of the shape (Color_Channels x Rows (Hight) x Columns (Width))\n",
    "        #\n",
    "        # Make a slice every \"slice_width\" as we are moving across dimension 1 (as we are moving\n",
    "        # vertically across rows)\n",
    "        img = img.unfold(1, self.slice_width, self.slice_width)\n",
    "        # Make a slice every slice_width as we are moving across dimension 2,\n",
    "        # Note that previous operation has added new dimension at the beginning\n",
    "        # refers to no. of vertical slices, hence 2 here still refers to the rows.\n",
    "        img = img.unfold(2, self.slice_width, self.slice_width)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.slice(img)\n",
    "        channels = img.size(0)\n",
    "\n",
    "        return img.reshape(-1, self.slice_width * self.slice_width * channels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((32, 32)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None):\n",
    "        \n",
    "        self.annotation = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 1])\n",
    "        # img.to(device)\n",
    "        labels = self.annotation.iloc[index, 2]\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "tr_annotation_file = path_tr+\"/tr_annotation.csv\"\n",
    "tst_annotation_file = path_tst+\"/tst_annotation.csv\"\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(tr_annotation_file, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=num_loader_images,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "\n",
    "Im_tst_dataset = Images_Dataset(tst_annotation_file, data_transform)\n",
    "\n",
    "Im_tst_loader = DataLoader(dataset=Im_tst_dataset,\n",
    "                          batch_size=num_loader_images,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original Image and the slices Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(Img_train_loader, index):\n",
    "\n",
    "    img = Img_train_loader.dataset[index][0]\n",
    "    img_t = Img_train_loader.dataset[index][1]\n",
    "\n",
    "    channels = img.size(0)\n",
    "    img_size = img.size(1)\n",
    "\n",
    "    total_img_slices = img_t.size(0)\n",
    "    slice_flat = img_t.size(1)\n",
    "\n",
    "    imgs_per_batch = Im_tr_loader.batch_size\n",
    "\n",
    "    slice_width = int(np.sqrt(slice_flat/channels))\n",
    "\n",
    "    return img, channels, img_size, slice_width, total_img_slices, slice_flat, imgs_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img).permute(1, 2, 0, 3, 4)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(4, 4))\n",
    "    subfigs = fig.subfigures(2, 1,height_ratios=[1., 1.])\n",
    "    \n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    axs0.imshow(img.permute(1, 2, 0))\n",
    "    axs0.axis('off')\n",
    "\n",
    "    grid = ImageGrid(subfigs[1], 111, nrows_ncols=(\n",
    "        sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 33 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, ch, img_size, slice_width, slices, slice_flat, batch_size = img_data(\n",
    "    Im_tr_loader, 2)\n",
    "img_plot(img, slice_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the VIT Model : Embedding &rarr; Transformer Encoder &rarr; MLP_Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1, slice_embed_size))\n",
    "        self.pos_to_embed = nn.Parameter(\n",
    "            torch.rand(1, img_slices + 1, slice_embed_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_imgs):\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        #\n",
    "        img_embedding = self.img_to_embed(flattened_imgs)\n",
    "\n",
    "        cls_embedding = self.cls_to_embed.repeat(img_embedding.size(0), 1, 1)\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "\n",
    "        position_embedding = self.pos_to_embed.repeat(\n",
    "            img_embedding.size(0), 1, 1)\n",
    "\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        embedding = self.dropout(img_and_pos_embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 17, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ImageEmbedding (ImageEmbedding)          [16, 16, 192]        [16, 17, 192]        3,456                True\n",
       "├─Linear (img_to_embed)                  [16, 16, 192]        [16, 16, 192]        37,056               True\n",
       "├─Dropout (dropout)                      [16, 17, 192]        [16, 17, 192]        --                   --\n",
       "========================================================================================================================\n",
       "Total params: 40,512\n",
       "Trainable params: 40,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.59\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 0.15\n",
       "Estimated Total Size (MB): 0.74\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, tr_img_t, _ = next(iter(Im_tr_loader))\n",
    "\n",
    "slice_embed = slice_flat\n",
    "\n",
    "embedding_layer = ImageEmbedding(slice_input_size=slice_flat,\n",
    "                                 slice_embed_size=slice_embed,\n",
    "                                 img_slices=slices,\n",
    "                                 dropout_ratio=0.2)\n",
    "\n",
    "embedding_output = embedding_layer(tr_img_t)\n",
    "\n",
    "print(\"Input Shape: \", tr_img_t.size())\n",
    "print(\"Output Shape: \", embedding_output.size())\n",
    "# print(embedding_output)\n",
    "\n",
    "summary(model=embedding_layer,\n",
    "        input_size=(16, 16, 192),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_attention = nn.LayerNorm(size)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.norm_feed_forward = nn.LayerNorm(size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(size, 4 * size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * size, size),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        attn_input = self.norm_attention(input_tensor)\n",
    "        query = key = value = attn_input\n",
    "        attn_output,_=self.attention(query=query, key=key, value=value)\n",
    "\n",
    "        attn_plus_norm = input_tensor + attn_output\n",
    "        \n",
    "        mlp_input = self.norm_feed_forward(attn_plus_norm)\n",
    "        output = attn_plus_norm + self.feed_forward(mlp_input)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  torch.Size([16, 17, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Encoder (Encoder)                        [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "├─LayerNorm (norm_attention)             [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "├─MultiheadAttention (attention)         --                   [16, 17, 192]        148,224              True\n",
       "├─LayerNorm (norm_feed_forward)          [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "├─Sequential (feed_forward)              [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "│    └─Linear (0)                        [16, 17, 192]        [16, 17, 768]        148,224              True\n",
       "│    └─Dropout (1)                       [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "│    └─GELU (2)                          [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "│    └─Linear (3)                        [16, 17, 768]        [16, 17, 192]        147,648              True\n",
       "│    └─Dropout (4)                       [16, 17, 192]        [16, 17, 192]        --                   --\n",
       "========================================================================================================================\n",
       "Total params: 444,864\n",
       "Trainable params: 444,864\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.75\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.21\n",
       "Forward/backward pass size (MB): 2.92\n",
       "Params size (MB): 1.19\n",
       "Estimated Total Size (MB): 4.32\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = Encoder(size=slice_flat, num_heads=12, dropout=0.1)\n",
    "encoder_output = encoder_layer(embedding_output)\n",
    "\n",
    "print(\"Output Shape: \", encoder_output.size())\n",
    "\n",
    "summary(model=encoder_layer,\n",
    "        input_size=(16, 17, 192),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders=1,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1,\n",
    "                 lr=1e-4, min_lr=4e-5,\n",
    "                 weight_decay=0.1,\n",
    "                 epochs=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "        \n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, num_heads, dropout=enc_dropout) for _ in range(num_encoders)],)\n",
    "        \n",
    "        self.mlp_head = nn.Linear(slice_embed_size, num_classes)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        output = self.mlp_head(attn[:, 0, :])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "VIT (VIT)                                          [16, 16, 192]        [16, 2]              --                   True\n",
       "├─ImageEmbedding (embedding)                       [16, 16, 192]        [16, 17, 192]        3,456                True\n",
       "│    └─Linear (img_to_embed)                       [16, 16, 192]        [16, 16, 192]        37,056               True\n",
       "│    └─Dropout (dropout)                           [16, 17, 192]        [16, 17, 192]        --                   --\n",
       "├─Sequential (encoders)                            [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "│    └─Encoder (0)                                 [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "│    │    └─LayerNorm (norm_attention)             [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "│    │    └─MultiheadAttention (attention)         --                   [16, 17, 192]        148,224              True\n",
       "│    │    └─LayerNorm (norm_feed_forward)          [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "│    │    └─Sequential (feed_forward)              [16, 17, 192]        [16, 17, 192]        295,872              True\n",
       "│    └─Encoder (1)                                 [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "│    │    └─LayerNorm (norm_attention)             [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "│    │    └─MultiheadAttention (attention)         --                   [16, 17, 192]        148,224              True\n",
       "│    │    └─LayerNorm (norm_feed_forward)          [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "│    │    └─Sequential (feed_forward)              [16, 17, 192]        [16, 17, 192]        295,872              True\n",
       "├─Linear (mlp_head)                                [16, 192]            [16, 2]              386                  True\n",
       "==================================================================================================================================\n",
       "Total params: 930,626\n",
       "Trainable params: 930,626\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 10.09\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 6.24\n",
       "Params size (MB): 2.52\n",
       "Estimated Total Size (MB): 8.96\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_embed = slice_flat\n",
    "\n",
    "vit_layer = VIT(slice_input_size=slice_flat,\n",
    "                slice_embed_size=slice_embed,\n",
    "                img_slices=slices,\n",
    "                num_classes=2,\n",
    "                num_heads=12,\n",
    "                num_encoders=2,\n",
    "                emb_dropout=0.1,\n",
    "                enc_dropout=0.1,\n",
    "                lr=1e-4, min_lr=4e-5,\n",
    "                weight_decay=0.1,\n",
    "                epochs=200)\n",
    "\n",
    "vit_output = vit_layer (tr_img_t)\n",
    "\n",
    "print(\"Input Shape: \", tr_img_t.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=vit_layer,\n",
    "        input_size=(16, 16, 192),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1= torch.tensor([[[1,2],[3,4],[5,6]]])\n",
    "# print(a1.size())\n",
    "# a2= torch.tensor([[[10,20],[30,40]]])\n",
    "# torch.bmm(a1,a2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
