{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from dataset_process import dataset_to_df, search_df\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/CIFAK'\n",
    "relative_paths = [\"/train/REAL\", \"/train/FAKE\", \"/test/REAL\", \"/test/FAKE\"]\n",
    "paths_classes = [\"REAL\", \"FAKE\", \"REAL\", \"FAKE\"]\n",
    "\n",
    "# path = '../../data/meso_data'\n",
    "# relative_paths= [\"/Real\", \"/DeepFake\"]\n",
    "# paths_classes=['REAL',\"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all, df_train, df_val, df_test, classes_stats = dataset_to_df(\n",
    "    path, relative_paths, paths_classes, 0.8, 0.19, 0.01)\n",
    "\n",
    "classes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First and Last Elements in the Whole dataset\")\n",
    "df_all.iloc[[0,-1]]\n",
    "# print(df_all.iloc[[0,-1]].to_markdown(headers='keys', tablefmt='psql'))\n",
    "# print(\"\")\n",
    "# print(\"First and Last Elements in the Training dataset\")\n",
    "# print(df_train.iloc[[0,-1]].to_markdown(headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_horizontal_slices= 4\n",
    "images_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = read_image(df_all.iloc[0, 0]).size()\n",
    "\n",
    "slice_width = img_shape[1]//Img_horizontal_slices\n",
    "total_img_slices = Img_horizontal_slices**2\n",
    "\n",
    "print(f\"slice_width: {slice_width} pixels\")\n",
    "print(\"\")\n",
    "print(\"Image shape: \", img_shape)\n",
    "print(\n",
    "    f\"Image will be divided into: {Img_horizontal_slices} x {Img_horizontal_slices} = {total_img_slices} slices each with shape {(img_shape[0],slice_width,slice_width)}\")\n",
    "print(\n",
    "    f\"Target Shape of the final flattened image: {total_img_slices} x {img_shape[0]*slice_width**2} \")\n",
    "print(\"\")\n",
    "print(f\"Feed ({images_batch}) Images to the Dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # c: color channels\n",
    "\n",
    "        # h: desired slice height (pixels)\n",
    "        # w: desired slice width (pixels)\n",
    "\n",
    "        # row: No. of vertical slices\n",
    "        # col: No. of horizontal slices\n",
    "\n",
    "        # b: Batch of Images\n",
    "\n",
    "        sliced_Img = rearrange(\n",
    "            img, 'c (row h) (col w) -> row col c h w', h=slice_width, w=slice_width)\n",
    "\n",
    "        return sliced_Img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sliced_img = self.slice(img)\n",
    "        sliced_flattened_img = rearrange(\n",
    "            sliced_img, 'row col c h w -> (row col) (c h w)')\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((64, 64)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        \n",
    "        self.annotation = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 0])\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 4], dtype=torch.float64)\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
