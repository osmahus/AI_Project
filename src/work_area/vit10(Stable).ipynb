{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from dataset_process import dataset_to_df, search_df\n",
    "from engine import train_fn , eval_fn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import albumentations as alb\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import timm\n",
    "import wandb\n",
    "import time\n",
    "from utils import progress_bar\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usewandb = False\n",
    "resume=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32  # 384 #for timm vit\n",
    "patch_size = 8\n",
    "trans_img_resize = 32\n",
    "embed_size=3*patch_size**2\n",
    "mlp_dim= 512\n",
    "train_batch_size = 512\n",
    "valid_batch_size = 100\n",
    "num_classes=10\n",
    "encoder_depth=6\n",
    "attention_heads=12\n",
    "\n",
    "print(f'Flattened dimension size(of a patch): {3*patch_size**2}')\n",
    "print(f'Embedding Size: {embed_size}')\n",
    "print(f'Output MLP size: {mlp_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, emb_dropout = 0., dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618]\n",
    "norm_std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandAugment(2, 14),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(trans_img_resize),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(trans_img_resize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root='../../data/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=NUM_WORKERS )\n",
    "\n",
    "valid_set = torchvision.datasets.CIFAR10(root='../../data/CIFAR10', train=False, download=True, transform=transform_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=valid_batch_size, shuffle=False, num_workers=NUM_WORKERS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_batch,label = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch of Images Shape: {sample_img_batch.size()}\")\n",
    "print(f\"Batch of Images Shape: {label.size()}\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # c: color channels\n",
    "\n",
    "        # h: desired slice height (pixels)\n",
    "        # w: desired slice width (pixels)\n",
    "\n",
    "        # row: No. of vertical slices\n",
    "        # col: No. of horizontal slices\n",
    "\n",
    "        # b: Batch of Images\n",
    "\n",
    "        sliced_Img = rearrange(\n",
    "            img, 'c (row h) (col w) -> row col c h w', h=self.slice_width, w=self.slice_width)\n",
    "\n",
    "        return sliced_Img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sliced_img = self.slice(img)\n",
    "        sliced_flattened_img = rearrange(\n",
    "            sliced_img, 'row col c h w -> (row col) (c h w)')\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(train_loader, index):\n",
    "\n",
    "    img, label = train_loader.dataset[index]\n",
    "\n",
    "    channels, img_H, img_W = img.size()\n",
    "    \n",
    "    batch = train_loader.batch_size\n",
    "\n",
    "    return img, label, channels, img_H, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img)\n",
    "    print(\"Sliced Image Shape: \", sliced_img.shape)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(8, 4))\n",
    "    subfigs = fig.subfigures(3, 1, height_ratios=[1., 1.5, 1.], hspace=0.05, squeeze='True')\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    img_a = rearrange(img, \"c h w -> h w c\").numpy()\n",
    "    \n",
    "    axs0.imshow(img_a)\n",
    "    axs0.axis('off')\n",
    "    # subfigs[0].suptitle('Input Image', fontsize=10)\n",
    "\n",
    "    grid1 = ImageGrid(subfigs[1], 111, nrows_ncols=(sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid1):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = rearrange(sliced_img[row][column], \"c h w -> h w c\").numpy()\n",
    "        \n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[1].suptitle('Slices', fontsize=10)\n",
    "\n",
    "    grid2 = ImageGrid(subfigs[2], 111, nrows_ncols=(1, sliced_img.size(0)*sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid2):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[2].suptitle('Position', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, _ = train_set[10]\n",
    "sample_img_tr, sample_img_label, ch, img_size, batch_size = img_data(train_loader, 2)\n",
    "print(f\"Input Image Shape: {sample_img_tr.size()}\")\n",
    "img_plot(sample_img, patch_size)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "slice_embed = embed_size\n",
    "print(\"slice_embed\", slice_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=slice_embed,\n",
    "    depth=encoder_depth,\n",
    "    heads=attention_heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    emb_dropout=0.1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# model = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
    "# model.head = nn.Linear(model.head.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_output = model(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape: \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path=\"../weights/vit10/checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=base_lr)  \n",
    "    \n",
    "# use cosine scheduling\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if resume:\n",
    "#     # Load checkpoint.\n",
    "#     print('==> Resuming from checkpoint..')\n",
    "#     # assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "#     checkpoint = torch.load('../weights/vit10/checkpoint/vit-ckpt.t7')\n",
    "#     model.load_state_dict(checkpoint['model'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model,epoch,epochs):\n",
    "#     print(f'\\n epoch: {epoch}/{epochs}')\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "#     for batch_idx, (inputs, targets) in loop:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "#         # Zero gradients for every batch\n",
    "#         optimizer.zero_grad()\n",
    "#         # Backward path (Gradient)\n",
    "#         loss.backward()\n",
    "#         # Optimizer(Adam) Step\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += targets.size(0)\n",
    "#         correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#         avg_tr_loss=train_loss/(batch_idx+1)\n",
    "#         avg_tr_accuracy=100.*correct/total\n",
    "\n",
    "#         loop.set_description(f\"Training--Epoch [{epoch+1}/{epochs}]\")\n",
    "#         loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "\n",
    "#     return avg_tr_loss\n",
    "\n",
    "# def validate(model,epoch,epochs,model_state_path):\n",
    "#     global best_acc\n",
    "#     model.eval()\n",
    "#     valid_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         loop = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "#         for batch_idx, (inputs, targets) in loop:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#             valid_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += targets.size(0)\n",
    "#             correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#             avg_valid_loss=valid_loss/(batch_idx+1)\n",
    "#             avg_valid_accuracy=100.*correct/total\n",
    "            \n",
    "#             loop.set_description(f\"Validation--Epoch [{epoch+1}/{epochs}]\")\n",
    "#             loop.set_postfix(loss=avg_valid_loss, acc=avg_valid_accuracy)\n",
    "\n",
    "#             # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#             #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "#     # Save checkpoint.\n",
    "#     if avg_valid_accuracy > best_acc:\n",
    "#         print('Saving..')\n",
    "#         state = {\"model\": model.state_dict(),\n",
    "#                  \"optimizer\": optimizer.state_dict()}\n",
    "\n",
    "#         torch.save(state,model_state_path+f'vit_{patch_size}_epoch_{epoch}')\n",
    "#         best_acc = avg_valid_accuracy\n",
    "\n",
    "\n",
    "#     return valid_loss, avg_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "list_loss = []\n",
    "list_acc = []\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    \n",
    "    print(f'\\n epoch: {epoch+1}/{n_epochs}')\n",
    "\n",
    "    ##########################################################\n",
    "    model.train(True)\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop1 = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in loop1:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        # Backward path (Gradient)\n",
    "        loss.backward()\n",
    "        # Optimizer(Adam) Step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_tr_loss=train_loss/(batch_idx+1)\n",
    "        avg_tr_accuracy=100.*correct/total\n",
    "\n",
    "        loop1.set_description(f\"Train--Epoch [{epoch+1}/{n_epochs}]\")\n",
    "        loop1.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "\n",
    "    ##########################################################\n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        loop2 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "        for batch_idx, (inputs, targets) in loop2:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            avg_valid_loss=valid_loss/(batch_idx+1)\n",
    "            avg_valid_accuracy=100.*correct/total\n",
    "            \n",
    "            loop2.set_description(f\"Valid--Epoch [{epoch+1}/{n_epochs}]\")\n",
    "            loop2.set_postfix(loss=avg_valid_loss, acc=avg_valid_accuracy)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if avg_valid_accuracy > best_acc:\n",
    "        best_acc = avg_valid_accuracy\n",
    "\n",
    "        print('Saving..')\n",
    "        state = {\"model\": model.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(state,model_state_path+f'vit_{patch_size}_epoch_{epoch}')\n",
    "        \n",
    "    ##########################################################\n",
    "    \n",
    "    scheduler.step() # step cosine scheduling\n",
    "    \n",
    "    list_loss.append(avg_valid_loss)\n",
    "    list_acc.append(avg_valid_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0  # best test accuracy\n",
    "# start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# list_loss = []\n",
    "# list_acc = []\n",
    "\n",
    "# model.cuda()\n",
    "# for epoch in range(start_epoch, n_epochs):\n",
    "#     start = time.time()\n",
    "#     tr_loss = train(model,epoch,n_epochs)\n",
    "#     val_loss, acc = validate(model,epoch,n_epochs,model_state_path)\n",
    "    \n",
    "#     scheduler.step() # step cosine scheduling\n",
    "    \n",
    "#     list_loss.append(val_loss)\n",
    "#     list_acc.append(acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timmvenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
