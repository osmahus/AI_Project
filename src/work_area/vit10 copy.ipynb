{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from dataset_process import dataset_to_df, search_df\n",
    "from engine import train_fn , eval_fn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import albumentations as alb\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import timm\n",
    "import wandb\n",
    "import time\n",
    "from utils import progress_bar\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2+cu121\n",
      "GPU Card: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Torch is using device: cuda:0\n",
      "CPU Count: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usewandb = False\n",
    "resume=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened dimension size(of a patch): 192\n",
      "Embedding Size: 192\n",
      "Output MLP size: 512\n"
     ]
    }
   ],
   "source": [
    "img_size = 32  # 384 #for timm vit\n",
    "patch_size = 8\n",
    "trans_img_resize = 32\n",
    "embed_size=3*patch_size**2\n",
    "mlp_dim= 512\n",
    "train_batch_size = 512\n",
    "valid_batch_size = 100\n",
    "num_classes=10\n",
    "encoder_depth=6\n",
    "attention_heads=12\n",
    "\n",
    "print(f'Flattened dimension size(of a patch): {3*patch_size**2}')\n",
    "print(f'Embedding Size: {embed_size}')\n",
    "print(f'Output MLP size: {mlp_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, emb_dropout = 0., dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618]\n",
    "norm_std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandAugment(2, 14),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(trans_img_resize),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(trans_img_resize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root='../../data/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=NUM_WORKERS )\n",
    "\n",
    "valid_set = torchvision.datasets.CIFAR10(root='../../data/CIFAR10', train=False, download=True, transform=transform_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=valid_batch_size, shuffle=False, num_workers=NUM_WORKERS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Images Shape: torch.Size([512, 3, 32, 32])\n",
      "Batch of Images Shape: torch.Size([512])\n",
      "tensor([8, 0, 7, 3, 4, 5, 8, 2, 0, 1, 4, 9, 6, 8, 9, 9, 1, 7, 7, 9, 2, 3, 7, 2,\n",
      "        1, 1, 9, 3, 8, 2, 0, 4, 1, 3, 9, 2, 5, 8, 4, 2, 8, 0, 5, 3, 9, 1, 1, 8,\n",
      "        0, 0, 5, 8, 5, 5, 4, 4, 6, 3, 3, 3, 0, 5, 3, 1, 5, 5, 5, 0, 8, 6, 0, 5,\n",
      "        8, 8, 5, 8, 7, 4, 1, 0, 0, 1, 9, 7, 6, 9, 8, 4, 8, 9, 4, 2, 9, 1, 9, 3,\n",
      "        7, 4, 0, 7, 9, 5, 0, 9, 8, 3, 4, 1, 9, 2, 2, 5, 1, 0, 7, 3, 8, 9, 8, 0,\n",
      "        2, 9, 9, 4, 1, 2, 2, 3, 3, 4, 2, 8, 1, 5, 0, 1, 5, 9, 4, 5, 4, 6, 5, 5,\n",
      "        9, 6, 7, 6, 5, 4, 3, 5, 4, 8, 3, 2, 8, 0, 0, 6, 5, 0, 9, 8, 4, 3, 2, 4,\n",
      "        4, 8, 7, 6, 2, 6, 5, 5, 0, 0, 8, 9, 1, 7, 2, 3, 9, 0, 5, 0, 7, 9, 0, 5,\n",
      "        5, 6, 7, 2, 8, 8, 7, 4, 1, 2, 3, 5, 6, 3, 9, 9, 7, 3, 7, 6, 8, 4, 5, 1,\n",
      "        1, 9, 7, 1, 0, 3, 0, 4, 7, 2, 9, 7, 9, 5, 4, 5, 9, 1, 3, 4, 4, 7, 7, 9,\n",
      "        3, 0, 7, 7, 3, 7, 6, 5, 5, 5, 3, 3, 9, 4, 5, 4, 0, 0, 8, 5, 8, 2, 8, 6,\n",
      "        9, 1, 7, 8, 0, 7, 7, 5, 9, 0, 3, 9, 1, 9, 3, 9, 8, 5, 7, 2, 8, 3, 8, 7,\n",
      "        2, 1, 0, 3, 2, 5, 7, 2, 3, 4, 4, 8, 6, 3, 9, 8, 2, 6, 0, 9, 1, 4, 6, 8,\n",
      "        7, 3, 4, 5, 3, 2, 4, 9, 5, 4, 2, 3, 7, 6, 7, 6, 8, 4, 3, 8, 4, 8, 2, 5,\n",
      "        0, 2, 7, 2, 2, 7, 0, 8, 6, 5, 1, 6, 1, 7, 9, 3, 0, 5, 1, 9, 2, 7, 2, 7,\n",
      "        8, 5, 7, 2, 3, 1, 8, 9, 3, 5, 2, 9, 6, 9, 5, 3, 4, 2, 7, 8, 3, 7, 0, 7,\n",
      "        4, 7, 8, 9, 3, 6, 7, 8, 5, 9, 8, 6, 5, 7, 3, 7, 9, 1, 1, 3, 8, 0, 2, 3,\n",
      "        4, 0, 1, 6, 8, 9, 0, 5, 4, 4, 5, 5, 5, 4, 7, 9, 3, 2, 6, 4, 2, 8, 6, 3,\n",
      "        5, 8, 5, 3, 7, 6, 7, 2, 2, 2, 7, 3, 9, 8, 7, 0, 9, 1, 8, 8, 0, 7, 6, 4,\n",
      "        0, 8, 1, 7, 0, 7, 4, 0, 3, 7, 8, 3, 7, 4, 9, 0, 3, 6, 1, 3, 4, 9, 1, 2,\n",
      "        7, 3, 1, 0, 1, 9, 5, 4, 7, 1, 9, 8, 0, 4, 3, 5, 9, 2, 1, 2, 2, 6, 3, 1,\n",
      "        6, 6, 7, 7, 6, 0, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "sample_img_batch,label = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch of Images Shape: {sample_img_batch.size()}\")\n",
    "print(f\"Batch of Images Shape: {label.size()}\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # c: color channels\n",
    "\n",
    "        # h: desired slice height (pixels)\n",
    "        # w: desired slice width (pixels)\n",
    "\n",
    "        # row: No. of vertical slices\n",
    "        # col: No. of horizontal slices\n",
    "\n",
    "        # b: Batch of Images\n",
    "\n",
    "        sliced_Img = rearrange(\n",
    "            img, 'c (row h) (col w) -> row col c h w', h=self.slice_width, w=self.slice_width)\n",
    "\n",
    "        return sliced_Img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sliced_img = self.slice(img)\n",
    "        sliced_flattened_img = rearrange(\n",
    "            sliced_img, 'row col c h w -> (row col) (c h w)')\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(train_loader, index):\n",
    "\n",
    "    img, label = train_loader.dataset[index]\n",
    "\n",
    "    channels, img_H, img_W = img.size()\n",
    "    \n",
    "    batch = train_loader.batch_size\n",
    "\n",
    "    return img, label, channels, img_H, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img)\n",
    "    print(\"Sliced Image Shape: \", sliced_img.shape)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(8, 4))\n",
    "    subfigs = fig.subfigures(3, 1, height_ratios=[1., 1.5, 1.], hspace=0.05, squeeze='True')\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    img_a = rearrange(img, \"c h w -> h w c\").numpy()\n",
    "    \n",
    "    axs0.imshow(img_a)\n",
    "    axs0.axis('off')\n",
    "    # subfigs[0].suptitle('Input Image', fontsize=10)\n",
    "\n",
    "    grid1 = ImageGrid(subfigs[1], 111, nrows_ncols=(sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid1):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = rearrange(sliced_img[row][column], \"c h w -> h w c\").numpy()\n",
    "        \n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[1].suptitle('Slices', fontsize=10)\n",
    "\n",
    "    grid2 = ImageGrid(subfigs[2], 111, nrows_ncols=(1, sliced_img.size(0)*sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid2):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[2].suptitle('Position', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape: torch.Size([3, 32, 32])\n",
      "Sliced Image Shape:  torch.Size([4, 4, 3, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "slice_embed 192\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 65 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img, _ = train_set[10]\n",
    "sample_img_tr, sample_img_label, ch, img_size, batch_size = img_data(train_loader, 2)\n",
    "print(f\"Input Image Shape: {sample_img_tr.size()}\")\n",
    "img_plot(sample_img, patch_size)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "slice_embed = embed_size\n",
    "print(\"slice_embed\", slice_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=slice_embed,\n",
    "    depth=encoder_depth,\n",
    "    heads=attention_heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    emb_dropout=0.1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# model = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
    "# model.head = nn.Linear(model.head.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([512, 3, 32, 32])\n",
      "Output Shape:  torch.Size([512, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "ViT (ViT)                                          [512, 3, 32, 32]     [512, 10]            3,456                True\n",
       "├─Sequential (to_patch_embedding)                  [512, 3, 32, 32]     [512, 16, 192]       --                   True\n",
       "│    └─Rearrange (0)                               [512, 3, 32, 32]     [512, 16, 192]       --                   --\n",
       "│    └─Linear (1)                                  [512, 16, 192]       [512, 16, 192]       37,056               True\n",
       "├─Dropout (dropout)                                [512, 17, 192]       [512, 17, 192]       --                   --\n",
       "├─Transformer (transformer)                        [512, 17, 192]       [512, 17, 192]       --                   True\n",
       "│    └─ModuleList (layers)                         --                   --                   --                   True\n",
       "│    │    └─ModuleList (0)                         --                   --                   788,096              True\n",
       "│    │    └─ModuleList (1)                         --                   --                   788,096              True\n",
       "│    │    └─ModuleList (2)                         --                   --                   788,096              True\n",
       "│    │    └─ModuleList (3)                         --                   --                   788,096              True\n",
       "│    │    └─ModuleList (4)                         --                   --                   788,096              True\n",
       "│    │    └─ModuleList (5)                         --                   --                   788,096              True\n",
       "├─Identity (to_latent)                             [512, 192]           [512, 192]           --                   --\n",
       "├─Sequential (mlp_head)                            [512, 192]           [512, 10]            --                   True\n",
       "│    └─LayerNorm (0)                               [512, 192]           [512, 192]           384                  True\n",
       "│    └─Linear (1)                                  [512, 192]           [512, 10]            1,930                True\n",
       "==================================================================================================================================\n",
       "Total params: 4,771,402\n",
       "Trainable params: 4,771,402\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.44\n",
       "==================================================================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 1510.78\n",
       "Params size (MB): 19.07\n",
       "Estimated Total Size (MB): 1536.14\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_output = model(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape: \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path=\"../weights/vit10/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=base_lr)  \n",
    "    \n",
    "# use cosine scheduling\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    # assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('../weights/vit10/checkpoint/vit-ckpt.t7')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,epochs):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in loop:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        # Backward path (Gradient)\n",
    "        loss.backward()\n",
    "        # Optimizer(Adam) Step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_tr_loss=train_loss/(batch_idx+1)\n",
    "        avg_tr_accuracy=100.*correct/total\n",
    "\n",
    "        loop.set_description(f\"Training--Epoch [{epoch+1}/{epochs}]\")\n",
    "        loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "\n",
    "    return avg_tr_loss\n",
    "\n",
    "def validate(epoch,epochs,model_state_path):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        loop = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "        for batch_idx, (inputs, targets) in loop:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            avg_valid_loss=valid_loss/(batch_idx+1)\n",
    "            avg_valid_accuracy=100.*correct/total\n",
    "            \n",
    "            loop.set_description(f\"Validation--Epoch [{epoch+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss=avg_valid_loss, acc=avg_valid_accuracy)\n",
    "\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if avg_valid_accuracy > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\"model\": model.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict()}\n",
    "\n",
    "        torch.save(state,model_state_path+f'vit_{patch_size}_epoch_{epoch}')\n",
    "        best_acc = avg_valid_accuracy\n",
    "\n",
    "\n",
    "    return valid_loss, avg_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "\n",
    "# def train(epoch):\n",
    "#     print('\\nEpoch: %d' % epoch)\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         # Train with amp\n",
    "#         with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += targets.size(0)\n",
    "#         correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                      % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "#     return train_loss/(batch_idx+1)\n",
    "\n",
    "# # Validation\n",
    "\n",
    "\n",
    "# def test(epoch):\n",
    "#     global best_acc\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#             test_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += targets.size(0)\n",
    "#             correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "#     # Save checkpoint.\n",
    "#     acc = 100.*correct/total\n",
    "#     if acc > best_acc:\n",
    "#         print('Saving..')\n",
    "#         state = {\"model\": model.state_dict(),\n",
    "#                  \"optimizer\": optimizer.state_dict(),\n",
    "#                  \"scaler\": scaler.state_dict()}\n",
    "#         # if not os.path.isdir('checkpoint'):\n",
    "#         #     os.mkdir('checkpoint')\n",
    "#         torch.save(state,f'../weights/vit10/checkpoint/vit-{patch_size}-ckpt.t7')\n",
    "#         best_acc = acc\n",
    "\n",
    "#     os.makedirs(\"log\", exist_ok=True)\n",
    "    # content = time.ctime() + ' ' + \\\n",
    "    #     f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, val loss: {test_loss:.5f}, acc: {(acc):.5f}'\n",
    "    # print(content)\n",
    "    # with open(f'log/log_vit_patch{patch_size}.txt', 'a') as appender:\n",
    "    #     appender.write(content + \"\\n\")\n",
    "    # return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs):\n\u001b[0;32m     14\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 15\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     val_loss, acc \u001b[38;5;241m=\u001b[39m validate(epoch,n_epochs,model_state_path)\n\u001b[0;32m     18\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# step cosine scheduling\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, epochs)\u001b[0m\n\u001b[0;32m      5\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Code\\python\\timmvenv11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Code\\python\\timmvenv11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Code\\python\\timmvenv11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "list_loss = []\n",
    "list_acc = []\n",
    "\n",
    "if usewandb:\n",
    "    watermark = f\"vit_lr{base_lr}\"\n",
    "    wandb.init(project=\"cifar10-challange\",name=watermark)\n",
    "    wandb.watch(model)\n",
    "\n",
    "model.cuda()\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    start = time.time()\n",
    "    tr_loss = train(epoch,n_epochs)\n",
    "    val_loss, acc = validate(epoch,n_epochs,model_state_path)\n",
    "    \n",
    "    scheduler.step() # step cosine scheduling\n",
    "    \n",
    "    list_loss.append(val_loss)\n",
    "    list_acc.append(acc)\n",
    "    \n",
    "    # Log training..\n",
    "    if usewandb:\n",
    "        wandb.log({'epoch': epoch, 'train_loss': trainloss, 'val_loss': val_loss, \"val_acc\": acc, \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"epoch_time\": time.time()-start})\n",
    "\n",
    "    # Write out csv..\n",
    "    with open(f'../weights/vit10/log_vit_patch{patch_size}.csv', 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(list_loss) \n",
    "        writer.writerow(list_acc) \n",
    "    print(list_loss)\n",
    "\n",
    "# writeout wandb\n",
    "if usewandb:\n",
    "    wandb.save(f\"../weights/vit10/wandb/wandb_vit.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timmvenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
