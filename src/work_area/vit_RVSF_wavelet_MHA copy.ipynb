{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "from helpers.DatasetProcess import dataset_to_df, search_df\n",
    "from helpers.PlotExtension import norm_to_plot, plot_img_grid,img_plot\n",
    "from helpers.WaveletPacketV3 import wpt_dec, plot_wpt_nodes, plot_wpt_fun\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/real-vs-fake'\n",
    "relative_paths = [\"/train/real\", \"/train/fake\", \"/test/real\", \"/test/fake\", \"/valid/real\", \"/valid/fake\"]\n",
    "paths_classes = [\"REAL\", \"FAKE\", \"REAL\", \"FAKE\", \"REAL\", \"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_all = pd.read_csv(f'{path}/df_all.csv')\n",
    "    df_train = pd.read_csv(f'{path}/df_train.csv')\n",
    "    df_val = pd.read_csv(f'{path}/df_val.csv')\n",
    "    df_test = pd.read_csv(f'{path}/df_test.csv')\n",
    "    classes_stats = pd.read_csv(f'{path}/classes_stats.csv')\n",
    "except FileNotFoundError:\n",
    "    df_all, df_train, df_val, df_test, classes_stats = dataset_to_df(\n",
    "        path, relative_paths, paths_classes, 0.8, 0.14, 0.06)\n",
    "    df_all.to_csv(f'{path}/df_all.csv', index=False)\n",
    "    df_train.to_csv(f'{path}/df_train.csv', index=False)\n",
    "    df_val.to_csv(f'{path}/df_val.csv', index=False)\n",
    "    df_test.to_csv(f'{path}/df_test.csv', index=False)\n",
    "    classes_stats.to_csv(f'{path}/classes_stats.csv', index=False)\n",
    "\n",
    "classes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_img_idx, f_img_idx = 6, -3\n",
    "step=64\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=(8, 4))\n",
    "fig.suptitle('Dataset Sample Images (Source 140k Real and Fake Faces)')\n",
    "\n",
    "r_img=plt.imread(df_all.iloc[r_img_idx, 0])\n",
    "f_img=plt.imread(df_all.iloc[f_img_idx, 0])\n",
    "\n",
    "axs[0].imshow(r_img)\n",
    "axs[0].set(xlabel=df_all.iloc[r_img_idx, 2])\n",
    "axs[0].set(xticks=np.arange(0, r_img.shape[1]+1, step=step), yticks=np.arange(0, r_img.shape[0]+1, step=step))\n",
    "\n",
    "axs[1].imshow(f_img)\n",
    "axs[1].set(xlabel=df_all.iloc[f_img_idx, 2])\n",
    "axs[1].set(xticks=np.arange(0, f_img.shape[1]+1, step=step), yticks=np.arange(0, f_img.shape[0]+1, step=step))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_phase = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vit\"\n",
    "add_extra_epochs = True  # if True then continue for extra epochs from the starting epoch\n",
    "\n",
    "# If True load a pretrained model, and start_epoch = epoch of the model . Else if False then start_epoch = 0\n",
    "start_from_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path = f\"../../../weights/{model_type}_wpt_rvsf/checkpoint/\"\n",
    "\n",
    "# model_file_name = f\"vit_wpt_rvsf_ph1_lastEpoch199_acc62.91\"  # file name of the saved model\n",
    "\n",
    "# checkpoint = torch.load(model_state_path+model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_from_model:\n",
    "    training_phase_lst = checkpoint['training_phase_lst']\n",
    "    training_phase = training_phase_lst[-1]+1\n",
    "\n",
    "print(training_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelet Packet Transform Hyper Parameters\n",
    "wpt_fun = 'haar'\n",
    "wpt_level = 1\n",
    "features = ((2**wpt_level)**2)\n",
    "patch_channels = 3 # Features of every color channel\n",
    "print(patch_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_trans_norm_mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618]\n",
    "img_trans_norm_std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_img_resize = 256\n",
    "img_size = int(in_img_resize/(2**wpt_level))  # Input image height/width in pixels\n",
    "# The height/width of a patch in pixels (patch is a slice of an image)\n",
    "patch_size = 8\n",
    "\n",
    "image_patch_flat_size = (patch_channels) * patch_size**2  # 16 Channels x Patch_height**2\n",
    "# embed_size = image_patch_flat_size\n",
    "embed_size = features * image_patch_flat_size\n",
    "mlp_dim = 384  # dim of the last mlp classifier\n",
    "\n",
    "train_batch_size = 50  # https://www.youtube.com/watch?v=Owm1H0ukjS4\n",
    "valid_batch_size = 50\n",
    "\n",
    "num_classes = 2\n",
    "encoder_depth = 2\n",
    "attention_heads = 8\n",
    "\n",
    "print(f'Flattened dimension size(of a patch): {image_patch_flat_size}')\n",
    "print(f'Embedding Size: {embed_size}')\n",
    "print(f'Output MLP size: {mlp_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_extra_epochs = 10\n",
    "max_epochs = 200\n",
    "\n",
    "start_epoch = checkpoint['epoch'] if start_from_model else 0\n",
    "end_epoch = start_epoch + n_extra_epochs if add_extra_epochs else max_epochs\n",
    "delta_epochs = end_epoch-start_epoch\n",
    "\n",
    "print(f\"Train another {delta_epochs} epochs after epoch {start_epoch} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.01         # Base Learning rate 0.1 for SGD , 0.0001 for Adam\n",
    "momentum = 0.9          # Momentum for SGD\n",
    "weight_decay = 0.001     # Weight decay for Adam\n",
    "\n",
    "\n",
    "loss_algo = CrossEntropyLoss\n",
    "\n",
    "optimize_algo = SGD # Adam\n",
    "optimize_args = {\"lr\": base_lr,\n",
    "                #  \"weight_decay\": weight_decay ,\n",
    "                #  \"momentum\": momentum  # comment momentum for Adam\n",
    "                 }\n",
    "\n",
    "use_scheduler = False\n",
    "schedule_algo = CosineAnnealingLR\n",
    "schedule_args = {\"T_max\": delta_epochs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    \"training_phase\": training_phase,\n",
    "    \"model_type\": model_type,\n",
    "    \"wpt_fun\": wpt_fun,\n",
    "    \"wpt_level\": wpt_level,\n",
    "    \"patch_channels\": patch_channels,\n",
    "    \"img_trans_norm_mean\": img_trans_norm_mean,\n",
    "    \"img_trans_norm_std\": img_trans_norm_std,\n",
    "    \"in_img_resize\":in_img_resize,\n",
    "    \"img_size\": img_size,\n",
    "    \"patch_size\": patch_size,\n",
    "    \"image_patch_flat_size\": image_patch_flat_size,\n",
    "    \"embed_size\": embed_size,\n",
    "    \"mlp_dim\": mlp_dim,\n",
    "    \"train_batch_size\": train_batch_size,\n",
    "    \"valid_batch_size\": valid_batch_size,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"encoder_depth\": encoder_depth,\n",
    "    \"attention_heads\": attention_heads,\n",
    "    \"n_extra_epochs \": n_extra_epochs,\n",
    "    \"base_lr\": base_lr,\n",
    "    \"momentum\": momentum,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"loss_algo\": loss_algo,\n",
    "    \"optimize_algo\": optimize_algo,\n",
    "    \"optimize_args\": optimize_args,\n",
    "    \"use_scheduler\": use_scheduler,\n",
    "    \"schedule_algo\": schedule_algo,\n",
    "    \"schedule_args\": schedule_args\n",
    "}\n",
    "\n",
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "# replaced the attention mechanism with MHA from Sebastian Raschka\n",
    "\n",
    "# helpers\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "\n",
    "class Patch_Embed(nn.Module):\n",
    "    def __init__(self, patch_height, patch_width, patch_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Sequential(\n",
    "            # Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "\n",
    "            # This rearrangement is suitable for the wavelet packet features\n",
    "            # torch.Size([200, 16, 3, 32, 32])\n",
    "            # Rearrange('b f c (h p1) (w p2) -> b (f h w) (c p1 p2)',\n",
    "            #           p1=patch_height, p2=patch_width),\n",
    "            Rearrange('b f c (h p1) (w p2) -> b (h w) (f c p1 p2)',\n",
    "                      p1=patch_height, p2=patch_width),\n",
    "            nn.Linear(patch_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.patch_embed(x)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# inspired by Sebstian Raschka with modifications\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_of_head: int = 12, Dropout=0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.inner_dim = embed_dim*2\n",
    "        self.num_of_head = num_of_head\n",
    "        self.dim_per_head = self.inner_dim // self.num_of_head\n",
    "        self.Wq = nn.Linear(in_features=embed_dim, out_features=self.inner_dim)\n",
    "        self.Wk = nn.Linear(in_features=embed_dim, out_features=self.inner_dim)\n",
    "        self.Wv = nn.Linear(in_features=embed_dim, out_features=self.inner_dim)\n",
    "\n",
    "        self.dropout = Dropout\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.inner_dim, out_features=embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        Q = rearrange(q, 'b s (h d) -> b h s d',\n",
    "                      h=self.num_of_head, d=self.dim_per_head)\n",
    "        K = rearrange(k, 'b s (h d) -> b h s d',\n",
    "                      h=self.num_of_head, d=self.dim_per_head)\n",
    "        V = rearrange(v, 'b s (h d) -> b h s d',\n",
    "                      h=self.num_of_head, d=self.dim_per_head)\n",
    "\n",
    "        use_dropout = 0.0 if not self.training else self.dropout\n",
    "\n",
    "        attention = F.scaled_dot_product_attention(\n",
    "            Q, K, V, dropout_p=use_dropout)\n",
    "\n",
    "        # Note:\n",
    "        # dimensions of tensors inside F.scaled_dot_product_attention as follows:\n",
    "        # Q(b h s d) . KT(b h d s) ==> b h s s\n",
    "        # weight(b h s s).V(b h s d) ==> attention (b h s d)\n",
    "\n",
    "        attention = rearrange(attention, 'b h s d -> b s (h d)')\n",
    "        attention = self.linear(attention)\n",
    "        return attention\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                # PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, MHA(embed_dim=dim,\n",
    "                        num_of_head=heads, Dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class vit(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, features, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3, dim_head=64, emb_dropout=0., dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        # num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        # patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = features * channels * patch_height * patch_width\n",
    "\n",
    "        assert pool in {\n",
    "            'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        # self.to_patch_embedding = nn.Sequential(\n",
    "        #     Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "        #               p1=patch_height, p2=patch_width),\n",
    "        #     nn.Linear(patch_dim, dim),\n",
    "        # )\n",
    "\n",
    "        self.to_patch_embedding = Patch_Embed(\n",
    "            patch_height, patch_width, patch_dim, dim)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "\n",
    "        self.annotation = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img = read_image(self.annotation.iloc[index, 0])\n",
    "        labels = torch.tensor(\n",
    "            self.annotation.iloc[index, 3], dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "        else:\n",
    "            img_t = img\n",
    "\n",
    "        # img, nodes_array, paths, features_rows, wp_fun, wp_name, node_shape, nodes_tensor\n",
    "        *_, nodes_tensor = wpt_dec(img_t, wpt_fun, wpt_level)\n",
    "        return img_t, nodes_tensor, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Note:\n",
    "    # We want to introduce augmentation for the dataset using transformations \n",
    "    # which doesn't change the real images into fake\n",
    "    # v2.CenterCrop(size=in_img_resize),\n",
    "    # v2.Resize(size=int(in_img_resize/2)),\n",
    "    transforms.ToPILImage(),\n",
    "    # v2.RandomAffine(degrees=(-90, 90), translate=(0.1, 0.3), scale=(0.75, 1)),\n",
    "    # transforms.ToTensor(),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(img_trans_norm_mean, img_trans_norm_std),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    # transforms.ToTensor(),\n",
    "    # v2.CenterCrop(size=in_img_resize),\n",
    "    # v2.Resize(size=int(in_img_resize/2)),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(img_trans_norm_mean, img_trans_norm_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "\n",
    "train_set  = Images_Dataset(df_train, transform_train)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=train_batch_size,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "valid_set  = Images_Dataset(df_val, transform_valid)\n",
    "\n",
    "valid_loader= DataLoader(dataset=valid_set ,\n",
    "                          batch_size=valid_batch_size,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of training batches:\", len(train_loader))\n",
    "print(\"Total number of validation batches:\", len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgt_bch, sample_nodes_tnsr_bch, label_bch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch of Images Shape: {sample_imgt_bch.size()}\")\n",
    "print(f\"Batch of WPT Nodes(features) Shape: {sample_nodes_tnsr_bch.size()}\")\n",
    "print(f\"Batch of Labels Shape: {label_bch.size()}\")\n",
    "# print(label_bch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_idx = 12\n",
    "\n",
    "sample_img = sample_imgt_bch[sample_img_idx, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_a,nodes, *_, wp_fun, wp_name, _, _ = wpt_dec(sample_img, wpt_fun, wpt_level)\n",
    "sample_img_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wpt_fun(sample_img, wpt_fun, wpt_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "\n",
    "ax.imshow(norm_to_plot(sample_img_a)[1])\n",
    "ax.set(xticks=np.arange(0, sample_img_a.shape[1]+1, step=32),\n",
    "       yticks=np.arange(0, sample_img_a.shape[0]+1, step=32))\n",
    "ax.set_title(f'Label: {\"Real\" if label_bch[sample_img_idx].item()==1 else \"Fake\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wpt_nodes(sample_img, wpt_fun, wpt_level,setticks1=32,setticks2=32,figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_plot(plt, nodes[0,:,:,:], patch_size, CHW_Image=False, figsize=(6, 8),axes_pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"vit\":\n",
    "    model = vit(\n",
    "        image_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        features=features,\n",
    "        num_classes=num_classes,\n",
    "        dim=embed_size,\n",
    "        depth=encoder_depth,\n",
    "        heads=attention_heads,\n",
    "        mlp_dim=mlp_dim,\n",
    "        emb_dropout=0.1,\n",
    "        dropout=0.1,\n",
    "        channels=patch_channels\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You must set the model_type variable to any of: vit ,....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nodes_tnsr_bch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(sample_nodes_tnsr_bch)\n",
    "\n",
    "print(\"Input Shape: \", sample_nodes_tnsr_bch.size())\n",
    "# print(\"Output Shape: \", output.size())\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=sample_nodes_tnsr_bch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = loss_algo()\n",
    "optimizer = optimize_algo(model.parameters(), **optimize_args)\n",
    "scheduler = schedule_algo(optimizer, **schedule_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_from_model:\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    if checkpoint['optimizer_lst'][-1]==optimize_algo:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    tr_loss_lst = checkpoint['tr_loss_lst']\n",
    "    tr_acc_lst = checkpoint['tr_acc_lst']\n",
    "    val_loss_lst = checkpoint['val_loss_lst']\n",
    "    val_acc_lst = checkpoint['val_acc_lst']\n",
    "    optimizer_lst = checkpoint['optimizer_lst']\n",
    "    running_lr_lst = checkpoint['running_lr_lst']\n",
    "    training_phase_lst = checkpoint['training_phase_lst']\n",
    "    hyper_parameters_lst = checkpoint['hyper_parameters_lst']\n",
    "\n",
    "else:\n",
    "    best_acc = 0\n",
    "\n",
    "    tr_loss_lst = []\n",
    "    tr_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "    optimizer_lst = []\n",
    "    running_lr_lst = []\n",
    "    training_phase_lst=[]\n",
    "    hyper_parameters_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "\n",
    "    print(f'\\n epoch: {epoch+1}/{end_epoch}')\n",
    "\n",
    "    ##########################################################\n",
    "    model.train(True)\n",
    "    tr_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_lr = 0\n",
    "\n",
    "    loop1 = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (*_, inputs, targets) in loop1:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        # Backward path\n",
    "        loss.backward()\n",
    "\n",
    "        running_lr += optimizer.param_groups[-1]['lr']\n",
    "\n",
    "        # Optimizer Step\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_tr_loss = tr_loss/(batch_idx+1)\n",
    "        avg_tr_accuracy = 100.*correct/total\n",
    "        avg_running_lr = running_lr/(batch_idx+1)\n",
    "\n",
    "        loop1.set_description(f\"Train--Epoch [{epoch+1}/{end_epoch}]\")\n",
    "        loop1.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "\n",
    "    ##########################################################\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        loop2 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "        for batch_idx, (*_, inputs, targets) in loop2:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss/(batch_idx+1)\n",
    "            avg_val_accuracy = 100.*correct/total\n",
    "\n",
    "            loop2.set_description(f\"Valid--Epoch [{epoch+1}/{end_epoch}]\")\n",
    "            loop2.set_postfix(loss=avg_val_loss, acc=avg_val_accuracy)\n",
    "    ##########################################################\n",
    "\n",
    "    tr_loss_lst.append(avg_tr_loss)\n",
    "    tr_acc_lst.append(avg_tr_accuracy)\n",
    "    val_loss_lst.append(avg_val_loss)\n",
    "    val_acc_lst.append(avg_val_accuracy)\n",
    "    optimizer_lst.append(type(optimizer).__name__)\n",
    "    running_lr_lst.append(avg_running_lr)\n",
    "    training_phase_lst.append(training_phase)\n",
    "    hyper_parameters_lst.append(hyper_parameters)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if avg_val_accuracy > best_acc:\n",
    "        best_acc = avg_val_accuracy\n",
    "\n",
    "        print('Saving..')\n",
    "        state = {\"model\": model.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict(),\n",
    "                 \"epoch\": epoch,\n",
    "                 \"best_acc\": best_acc,\n",
    "                 \"tr_loss_lst\": tr_loss_lst,\n",
    "                 \"tr_acc_lst\": tr_acc_lst,\n",
    "                 \"val_loss_lst\": val_loss_lst,\n",
    "                 \"val_acc_lst\": val_acc_lst,\n",
    "                 \"optimizer_lst\": optimizer_lst,\n",
    "                 \"running_lr_lst\": running_lr_lst,\n",
    "                 \"training_phase_lst\":training_phase_lst,\n",
    "                 \"hyper_parameters_lst\": hyper_parameters_lst\n",
    "                 }\n",
    "        torch.save(state, model_state_path +\n",
    "                   f'{model_type}_wpt_rvsf_ph{training_phase}_Epoch{epoch}_acc{best_acc:.2f}')\n",
    "\n",
    "    ##########################################################\n",
    "\n",
    "    if use_scheduler:\n",
    "        scheduler.step()  # step scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"tr_loss_lst\": tr_loss_lst,\n",
    "            \"tr_acc_lst\": tr_acc_lst,\n",
    "            \"val_loss_lst\": val_loss_lst,\n",
    "            \"val_acc_lst\": val_acc_lst,\n",
    "            \"optimizer_lst\": optimizer_lst,\n",
    "            \"running_lr_lst\": running_lr_lst,\n",
    "            \"training_phase_lst\":training_phase_lst,\n",
    "            \"hyper_parameters_lst\": hyper_parameters_lst\n",
    "            }\n",
    "torch.save(state, model_state_path +\n",
    "            f'{model_type}_wpt_rvsf_ph{training_phase}_lastEpoch{epoch}_acc{avg_val_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(tr_loss_lst )), tr_loss_lst , label='Train Loss')\n",
    "axs[0].plot(range(len(val_loss_lst)), val_loss_lst, label='Validation Loss')\n",
    "axs[0].set_ylim([0,2.5])\n",
    "\n",
    "axs[1].plot(range(len(tr_acc_lst)), tr_acc_lst, label='Train Accuracy')\n",
    "axs[1].plot(range(len(val_acc_lst)), val_acc_lst, label='Validation Accuracy')\n",
    "axs[1].set_ylim([40,80])\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Validation Accuracy is: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "axs.plot(tr_loss_lst, running_lr_lst, label=f'lr change for the {optimizer_lst[-1]} optimizer')\n",
    "axs.set_xlabel('Training_Loss')\n",
    "axs.set_ylabel('Running Learning Rate')\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timmvenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
