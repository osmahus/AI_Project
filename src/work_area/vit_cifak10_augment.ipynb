{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from helpers.dataset_process import dataset_to_df, search_df\n",
    "# from engine import train_fn, eval_fn\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "# from torch import Tensor\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# import albumentations as alb\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# from typing import Optional, Tuple\n",
    "# import timm\n",
    "# import wandb\n",
    "# import time\n",
    "# from utils import progress_bar\n",
    "# import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2+cu121\n",
      "GPU Card: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Torch is using device: cuda:0\n",
      "CPU Count: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"GPU Card: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, emb_dropout = 0., dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/CIFAK'\n",
    "relative_paths = [\"/train/REAL\", \"/train/FAKE\", \"/test/REAL\", \"/test/FAKE\"]\n",
    "paths_classes = [\"REAL\", \"FAKE\", \"REAL\", \"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REAL</th>\n",
       "      <th>FAKE</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>48000</td>\n",
       "      <td>48000</td>\n",
       "      <td>96000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>11400</td>\n",
       "      <td>11400</td>\n",
       "      <td>22800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row_Total</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             REAL   FAKE   Total\n",
       "Training    48000  48000   96000\n",
       "Validation  11400  11400   22800\n",
       "Testing       600    600    1200\n",
       "Row_Total   60000  60000  120000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all, df_train, df_val, df_test, classes_stats = dataset_to_df(path, relative_paths, paths_classes, 0.8, 0.19, 0.01)\n",
    "\n",
    "classes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "patch_size = 8\n",
    "trans_img_resize = 32\n",
    "image_patch_flat_size = 3*patch_size**2\n",
    "embed_size = image_patch_flat_size\n",
    "mlp_dim = 512\n",
    "train_batch_size = 64\n",
    "valid_batch_size = 32\n",
    "num_classes = 2\n",
    "encoder_depth = 6\n",
    "attention_heads = 12\n",
    "\n",
    "print(f'Flattened dimension size(of a patch): {image_patch_flat_size}')\n",
    "print(f'Embedding Size: {embed_size}')\n",
    "print(f'Output MLP size: {mlp_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618]\n",
    "norm_std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_train = transforms.Compose([\n",
    "#     # transforms.RandAugment(2, 14),\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.Resize(trans_img_resize),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(norm_mean, norm_std),\n",
    "# ])\n",
    "\n",
    "# transform_valid = transforms.Compose([\n",
    "#     transforms.Resize(trans_img_resize),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(norm_mean, norm_std),\n",
    "# ])\n",
    "\n",
    "# Prepare dataset\n",
    "# train_set = torchvision.datasets.CIFAR10(root='../../data/CIFAR10', train=True, download=True, transform=transform_train)\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=NUM_WORKERS )\n",
    "\n",
    "# valid_set = torchvision.datasets.CIFAR10(root='../../data/CIFAR10', train=False, download=True, transform=transform_valid)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=valid_batch_size, shuffle=False, num_workers=NUM_WORKERS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        \n",
    "        self.annotation = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 0])\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 3], dtype=torch.float64)\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.RandAugment(2, 14),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(trans_img_resize),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(trans_img_resize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "\n",
    "train_set  = Images_Dataset(df_train, transform_train)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=train_batch_size,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "valid_set  = Images_Dataset(df_val, transform_valid)\n",
    "\n",
    "valid_loader= DataLoader(dataset=valid_set ,\n",
    "                          batch_size=valid_batch_size,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First and Last Elements in the Whole dataset\")\n",
    "df_all.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_batch,label = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch of Images Shape: {sample_img_batch.size()}\")\n",
    "print(f\"Batch of Images Shape: {label.size()}\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # c: color channels\n",
    "\n",
    "        # h: desired slice height (pixels)\n",
    "        # w: desired slice width (pixels)\n",
    "\n",
    "        # row: No. of vertical slices\n",
    "        # col: No. of horizontal slices\n",
    "\n",
    "        # b: Batch of Images\n",
    "\n",
    "        sliced_Img = rearrange(\n",
    "            img, 'c (row h) (col w) -> row col c h w', h=self.slice_width, w=self.slice_width)\n",
    "\n",
    "        return sliced_Img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sliced_img = self.slice(img)\n",
    "        sliced_flattened_img = rearrange(\n",
    "            sliced_img, 'row col c h w -> (row col) (c h w)')\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(train_loader, index):\n",
    "\n",
    "    img, label = train_loader.dataset[index]\n",
    "\n",
    "    channels, img_H, img_W = img.size()\n",
    "    \n",
    "    batch = train_loader.batch_size\n",
    "\n",
    "    return img, label, channels, img_H, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img)\n",
    "    print(\"Sliced Image Shape: \", sliced_img.shape)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(8, 4))\n",
    "    subfigs = fig.subfigures(3, 1, height_ratios=[1., 1.5, 1.], hspace=0.05, squeeze='True')\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    img_a = rearrange(img, \"c h w -> h w c\").numpy()\n",
    "    \n",
    "    axs0.imshow(img_a)\n",
    "    axs0.axis('off')\n",
    "    # subfigs[0].suptitle('Input Image', fontsize=10)\n",
    "\n",
    "    grid1 = ImageGrid(subfigs[1], 111, nrows_ncols=(sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid1):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = rearrange(sliced_img[row][column], \"c h w -> h w c\").numpy()\n",
    "        \n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[1].suptitle('Slices', fontsize=10)\n",
    "\n",
    "    grid2 = ImageGrid(subfigs[2], 111, nrows_ncols=(1, sliced_img.size(0)*sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid2):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[2].suptitle('Position', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, _ = train_set[100]\n",
    "sample_img_tr, sample_img_label, ch, img_size, batch_size = img_data(train_loader, 2)\n",
    "print(f\"Input Image Shape: {sample_img_tr.size()}\")\n",
    "img_plot(sample_img, patch_size)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "slice_embed = embed_size\n",
    "print(\"slice_embed\", slice_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=slice_embed,\n",
    "    depth=encoder_depth,\n",
    "    heads=attention_heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    emb_dropout=0.1,\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_output = model(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape: \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path=\"../weights/vit10/checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)\n",
    "\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "best_acc = 0  # best test accuracy\n",
    "\n",
    "list_tr_loss = []\n",
    "list_tr_acc = []\n",
    "list_val_loss = []\n",
    "list_val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_state_path+'vit_patch8_cifar10_epoch_90')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_acc = checkpoint['best_acc']\n",
    "list_tr_loss = checkpoint['list_tr_loss']\n",
    "list_tr_acc = checkpoint['list_tr_acc']\n",
    "list_val_loss = checkpoint['list_val_loss']\n",
    "list_val_acc = checkpoint['list_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=base_lr)  \n",
    "    \n",
    "# use cosine scheduling\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "    print(f'\\n epoch: {epoch+1}/{n_epochs}')\n",
    "\n",
    "    ##########################################################\n",
    "    model.train(True)\n",
    "    tr_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop1 = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in loop1:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        # Backward path (Gradient)\n",
    "        loss.backward()\n",
    "        # Optimizer(Adam) Step\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_tr_loss = tr_loss/(batch_idx+1)\n",
    "        avg_tr_accuracy = 100.*correct/total\n",
    "\n",
    "        loop1.set_description(f\"Train--Epoch [{epoch+1}/{n_epochs}]\")\n",
    "        loop1.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "\n",
    "    ##########################################################\n",
    "    list_tr_loss.append(avg_tr_loss)\n",
    "    list_tr_acc.append(avg_tr_accuracy)\n",
    "\n",
    "    ##########################################################\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        loop2 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "        for batch_idx, (inputs, targets) in loop2:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss/(batch_idx+1)\n",
    "            avg_val_accuracy = 100.*correct/total\n",
    "\n",
    "            loop2.set_description(f\"Valid--Epoch [{epoch+1}/{n_epochs}]\")\n",
    "            loop2.set_postfix(loss=avg_val_loss, acc=avg_val_accuracy)\n",
    "\n",
    "    list_val_loss.append(avg_val_loss)\n",
    "    list_val_acc.append(avg_val_accuracy)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if avg_val_accuracy > best_acc:\n",
    "        best_acc = avg_val_accuracy\n",
    "\n",
    "        print('Saving..')\n",
    "        state = {\"model\": model.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict(),\n",
    "                 \"epoch\": epoch,\n",
    "                 \"list_tr_loss\": list_tr_loss,\n",
    "                 \"list_tr_acc\": list_tr_acc,\n",
    "                 \"list_val_loss\": list_val_loss,\n",
    "                 \"list_val_acc\": list_val_acc\n",
    "                 }\n",
    "        torch.save(state, model_state_path +\n",
    "                   f'vit_patch{patch_size}_cifar10_epoch_{epoch}')\n",
    "\n",
    "    ##########################################################\n",
    "\n",
    "    scheduler.step()  # step cosine scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(list_tr_loss )), list_tr_loss , label='Train Loss')\n",
    "axs[0].plot(range(len(list_val_loss)), list_val_loss, label='Validation Loss')\n",
    "axs[0].set_ylim([0,5])\n",
    "\n",
    "axs[1].plot(range(len(list_tr_acc)), list_tr_acc, label='Train Accuracy')\n",
    "axs[1].plot(range(len(list_val_acc)), list_val_acc, label='Validation Accuracy')\n",
    "axs[1].set_ylim([0,80])\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timmvenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
