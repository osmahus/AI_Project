{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from dataset_process import dataset_to_df, search_df\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.1+cu121\n",
      "Torch is using device: cuda:0\n",
      "CPU Count: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"CPU Count:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/CIFAK'\n",
    "relative_paths = [\"/train/REAL\", \"/train/FAKE\", \"/test/REAL\", \"/test/FAKE\"]\n",
    "paths_classes = [\"REAL\", \"FAKE\", \"REAL\", \"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REAL</th>\n",
       "      <th>FAKE</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>48000.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>96000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>11400.0</td>\n",
       "      <td>11400.0</td>\n",
       "      <td>22800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing</th>\n",
       "      <td>600.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               REAL     FAKE    Total\n",
       "Training    48000.0  48000.0  96000.0\n",
       "Validation  11400.0  11400.0  22800.0\n",
       "Testing       600.0    600.0   1200.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all, df_train, df_val, df_test, classes_stats = dataset_to_df(\n",
    "    path, relative_paths, paths_classes, 0.8, 0.19, 0.01)\n",
    "\n",
    "classes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>Class</th>\n",
       "      <th>Class_Codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/CIFAK//train/REAL/0000(10).jpg</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/CIFAK//train/REAL/0000(2).jpg</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>../../data/CIFAK//test/FAKE/999(9).jpg</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>../../data/CIFAK//test/FAKE/999.jpg</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Image_path Class  Class_Codes\n",
       "0       ../../data/CIFAK//train/REAL/0000(10).jpg  REAL            1\n",
       "1        ../../data/CIFAK//train/REAL/0000(2).jpg  REAL            1\n",
       "119998     ../../data/CIFAK//test/FAKE/999(9).jpg  FAKE            0\n",
       "119999        ../../data/CIFAK//test/FAKE/999.jpg  FAKE            0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.iloc[[0,1,119998,119999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_horizontal_slices= 4\n",
    "images_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape:  torch.Size([3, 32, 32])\n",
      "Image will be divided into: 4 x 4 = 16 slices\n",
      "Image Slice Shape:  (3, 8, 8)\n",
      "\n",
      "Feed (16) Images to the Dataloader\n",
      "\n",
      "Target Shape of the final flattened image: 16 x 192 \n"
     ]
    }
   ],
   "source": [
    "img_shape = read_image(df_all.iloc[0, 0]).size()\n",
    "\n",
    "slice_width = img_shape[1]//Img_horizontal_slices\n",
    "total_img_slices = Img_horizontal_slices**2\n",
    "\n",
    "\n",
    "print(\"Image Shape: \",img_shape)\n",
    "print(f\"Image will be divided into: {Img_horizontal_slices} x {Img_horizontal_slices} = {total_img_slices} slices\")\n",
    "print(\"Image Slice Shape: \",(img_shape[0],slice_width,slice_width))\n",
    "print(\"\")\n",
    "print(f\"Feed ({images_batch}) Images to the Dataloader\")\n",
    "print(\"\")\n",
    "print(f\"Target Shape of the final flattened image: {total_img_slices} x {img_shape[0]*slice_width**2} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # img: is a tensor of the shape (Color_Channels x Rows (Hight) x Columns (Width))\n",
    "        #\n",
    "        # Make a slice every \"slice_width\" as we are moving across dimension 1 (as we are moving\n",
    "        # vertically across rows)\n",
    "        img = img.unfold(1, self.slice_width, self.slice_width)\n",
    "        # Make a slice every slice_width as we are moving across dimension 2,\n",
    "        # Note that previous operation has added new dimension at the beginning\n",
    "        # refers to no. of vertical slices, hence 2 here still refers to the rows.\n",
    "        img = img.unfold(2, self.slice_width, self.slice_width)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.slice(img)\n",
    "        channels = img.size(0)\n",
    "        sliced_flattened_img= img.reshape(-1, self.slice_width * self.slice_width * channels)\n",
    "\n",
    "        return sliced_flattened_img\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((32, 32)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        \n",
    "        self.annotation = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 0])\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 2], dtype=torch.float64)\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(df_train, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "Im_val_dataset = Images_Dataset(df_val, data_transform)\n",
    "\n",
    "Im_val_loader = DataLoader(dataset=Im_val_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original Image and the slices Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(Img_train_loader, index):\n",
    "\n",
    "    date_sample = Img_train_loader.dataset[index]\n",
    "\n",
    "    img = date_sample[0]\n",
    "    img_t = date_sample[1]\n",
    "    label = date_sample[2]\n",
    "\n",
    "    channels = img.size(0)\n",
    "    img_size = img.size(1)\n",
    "\n",
    "    img_slices = img_t.size(0)\n",
    "    slice_flatsize = img_t.size(1)\n",
    "\n",
    "    imgs_per_batch = Im_tr_loader.batch_size\n",
    "\n",
    "    slice_width = int(np.sqrt(slice_flatsize/channels))\n",
    "\n",
    "    return img, img_t, label, channels, img_size, slice_width, img_slices, slice_flatsize, imgs_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img).permute(1, 2, 0, 3, 4)\n",
    "    print(\"Sliced Image Shape: \",sliced_img.shape)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(8, 4))\n",
    "    subfigs = fig.subfigures(3, 1, height_ratios=[1., 1.5,1.],hspace=0.05,squeeze='True')\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    axs0.imshow(img.permute(1, 2, 0))\n",
    "    axs0.axis('off')\n",
    "    # subfigs[0].suptitle('Input Image', fontsize=10)\n",
    "\n",
    "    grid1 = ImageGrid(subfigs[1], 111, nrows_ncols=(\n",
    "        sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid1):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[1].suptitle('Slices', fontsize=10)\n",
    "    \n",
    "    grid2 = ImageGrid(subfigs[2], 111, nrows_ncols=\n",
    "    (1,sliced_img.size(0)*sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid2):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    # subfigs[2].suptitle('Position', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape: torch.Size([3, 32, 32])\n",
      "Sliced Image Shape:  torch.Size([4, 4, 3, 8, 8])\n",
      "Flattened Image Shape: torch.Size([16, 192])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 65 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img, sample_img_trans, sample_img_label, ch, img_size, slice_width, slices, slice_flatsize, batch_size = img_data(\n",
    "    Im_tr_loader, 2)\n",
    "print(f\"Input Image Shape: {sample_img.size()}\")\n",
    "img_plot(sample_img, slice_width)\n",
    "print(f\"Flattened Image Shape: {sample_img_trans.size()}\")\n",
    "\n",
    "\n",
    "slice_embed = slice_flatsize*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 3, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 192])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "bbb = rearrange(sample_img, 'c (h1 h) (w1 w) -> h1 w1 c h w', h1=4, w1=4)\n",
    "print(bbb.shape)\n",
    "plt.imshow(rearrange(bbb[1][1],'c h w -> h w c'))\n",
    "ccc=rearrange(bbb,'h1 w1 c h w -> (h1 w1) (c h w)')\n",
    "# bbb[0][0].shape\n",
    "ccc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 3, 8, 8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rearrange(bbb.transpose(-1, -2)[1][1], 'c h w -> h w c'))\n",
    "# transposes the last dimension (-1) with the dimension just before it (-2)\n",
    "print(bbb.transpose(-1, -2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_a=torch.tensor(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6830,  0.9764, -0.6795, -0.3283],\n",
      "         [ 0.1400,  0.3466, -1.7832, -0.9688],\n",
      "         [ 1.5097,  0.5240,  0.7832,  0.3844],\n",
      "         [ 1.9285,  0.5260, -1.7167, -0.1973]],\n",
      "\n",
      "        [[ 0.6513,  0.6141, -0.9287, -0.6731],\n",
      "         [-1.8737, -1.2920, -1.2000,  0.5065],\n",
      "         [-0.2017,  0.1263,  0.0503, -0.5222],\n",
      "         [-1.2398,  0.2507, -0.7054, -0.0892]]])\n",
      "tensor([[ 0.6830,  0.9764, -0.6795, -0.3283],\n",
      "        [ 0.1400,  0.3466, -1.7832, -0.9688],\n",
      "        [ 1.5097,  0.5240,  0.7832,  0.3844],\n",
      "        [ 1.9285,  0.5260, -1.7167, -0.1973]])\n",
      "tensor([[ 0.6513,  0.6141, -0.9287, -0.6731],\n",
      "        [-1.8737, -1.2920, -1.2000,  0.5065],\n",
      "        [-0.2017,  0.1263,  0.0503, -0.5222],\n",
      "        [-1.2398,  0.2507, -0.7054, -0.0892]])\n"
     ]
    }
   ],
   "source": [
    "aaaa=torch.randn(2,4,4)\n",
    "bbbb,cccc= aaaa\n",
    "print(aaaa)\n",
    "print(bbbb)\n",
    "print(cccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Images Shape: torch.Size([16, 16, 192])\n"
     ]
    }
   ],
   "source": [
    "_, sample_img_batch,_ = next(iter(Im_tr_loader))\n",
    "\n",
    "print(f\"Batch of Images Shape: {sample_img_batch.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the VIT Model : Embedding &rarr; Transformer Encoder &rarr; MLP_Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1,1, slice_embed_size),requires_grad=True)\n",
    "        self.pos_to_embed = nn.Parameter(torch.rand(img_slices + 1, slice_embed_size),requires_grad=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_imgs):\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        #\n",
    "        img_embedding = self.img_to_embed(flattened_imgs)\n",
    "\n",
    "        # create the same cls embedding for all the images in the batch\n",
    "        #note that img_embedding.size(0) is the number of images in the batch\n",
    "        cls_embedding = self.cls_to_embed.repeat(img_embedding.size(0), 1, 1)\n",
    "        # Append the cls embedding to the beginning of the image slices\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "\n",
    "        position_embedding = self.pos_to_embed.repeat(img_embedding.size(0), 1, 1)\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        embedding = self.dropout(img_and_pos_embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:   torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 17, 768])\n",
      "Output Gradient Function:  <MulBackward0 object at 0x7f754c2e20b0>\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = ImageEmbedding(slice_input_size=slice_flatsize,\n",
    "                                 slice_embed_size=slice_embed,\n",
    "                                 img_slices=slices,\n",
    "                                 dropout_ratio=0.2)\n",
    "\n",
    "embedding_output = embedding_layer(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape:  \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", embedding_output.size())\n",
    "print(\"Output Gradient Function: \",embedding_output.grad_fn)\n",
    "# print(sample_img_label)\n",
    "\n",
    "# summary(model=embedding_layer,\n",
    "#         input_size=sample_img_batch.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "red { color: red }\n",
    "yellow { color: yellow }\n",
    "</style>\n",
    "\n",
    "##### **<yellow> Query: </yellow>** The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "\n",
    "##### **<yellow> Keys: </yellow>** For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is offering, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "\n",
    "##### **<yellow> Values: </yellow>** For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "\n",
    "##### **<yellow> Score function: </yellow>** To rate which elements we want to pay attention to, we need to specify a score function . The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "##### A word asks the same question to all words in the sequence using the query vector. Similarly, it provides the same answer to all words using the key vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{Softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right) V\n",
    "$$\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAPyTorchScaledDotProduct(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, block_size, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.block_size = block_size\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_in, d_out)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv.unbind(0)\n",
    "\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(\n",
    "            queries, keys, values, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(\n",
    "            batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:   torch.Size([16, 17, 384])\n",
      "Output Shape:  torch.Size([16, 17, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "MHAPyTorchScaledDotProduct (MHAPyTorchScaledDotProduct)      [16, 17, 384]        [16, 17, 384]        147,840              True\n",
       "Linear (qkv)                                               [16, 17, 384]        [16, 17, 1152]       442,368              True\n",
       "============================================================================================================================================\n",
       "Total params: 590,208\n",
       "Trainable params: 590,208\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 7.08\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.42\n",
       "Forward/backward pass size (MB): 2.51\n",
       "Params size (MB): 1.77\n",
       "Estimated Total Size (MB): 4.69\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MHA_layer = MHAPyTorchScaledDotProduct(d_in=embedding_output.size(2),\n",
    "#                                  d_out=embedding_output.size(2),\n",
    "#                                  num_heads=12,\n",
    "#                                  block_size=embedding_output.size(1),\n",
    "#                                  dropout=0.0\n",
    "#                                  )\n",
    "\n",
    "# MHA_output = MHA_layer(embedding_output)\n",
    "\n",
    "# print(\"Input Shape:  \", embedding_output.size())\n",
    "# print(\"Output Shape: \", MHA_output.size())\n",
    "\n",
    "# summary(model=MHA_layer,\n",
    "#         input_size=embedding_output.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, size, context_len, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_attention = nn.LayerNorm(size)\n",
    "\n",
    "        # self.attention = nn.MultiheadAttention(\n",
    "        #     embed_dim=size,\n",
    "        #     num_heads=num_heads,\n",
    "        #     dropout=0.0,\n",
    "        #     bias=False,\n",
    "        #     add_bias_kv=False,\n",
    "        #     batch_first=True)\n",
    "\n",
    "        self.attention = MHAPyTorchScaledDotProduct(\n",
    "            d_in=size,\n",
    "            d_out=size,\n",
    "            block_size=context_len,\n",
    "            dropout=0.0,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=False\n",
    "        )\n",
    "\n",
    "        self.norm_feed_forward = nn.LayerNorm(size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(size, 4 * size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * size, size),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        attn_input = self.norm_attention(input_tensor)\n",
    "\n",
    "        attn_output = self.attention(attn_input)\n",
    "\n",
    "        attn_plus_norm = input_tensor + attn_output\n",
    "\n",
    "        mlp_input = self.norm_feed_forward(attn_plus_norm)\n",
    "        output = attn_plus_norm + self.feed_forward(mlp_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = Encoder(size=slice_flatsize, num_heads=12, dropout=0.1)\n",
    "# encoder_output = encoder_layer(embedding_output)\n",
    "\n",
    "# print(\"Output Shape: \", encoder_output.size())\n",
    "\n",
    "# summary(model=encoder_layer,\n",
    "#         input_size=embedding_output.size(),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders=1,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, img_slices, num_heads, dropout=enc_dropout) for _ in range(num_encoders)],)\n",
    "\n",
    "        self.mlp_head = nn.Linear(slice_embed_size, num_classes)\n",
    "        self.out_class = nn.Sigmoid()\n",
    "        # self.out_class = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        # output = torch.round(self.sigmoid(self.mlp_head(attn[:, 0, :])))\n",
    "        output = self.out_class(self.mlp_head(attn[:, 0]))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model Hyper-parameters Hereunder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "VIT (VIT)                                               [16, 16, 192]        [16, 1]              --                   True\n",
       "ImageEmbedding (embedding)                            [16, 16, 192]        [16, 17, 192]        3,456                True\n",
       "    Linear (img_to_embed)                            [16, 16, 192]        [16, 16, 192]        37,056               True\n",
       "    Dropout (dropout)                                [16, 17, 192]        [16, 17, 192]        --                   --\n",
       "Sequential (encoders)                                 [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "    Encoder (0)                                      [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "        LayerNorm (norm_attention)                  [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "        MHAPyTorchScaledDotProduct (attention)      [16, 17, 192]        [16, 17, 192]        147,648              True\n",
       "        LayerNorm (norm_feed_forward)               [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "        Sequential (feed_forward)                   [16, 17, 192]        [16, 17, 192]        295,872              True\n",
       "    Encoder (1)                                      [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "        LayerNorm (norm_attention)                  [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "        MHAPyTorchScaledDotProduct (attention)      [16, 17, 192]        [16, 17, 192]        147,648              True\n",
       "        LayerNorm (norm_feed_forward)               [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "        Sequential (feed_forward)                   [16, 17, 192]        [16, 17, 192]        295,872              True\n",
       "Linear (mlp_head)                                     [16, 192]            [16, 1]              193                  True\n",
       "Sigmoid (out_class)                                   [16, 1]              [16, 1]              --                   --\n",
       "=======================================================================================================================================\n",
       "Total params: 929,281\n",
       "Trainable params: 929,281\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 13.63\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 8.75\n",
       "Params size (MB): 3.41\n",
       "Estimated Total Size (MB): 12.35\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model = VIT(slice_input_size=slice_flatsize,\n",
    "                slice_embed_size=slice_embed,\n",
    "                img_slices=slices,\n",
    "                num_classes=1,\n",
    "                num_heads=12,\n",
    "                num_encoders=2,\n",
    "                emb_dropout=0.1,\n",
    "                enc_dropout=0.1)\n",
    "\n",
    "vit_output = vit_model(sample_img_batch)\n",
    "\n",
    "print(\"Input Shape: \", sample_img_batch.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=vit_model,\n",
    "        input_size=sample_img_batch.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_epochs = 50         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "vit_optimizer = Adam(vit_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "criterion = CrossEntropyLoss() # returns the mean loss for the batch\n",
    "# scheduler = lr_scheduler.LinearLR(vit_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path=\"../weights/vit3\"\n",
    "\n",
    "# saved_model = vit_model()\n",
    "# saved_model.load_state_dict(torch.load(model_state_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|| 6000/6000 [00:40<00:00, 147.25it/s, acc=0.14, loss=nan] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: nan | Validation accuracy: 0.00\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|| 6000/6000 [00:41<00:00, 144.73it/s, acc=0, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: nan | Validation accuracy: 0.00\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]:  95%|| 5675/6000 [00:39<00:02, 144.69it/s, acc=0, loss=nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m avg_tr_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39maccumulated_tr_accuracy \u001b[38;5;241m/\u001b[39m (batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward path (Gradient)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Optimizer(Adam) Step\u001b[39;00m\n\u001b[1;32m     43\u001b[0m vit_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_loss_history = [1000]\n",
    "tr_accuracy_history = [0]\n",
    "val_loss_history = [1000]\n",
    "val_accuracy_history = [0]\n",
    "\n",
    "vit_model.to(device)\n",
    "\n",
    "for epoch in range(vit_epochs):\n",
    "\n",
    "    # Training Loop\n",
    "    vit_model.train(True)\n",
    "\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_tr_accuracy = 0\n",
    "    accumulated_val_loss = 0\n",
    "    accumulated_val_accuracy = 0\n",
    "\n",
    "    loop = tqdm(enumerate(Im_tr_loader), total=len(Im_tr_loader))\n",
    "    for batch, (_, inputs, targets) in loop:\n",
    "\n",
    "        # Put inputs and labels in device cuda\n",
    "        tr_images = inputs.to(device)\n",
    "        sample_img_labels = targets.to(device)\n",
    "\n",
    "        # Zero gradients for every batch\n",
    "        vit_optimizer.zero_grad()\n",
    "\n",
    "        # Forward Path\n",
    "        tr_outputs = vit_model(tr_images).squeeze(1)\n",
    "\n",
    "        loss = criterion(tr_outputs,sample_img_labels)\n",
    "\n",
    "        accumulated_tr_loss += loss.item()\n",
    "        accumulated_tr_accuracy += (sample_img_labels == tr_outputs).sum().item()/sample_img_labels.size(0)\n",
    "        \n",
    "        avg_tr_loss = accumulated_tr_loss / (batch+1)\n",
    "        avg_tr_accuracy = 100*accumulated_tr_accuracy / (batch+1)\n",
    "\n",
    "        # Backward path (Gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer(Adam) Step\n",
    "        vit_optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{vit_epochs}]\")\n",
    "        loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "    \n",
    "    tr_loss_history.append(avg_tr_loss)\n",
    "    tr_accuracy_history.append(avg_tr_accuracy)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "# Validation Loop\n",
    "    vit_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch, (_, inputs, targets) in enumerate(Im_val_loader):\n",
    "\n",
    "            # Put inputs and labels in device cuda\n",
    "            val_images = inputs.to(device)\n",
    "            val_labels = targets.to(device)\n",
    "\n",
    "            val_outputs = vit_model(val_images).squeeze(1)\n",
    "\n",
    "            loss = criterion(val_outputs,val_labels)\n",
    "\n",
    "            accumulated_val_loss += loss.item()\n",
    "            accumulated_val_accuracy += (val_labels == val_outputs).sum().item()/val_labels.size(0)\n",
    "\n",
    "            avg_val_loss = accumulated_val_loss / (batch+1)\n",
    "            avg_val_accuracy = 100*accumulated_val_accuracy / (batch+1)\n",
    "\n",
    "        \n",
    "        best_val_loss=min(val_loss_history)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            torch.save(vit_model.state_dict(), model_state_path+f'_epoch_{epoch}')\n",
    "\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_accuracy_history.append(avg_val_accuracy)\n",
    "\n",
    "        print(f'Validation loss: {avg_val_loss:.2f} | Validation accuracy: {avg_val_accuracy:.2f}')\n",
    "        print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in vit_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", vit_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(tr_loss_history )), tr_loss_history , label='Train Loss')\n",
    "axs[1].plot(range(len(tr_accuracy_history)), tr_accuracy_history, label='Validation Loss')\n",
    "axs[0].plot(range(len(val_loss_history)), val_loss_history, label='Train Accuracy')\n",
    "axs[1].plot(range(len(val_accuracy_history)), val_accuracy_history, label='Validation Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
