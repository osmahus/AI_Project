{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# The cross-entropy loss penalizes the model more when it is more confident in the incorrect class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Adam is an optimization algorithm that can be used instead of the classical SGD procedure\n",
    "# to update network weights iterative based in training data.\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osmahus/pytVenv/lib/python3.11/site-packages/cudf/utils/_ptxcompiler.py:61: UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/osmahus/pytVenv/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py\", line 254, in ensure_initialized\n",
      "    self.cuInit(0)\n",
      "  File \"/home/osmahus/pytVenv/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py\", line 327, in safe_cuda_api_call\n",
      "    self._check_ctypes_error(fname, retcode)\n",
      "  File \"/home/osmahus/pytVenv/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py\", line 395, in _check_ctypes_error\n",
      "    raise CudaAPIError(retcode, msg)\n",
      "numba.cuda.cudadrv.driver.CudaAPIError: [100] Call to cuInit results in CUDA_ERROR_NO_DEVICE\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 4, in <module>\n",
      "  File \"/home/osmahus/pytVenv/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py\", line 292, in __getattr__\n",
      "    self.ensure_initialized()\n",
      "  File \"/home/osmahus/pytVenv/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py\", line 258, in ensure_initialized\n",
      "    raise CudaSupportError(f\"Error at driver init: {description}\")\n",
      "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: Call to cuInit results in CUDA_ERROR_NO_DEVICE (100)\n",
      "\n",
      "\n",
      "Not patching Numba\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch is using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)\n",
    "# torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch is using device:', device)\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr = '../../data/CIFAK/train'\n",
    "path_val = '../../data/CIFAK/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Spaces from the Dataset folder to simplify reading from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_fname_space(path):\n",
    "#     for filename in os.listdir(path):\n",
    "#         my_source = path + \"/\" + filename\n",
    "#         my_dest = path + \"/\" + filename.strip().replace(\" \", \"\")\n",
    "#         os.rename(my_source, my_dest)\n",
    "\n",
    "# remove_fname_space(path_tr + \"/REAL\")\n",
    "# remove_fname_space(path_tr + \"/FAKE\")\n",
    "# remove_fname_space(path_val + \"/REAL\")\n",
    "# remove_fname_space(path_val + \"/FAKE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all the data from the \"CIFAK\" dataset to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_data(img_path, folder_name, img_class):\n",
    "    fname = os.listdir(img_path + \"/\"+folder_name)\n",
    "    fname.sort()\n",
    "    fpath = [img_path + \"/\"+folder_name+\"/\" + f for f in fname]\n",
    "    labels = [img_class]*len(fname)\n",
    "    return fpath, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of:          Real         Fake         Total\n",
      "------------------------------------------------------------\n",
      "Training Samples:         50000       50000         100000\n",
      "Validation Samples:       10000       10000         20000\n"
     ]
    }
   ],
   "source": [
    "fpath_tr_real, labels_tr_real = dataset_data(path_tr, \"REAL\", 1.0)\n",
    "fpath_tr_fake, labels_tr_fake = dataset_data(path_tr, \"FAKE\", 0.0)\n",
    "fpath_tr = fpath_tr_real + fpath_tr_fake\n",
    "labels_tr = labels_tr_real+labels_tr_fake\n",
    "tr_dict = {'Image_path': fpath_tr, 'True?': labels_tr}\n",
    "tr_df = pd.DataFrame(tr_dict)\n",
    "tr_df.to_csv(path_tr+\"/tr_annotation.csv\")\n",
    "\n",
    "fpath_val_real, labels_val_real = dataset_data(path_val, \"REAL\", 1.0)\n",
    "fpath_val_fake, labels_val_fake = dataset_data(path_val, \"FAKE\", 0.0)\n",
    "fpath_val = fpath_val_real + fpath_val_fake\n",
    "labels_val = labels_val_real+labels_val_fake\n",
    "val_dict = {'Image_path': fpath_val, 'True?': labels_val}\n",
    "val_df = pd.DataFrame(val_dict)\n",
    "val_df.to_csv(path_val+\"/val_annotation.csv\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total Number of:          Real         Fake         Total\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Training Samples:        \",len(fpath_tr_real),\"     \",len(fpath_tr_fake),\"       \",len(fpath_tr))\n",
    "print(\"Validation Samples:      \",len(fpath_val_real),\"     \",len(fpath_val_fake),\"       \",len(fpath_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>True?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8941</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0894(2).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0420.jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0823(10).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19686</th>\n",
       "      <td>../../data/CIFAK/test/FAKE/970(7).jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17465</th>\n",
       "      <td>../../data/CIFAK/test/FAKE/770(6).jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7035</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0703(6).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0052(3).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19237</th>\n",
       "      <td>../../data/CIFAK/test/FAKE/93(8).jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6218</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0621(9).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13314</th>\n",
       "      <td>../../data/CIFAK/test/FAKE/397(5).jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Image_path  True?\n",
       "8941    ../../data/CIFAK/test/REAL/0894(2).jpg    1.0\n",
       "4209       ../../data/CIFAK/test/REAL/0420.jpg    1.0\n",
       "8230   ../../data/CIFAK/test/REAL/0823(10).jpg    1.0\n",
       "19686    ../../data/CIFAK/test/FAKE/970(7).jpg    0.0\n",
       "17465    ../../data/CIFAK/test/FAKE/770(6).jpg    0.0\n",
       "...                                        ...    ...\n",
       "7035    ../../data/CIFAK/test/REAL/0703(6).jpg    1.0\n",
       "522     ../../data/CIFAK/test/REAL/0052(3).jpg    1.0\n",
       "19237     ../../data/CIFAK/test/FAKE/93(8).jpg    0.0\n",
       "6218    ../../data/CIFAK/test/REAL/0621(9).jpg    1.0\n",
       "13314    ../../data/CIFAK/test/FAKE/397(5).jpg    0.0\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation,test=train_test_split(val_df,test_size=0.1,stratify=val_df[['True?']])\n",
    "validation\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>True?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/CIFAK/train/REAL/0000(10).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/CIFAK/train/REAL/0000(2).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>../../data/CIFAK/train/FAKE/5999(9).jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>../../data/CIFAK/train/FAKE/5999.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Image_path  True?\n",
       "0      ../../data/CIFAK/train/REAL/0000(10).jpg    1.0\n",
       "1       ../../data/CIFAK/train/REAL/0000(2).jpg    1.0\n",
       "99998   ../../data/CIFAK/train/FAKE/5999(9).jpg    0.0\n",
       "99999      ../../data/CIFAK/train/FAKE/5999.jpg    0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"tr_df\")\n",
    "tr_df.iloc[[0,1,99998,99999]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>True?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0000(10).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/CIFAK/test/REAL/0000(2).jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>../../data/CIFAK/test/FAKE/999(9).jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>../../data/CIFAK/test/FAKE/999.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Image_path  True?\n",
       "0      ../../data/CIFAK/test/REAL/0000(10).jpg    1.0\n",
       "1       ../../data/CIFAK/test/REAL/0000(2).jpg    1.0\n",
       "19998    ../../data/CIFAK/test/FAKE/999(9).jpg    0.0\n",
       "19999       ../../data/CIFAK/test/FAKE/999.jpg    0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"val_df\")\n",
    "val_df.iloc[[0,1,19998,19999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the number of slices (patches) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_horizontal_slices= 4\n",
    "images_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*6250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape:  (32, 32, 3)\n",
      "Image Slice Shape:  (8, 8)\n",
      "Total Number of Slices per Image:  16\n",
      "Total Number of Images per Loader:  16\n"
     ]
    }
   ],
   "source": [
    "img_shape = cv2.imread(fpath_tr[0]).shape\n",
    "print(\"Image Shape: \",img_shape)\n",
    "\n",
    "slice_width = img_shape[0]//Img_horizontal_slices\n",
    "print(\"Image Slice Shape: \",(slice_width,slice_width))\n",
    "\n",
    "total_img_slices = Img_horizontal_slices**2\n",
    "print(\"Total Number of Slices per Image: \",total_img_slices)\n",
    "print(\"Total Number of Images per Loader: \",images_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceImage:\n",
    "    def __init__(self, slice_width):\n",
    "        self.slice_width = slice_width\n",
    "\n",
    "    def slice(self, img):\n",
    "        # img: is a tensor of the shape (Color_Channels x Rows (Hight) x Columns (Width))\n",
    "        #\n",
    "        # Make a slice every \"slice_width\" as we are moving across dimension 1 (as we are moving\n",
    "        # vertically across rows)\n",
    "        img = img.unfold(1, self.slice_width, self.slice_width)\n",
    "        # Make a slice every slice_width as we are moving across dimension 2,\n",
    "        # Note that previous operation has added new dimension at the beginning\n",
    "        # refers to no. of vertical slices, hence 2 here still refers to the rows.\n",
    "        img = img.unfold(2, self.slice_width, self.slice_width)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.slice(img)\n",
    "        channels = img.size(0)\n",
    "\n",
    "        return img.reshape(-1, self.slice_width * self.slice_width * channels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms on the input data (x) tensor\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize((32, 32)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    SliceImage(slice_width=slice_width)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None):\n",
    "        \n",
    "        self.annotation = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = read_image(self.annotation.iloc[index, 1])\n",
    "\n",
    "        labels = torch.tensor(self.annotation.iloc[index, 2])\n",
    "\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "\n",
    "        return img, img_t, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # To return the length of the dataset\n",
    "        return self.annotation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset object and configure the Data Loader\n",
    "tr_annotation_file = path_tr+\"/tr_annotation.csv\"\n",
    "val_annotation_file = path_val+\"/val_annotation.csv\"\n",
    "\n",
    "Im_tr_dataset = Images_Dataset(tr_annotation_file, data_transform)\n",
    "\n",
    "Im_tr_loader = DataLoader(dataset=Im_tr_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "\n",
    "Im_val_dataset = Images_Dataset(val_annotation_file, data_transform)\n",
    "\n",
    "Im_val_loader = DataLoader(dataset=Im_val_dataset,\n",
    "                          batch_size=images_batch,\n",
    "                          # Drops the last mini batch if less than the batch size (could enhance the model accuracy)\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "validation_split= int(np.round(len(Im_val_dataset)*0.8))\n",
    "test_split= len(Im_val_dataset)-validation_split\n",
    "print(validation_split)\n",
    "print(test_split)\n",
    "# validation_dataset, test_dataset = random_split(Im_val_dataset, [, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original Image and the slices Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_data(Img_train_loader, index):\n",
    "\n",
    "    date_sample=Img_train_loader.dataset[index]\n",
    "\n",
    "    img = date_sample[0]\n",
    "    img_t = date_sample[1]\n",
    "    label = date_sample[2]\n",
    "\n",
    "    channels = img.size(0)\n",
    "    img_size = img.size(1)\n",
    "\n",
    "    total_img_slices = img_t.size(0)\n",
    "    slice_flat = img_t.size(1)\n",
    "\n",
    "    imgs_per_batch = Im_tr_loader.batch_size\n",
    "\n",
    "    slice_width = int(np.sqrt(slice_flat/channels))\n",
    "\n",
    "    return img, channels, img_size, slice_width, total_img_slices, slice_flat, imgs_per_batch, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_plot(img, slice_width):\n",
    "    slice = SliceImage(slice_width)\n",
    "    sliced_img = slice.slice(img).permute(1, 2, 0, 3, 4)\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(4, 4))\n",
    "    subfigs = fig.subfigures(2, 1, height_ratios=[1., 1.])\n",
    "\n",
    "    axs0 = subfigs[0].subplots(1, 1)\n",
    "    axs0.imshow(img.permute(1, 2, 0))\n",
    "    axs0.axis('off')\n",
    "\n",
    "    grid = ImageGrid(subfigs[1], 111, nrows_ncols=(\n",
    "        sliced_img.size(0), sliced_img.size(1)), axes_pad=0.03)\n",
    "\n",
    "    for i, ax in enumerate(grid):\n",
    "        i_b4 = str(np.base_repr(i, sliced_img.size(0))).zfill(2)\n",
    "        row = int(i_b4[0])\n",
    "        column = int(i_b4[1])\n",
    "        patch = sliced_img[row][column].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAGKCAYAAACSBKRvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqT0lEQVR4nO2dSYxkV3ae7xtijoyIjMixMquyBhaLRbaabPYkQd1uw4AWQgNqW4AFCBBgwzIgbeyFYRjaCJA3BmQbhmEt2jsbBiy4vfDKgAyYsAR1g5DVJtkDWVWsiZlZWTlFZMzze/GeFw2y88T5XzKgy+rOBv5vw7onD1/cuC9OvLjnnsGJ4zg2hJC/Me7PewKE/KJDIyLEEhoRIZbQiAixhEZEiCU0IkIsoRERYgmNiBBLaESEWOIvqvjt7/0elP/wB+9Ded6vQPlK7SqUD1v4dWuVTSi/ufMylE/HQyXb2FyGus+e34fyo+YHUF5ZjaD82s08lEfpNpQPxodQ7nn4O63gr0H5sBfi1515UH5l/Q5+XWcLyjutMZTPnAGUt1p1JRuMR1B3Os5AebeF19LE+D31R3gtO91TKA8T4nPqp2dQ/p0/eg//D+fgk4gQS2hEhFhCIyLEEhoRIZbQiAixZGHv3NYW9uA8fvQRlD95iOX9XgDlq8s7UN5onkD5aIC9Phsb60pWCXJQd+safk8rW/i7pT95BuX1BvYQTWLtrTLGmHQee/lWVqpQnuSFy+exvu9hz5fn43UIJthlFcVpKO93J1D+9Glbv2ZqCnULuRSUj8f42p029ggOJ/jzEYTaS2uMMW7CGjgGv9dF4JOIEEtoRIRYQiMixBIaESGW0IgIsWRh79z7934M5Wvrq1CeThWhPJsqQfnyUg3KP7yHvXz37+O4t5s3tJfvqI7ncv3WBpRXavi7JfawByeKs1Aehlg+HWCP1TToQvnV9degvJDFMYGjEfZwNTv4daMp9v5NBgUoP9jF13/7L58r2RXsdDW3b+PPQRwlrSX2aHoJn+BJgN/ToIPlfbz0C8EnESGW0IgIsYRGRIglNCJCLKEREWLJwt65Bx/eg/Jf/urXoLxaxZ6vwwOccVhvHeEX9rBHyUmIyTqs7yvZ1o03oe5khuOrjk57UD6Y4LnHKayfzuFszHwRx7yVy9jbVijh7N6kmLqjExyz5yZkdS7lsEfM9fA9DCZlKB90tafWcfAcfS8phg3H1HlugjyFPaatFo7RbDXxIgSThU1BwScRIZbQiAixhEZEiCU0IkIsoRERYsnCLon6GS4M9/DJYygvFrCnqdPEnqyUm5B9uqMzVY0xJl/CXpkHH2gv4mlTe+yMMaYzwd8hjos9f24ay9MGx3XFKeydK2exd2t1/RaUt9vYw1U/xQFfzTaObVup4li4IFqC8n4bZ8hGIa6Dt33lV5VsbfUAX8Pg99Tu4gzW/gh7Uk1CBmuriz9nwagC5cU8jt1cBD6JCLGERkSIJTQiQiyhERFiCY2IEEsW9s5FCVX5Hz1+CuVrK9iDU8hjz5Sbwh6uXBG/brGMrx9E2mPVnzag7nA2w3PxsLxSw96tQglnaSa9p24Xd1t4uo89WZ6L12w0w/FhuSU8n0wee0yfPsSe13e++wTKh61tKI+NXrfsCb52Z9CB8lYTf6+n/CRPKv58JNWRy2VxlnPS53IR+CQixBIaESGW0IgIsYRGRIglNCJCLFnYO1fMVfAfHOyBms1wBmGvj70y9TGu7j8ctaH8pds4zuz1L95WsuISjsvrDvC1D4/2oLzTb0J5dIY7VOSWcOyZm/DVNU7wtq1v4MzTTMnBr+tjT5Nn8Drs7WLv5ffextnMToD7m25t6+zTqKBr0RljTG0Fx855Ps76zWTwWo4m2AuXMtgLl8lh+SzEHtNF4JOIEEtoRIRYQiMixBIaESGW0IgIsWRh79zzvWMo376K+57mUjjOLDQJWaMJ5nzaxPXohvfaUL6yUlGyQhF7dtY3cUeLcjUP5f0x9mKddXCdt+IMX6dSwd6zTDbhdqSw989zsb7r49g/Z5bQlzTGHquk2nAJoXlm86q+/tIKvnZtBXtvnRn+3AwTev02E2oEDofYa5wu4etMpgmZswvAJxEhltCICLGERkSIJTQiQixZ2LHgxHhTGuLqTKbbwhu1zW28mfcM3mh+tPcMyqMYb9qngQ4rclP42o1WUkgK3vgvVfCmd8lgue/jhLFKpQLl5SoO7wkMDpUyKVzkvdlOSCpM34DyXgfr5/N4jb/59z4P5YORbgF6/aWbUDeY4vfUPsXhN80hLg8WTvEaOAlhZ5NxG8ozGRa0J+TnBo2IEEtoRIRYQiMixBIaESGWLOyS8GbYCzLp4zAK12DPVOcswcsS43CgQh4XW8+ksP37oLSS6yaVrsJziWKcMLa+jovrl8vYm5dOY49mMYf1SwXsDesEOLTF9XBSXqqI43KCPtbv97CLtVTG4VIv3cEfm9q69v4NejhUqp3gvT2d4OTMRv0Qyod9/LlcLuPPTXEJf15jh2E/hPzcoBERYgmNiBBLaESEWEIjIsSSxWPnsPPMDBK8LAmOI3M2xQlm6Ry252Iee5rCEF8nAs6X1dUNqNtu42Lr4RjPJQ6wt8pJSL6bDnH8VmuKFzMIsOcoX8MeqONDnAw4buP5d0/x/I8PcLxatYK9i46Lmxh4Kf1ZiKOEOL4iXrOr29gDGiVUtBr3khIHcTyjl8L3vD/A81wEPokIsYRGRIglNCJCLKEREWIJjYgQSxZP5wux9yIKcFxaKsaxS5MB9qqZCLvz8gneuW4vIZ5spL8XKiX8XZFPrUB5qVSB8rXlK1C+XKrhuSSUtEr67nI9vDbtk8dQvvcQZ4dOu9hj1avjQvSjHr63r9zGbSXHwS6UH5+eKlk6wiXVqiWcZbu5gvVfvv4qlPfb2AM6C3A8YBjjpgTTAHsFF4FPIkIsoRERYgmNiBBLaESEWEIjIsSShb1zGSchkzSD47oqBeydG4ywFy5OaFvphPj64QBPPQL16472BlD32g72EK2Wca20vL8M5U6YEKfl4Vi1OMQepWEPx3U9eoQ9kfUTfE9SM+zRHLVwtvE4Ifu0UMAeq5SH4yXdWMsLGRx/F00SMqLx7TYbqxUon1Xx52bYx/c8jrHnMpfD8kXgk4gQS2hEhFhCIyLEEhoRIZbQiAixZGHvXC6hzlsuoVZaxktohZjD7QeDhFpvs4QsUz+hE8Mk0HFg4y6eix/jmLeCh2PkzBh3bej3Euq/uVh/NsUepU6CxzEXvwTlSz722k0HuPOGGySspYNbei6XsDevnNC+czDSaz8Z45TU+hnuyNH0cC3Aq9v4npQqeI6Rg+MQJwlrPAVz/8kLY/F5+CQixBIaESGW0IgIsYRGRIglNCJCLFnYOxejgm7GGGeG5e0GromWzmOvWuQmxHUNcIai7ybUegu1ly9fwN4qM8UxZtMhjqNy0wndHxzsrcr4CTF1WRxPlvVwrNpK+RqUX1vD34Gn+1Bs2tk+lB/t4/+hUsIfj5Uqzgh2mm0lCxM6UbghnnuQkEE97OtrG2NMKoNr+E0Tavv1OgkZ18OE5sOvY/F5+CQixBIaESGW0IgIsYRGRIglNCJCLFnYOxfgREEzxkmaptvHcV3LVezJ8hIyZPsJZeoKZRyDF4PYua21O1A35VShvNPEHqWJjz1HTpyQlRsnTD6hP20c48V0XOzN29m5BeXpTTxPz9mF8l4P90l9foRj/16d4nUrFrXXLl1O6HNbwR+9YJjQkcPB3rP+BNfSi2N8D3PFCpSn0/i9LgKfRIRYQiMixBIaESGW0IgIsYRGRIglC3vntlbfhPJUQl2xW9dwjFJpGXt2jI+9c8srOO7t8Uc43quX1Z6vtIOvkYpxLFzvDHvJggz2+KRTOO5vMsZesiDA3rmzehvKK3m8Zh/dfwfKd27j+nj7h+9BeamK1344wVmpj3exR6ywrOMou90fY90MjltcLm1CuefgezIK8ByLefz5q63hmEvfxd7eReCTiBBLaESEWEIjIsQSGhEhltCICLFkYe/cN77+TXyBFPbs+GkszyV0izAu9nwVytiDNh7jqe8/0zXU9j86hLrFJezZiWb42sUijpGbhThO8OQEe7HSKZw5u5TghTvYw10bugkxbwcn34fyUYzXoT/EWcgdnAhrjHsdiutNPc/HTz6AuiHIQDbGmGoZ92xdreFsWt/FQZ2FLF7jeBN759Zqf/PnCZ9EhFhCIyLEEhoRIZbQiAixhEZEiCULe+du3MDZoZ6PvWpRQpaml8YvmZCIaFJ5nHE4HeO+oXu7j5UswI6gRA9Rr4+zKEtL2Au3sY5bB9SqOD4sinGsnWNwrF27i71zYYw7KHTa2GvnF7AnazjF1zmu4/UJI9yhYTbT8/c8/F4PD3EnilYdxxUGEzyXpTz+nPUc7FqMAxwjN53gD+AXK1As4JOIEEtoRIRYQiMixBIaESGW0IgIsWRh71xpCccuJTRzMNMQe7hiF3ugkuSei+PVBv0WlB8d7ipZJoe7MxSKOL6q3cLesJOjYyhPqhe3WtuG8ukY16PbO9iF8kFCENs4wF64qX8A5X6AvYvBDM9nFuGbe9rArxu6+p7s3LwBdUcJ3rbJAH+vux6OxXR9vPZhQneJM9C5whhjhkP8OTOvYrGYw6erEEIugkZEiCU0IkIsoRERYgmNiBBLFu/ZarB3JIqwd2QW4hioKMZemSDGWaYebglrxoM2lg91NmkhnxCrFuFYsum4CeXHR/g1BwN8ndU1nEma8nD8VqOBvYJxhNfypPEhlDvZUyj3C3iNb93dgfLXv/AKlAdxB8pnoK9vwsfDrG/i+Ds3wpnPSZmq0QzH/RkPf85SHo7FnCUFby4An0SEWEIjIsQSGhEhltCICLGERkSIJQt75wZ97NlxEsxwMk3wfAU4Dmw4wrFwThp7WcbjNpTnc9rLUiziSR4cYO/WwT7OupyGCYGCJiG7N8LxWJVl3Md0FuNs3WYLd8BI5fE9yeKEWhMY7OW7slWD8q1reJ5n/TaUl6u6G0Wriz151eoGlG/UbkJ5FGI3X+NUZzIbY0yY0Hkjl9CNwndxL+FF4JOIEEtoRIRYQiMixBIaESGW0IgIsWRh75yT0FPVcbAHKpXQLSKKsd1GMc6E9SLsnVup4mzVz929rq+dEBbV7+GuDcsV7KnJZHCHikmIl9H38dq4Dg4IbHewV7A7xPKvvPESlI9inNXZH2OPaaWGOyWMEjJhw4R7kk7rmMBUKmEt0/g1C3m8xkmRbZMR1h/28dzjhBi5aVJxwgXgk4gQS2hEhFhCIyLEEhoRIZbQiAixZGHv3GyGY5GchHpxxsHeDsfF8ijCcWB+QiZspYS9PumUjskqlnC25Gt3X4Zy4+AsykYLx7a98y6OwesOsHcug5MrzWCEM1s3ruD5f/7NW1B+/wmuj5c1+Dp3gEfTGGPKq9gDWhzhz4KX0e83CPEaDMfYG9ts4li7UhHPPZ/QAzipFuCkm+C57OF7uwh8EhFiCY2IEEtoRIRYQiMixJLFHQsGb/wbdVzcvLSUhvKVKg73cF2cLHXWwAlpw14dyv/8L95Ssq/+yleg7lIZJ6MNxzhBMJetQHlSgmChgENSqgkhS2GIyz8t13ByXH+I134W48TH1Q28xvU6Div60YPnUJ4t4rCl5VUd9pPO4PvdbuGyXs2ElpvVZbyWlQoUm3IJv1c3l/D5c3CZtEXgk4gQS2hEhFhCIyLEEhoRIZbQiAixZGHvnJ/DXpN8ESc5uT4O6zg5xSEpDx+8C+Wnp7tQPhpgb8rGqvbKPLr/Y6hbLOkST8YY02jjEJAr2zgJ7vbtLSjvJYT99EDRfWOMyeZxqMqVLeyZ8n0cfpPO4Ov4CSFae/tPofyH93A4U4Kz0Nx5Ta9POpXQyjLE3ttsHn+vHz7Hn5v9Xeyh/PKX3oDy61vXoPzsDHtkF4FPIkIsoRERYgmNiBBLaESEWEIjIsSShb1zSWWbGh3sNTEh9spMR20oP63vQflZ/QDKoxCXf3JA8mD9BMdpxQkF53stHHvWK1Wg/O4v4di89+9jr1e/jz2LV3dW8PVfxUXeZy6+Tq6DvxtdB3vngimOiywv4Y/H+iqOOczndOzc8TFONGydYQ9oPoXXftDHcYVhgD9/165iF+J6bRvKu2165wj5uUEjIsQSGhEhltCICLGERkSIJQt75+pnT6D8rIm9I3GIY+eSvHZRhL0ynoPjw9yEQvq5rC6kf+fWVaiblHWZTZDXqrg80/Y29qrtPn8G5bMj7FkMp7gIe6+Ds3idDNb3XLw27Tb25o2muPlAuViB8rU17PmKQx1HeWUNxxXWDx9A+ZNd7NEs5PAct7Zw/GM4xZ+/o0N8T1wPr9ki8ElEiCU0IkIsoRERYgmNiBBLaESEWLKwd643xjFsbgZ7QVJpnF0ZjhL0fVy4PpfFdl5IaP1YKl7R18jhOm+7BzgecDjGHsFMKmEuCZ6jG9exZ6o9wEXbf/Q+jh/sdLFXbWUdZ4dev46zN7sJLRg/2sOZtsEE35NRH99DP6PX58rmGtRdrWH5s8ePoDxOaAdZulOF8nYTv6d+QlzkeoIXcRH4JCLEEhoRIZbQiAixhEZEiCU0IkIsWdg7Nxjj+l6+60E5lhqTxZ0czY1b2qtmjDHxBGdRBgMcgxdOdceC8QjHqnUSPDidNvbgzDzshfvw4T0od7M4Bu/OKzhTdWUdexHX1nF/Si+DM1Irq3iemQJe/GvXsOer3mpDeezhtXfATR/28By3N3GG6dE6zjBtNHCG7KCH71UU4ExYJ8Jrs7ayCuWLwCcRIZbQiAixhEZEiCU0IkIsoRERYsnC3rnYw14WJ8E7N53g+CrXxS9ZS+jQ4OczUH7UwXFgx88PlWyUEK83C7FXynFw3J8b4+zHkyP9msYY4xWxty32cczb7ZdvQPlkgmPnhmPsgWo38dqUHRxntr25A+UbCR60kzOcHdod6vnMZngtKxU8l7t3X4Pyeh337i2VcJxjnJAp7bt47T2LxwmfRIRYQiMixBIaESGW0IgIsYRGRIglC3vnyhUcd1VICIYbtbHnqHmEOzQMGjiOzQlwJ4NuHV+/19OeqSSPYKVSgXI3hWPVNq7i+nWZhOscJMR7vfcO7k/baOGM1JUqXuNcCXsLowTP6P4+9nAFU3z9zavYa7e8jD1rDnBxHZ/i+91sJnS0yOC1v7lzHcojg+setlvYQzkLsDev1cLzWQQ+iQixhEZEiCU0IkIsoRERYgmNiBBLnDiOcXATIWQh+CQixBIaESGW0IgIsYRGRIglNCJCLKEREWIJjYgQS2hEhFhCIyLEEhoRIZbQiAixhEZEiCU0IkIsoRERYgmNiBBLaESEWEIjIsQSGhEhltCICLGERkSIJTQiQiyhERFiCY2IEEtoRIRYQiMixBIaESGW0IgIsYRGRIglNCJCLKEREWIJjYgQSxbuHv6i+fb3fk+Mf/iD95VO3q+I8UpNd/MetuS4VtlUOjd3Xhbj6XiodP7+N37XGGPMdx/+DyF/9vy+0j1qfiDGlVXd1fvazbwYR+m20hmMD8XYA924v7X19if/fuvk74q/DXuh0o9mnhhfWb+jdDxnS4w7rbHSmTkDMW616krnt9749if//k9/9Q/F36bjjNLvtuSamNhTOv2RXJNOV3cjD+c6bNVPdSf67/zRe0r2WcEnESGW0IgIseTS/Jzb2pI/KR4/+kjpPHkoZf1eoHRWl3fEuNE8UTqjwUiMNzbWE+c1DuRPm61rW0pnZUt+F/Unz5ROvSF/lkxi/XMonZc/A1dWqonzMsaY4Uj+fMvntb7vyZ9Rnp9TOsFE/h6K4rTS6XcnYvz0aVtP6I2f/rNR74k/FXIppT4ey2t22gOlM5zI+xeE+qe3O/eeHKPn/yLhk4gQS2hEhFhCIyLEEhoRIZZcGsfC+/d+LMZr66tKJ50qinE2VVI6y0s1Mf7wnnZQ3L8vz3pu3thROubXf/Kfv37nbSG+fmtDqVZq8rso9vTGNoqzYhyGWaUzHUzlOOjqeZ17e/mCdHIUsstKfTSSm/dmZ6p0oql0UEwGBaVzsCuv8/ZfPtdz+82f/vPJY+k4uX1b36s4ml8Tfb7mzX1CJ4E+Cxt0pKwPlu1FwicRIZbQiAixhEZEiCWXZk/04MN7YvzLX/2a0qlW5X7k8EDHUdVbR1Lg6T2Ak5Kyw/p+4rzyZfm7fTLTh31Hp/JgcTDR84pTUied03Fi+aI8LC2X9R7nPIWSjAtEsXNHJ3Jv4sZKxSzl5Ht0Pb3vCyZlMR509Z71PI4j5+57+pDXMfIA1nP1gayXkvvLVksfsLea8k0Fk5/tx5pPIkIsoRERYgmNiBBLaESEWHJpHAv1M5lN9/DJY6VTLMjNaqfZUzopV25gt3Z0hHa+JDerDz64p3Q+5rQpnQ6dif7ecVzpqHDT2pmRNvIgMU5px0I5Kzfvq+u3EudljDHtrnQk1E/1KWOzLQ9JV6r6IDWIlsS439YJdFG4JsbbV371wrmtrcq5R0Y7PdpdGbXdH2mnjZmL2m519T0PRhUxLuZrSudFwicRIZbQiAixhEZEiCWXZk8UzRWpePT4qdJZW5G/ywv5stJxU3LvkSvqvUexLK8TRMkRi/1pQ4yHs5l+TU/KKjW97yiUZADm/DyNMabblVm0T/cPlM7Xzm01mnORlqOZPojMLcnXzeT1Ae7Th3I/+s53nyidYWtbjGOj1+E8xyfymp1BR+m0mvI7POWj/aa8fyhrNZeVgcnoc/Ei4ZOIEEtoRIRYQiMixBIaESGWXBrHQjFXkQJHb7xnMxmt2+vrzWp9LEssDUdtpfPSbXkQ+PoXbyfO62/9nS+LcXegr3d4tCfGnX5T6URnskxXbkkfaLpzX2lj4Cg4j5OSB6mZkqN0cr7cZHtGR1Pv7Urnyffe1ofPTiCrim5t64jr8zw/lpmvtRV92Or5Mmo9k9FrMppIR0LKFJVOJidls1BXcH2R8ElEiCU0IkIsoRERYsml2RM93zsW4+2rulxvLiUPMUOjAz3n9xWnzSOlM7zXFuOVlYqe0Js/+c/7D94V4vVNndFZrsruBv1xQ+mcdWSGaXGWVzqVity/ZLKfcntScp/luVrf9eWhqDMDJXZjuadAWahzZ7Zm8+rFpXprK8W5sU6pdWbyfg5BWejmXNbwcKj3yumS/P8mUxDI+gLhk4gQS2hEhFhCIyLEEhoRIZY4cRyDIkqEkEXhk4gQS2hEhFhyac6JfuV35bnQOugKkcvJ2KrNba3TH8i4tY/2HiidpYo8o8mCaqR/+s9+Eg/3O39yTcjLZR27VVuR5zvp3Ke3O/R9/ZqVSkW+Vk13jvjtK3/6yb//y8m35B9BBdFRW57FVNJfVTp/9t9kwZDvv62LxPzGb74pxoOR7qL+H37/33/y73/31j8XfwumOs6xfSpj3I4PWkqnftKX15nos8FcTsblZTL6Y/2df/22kn1W8ElEiCU0IkIsoRERYsml2RN5M/l7ftLXcVSukfuIzpkuMBLG8jdzIb+kdDIp+d3hgwIZn/xtrlBGt6tfM4rlb/L1dV0wslye2zel9b6pmJM6pYKOrxNz82QcmevpfKJUUQa9BX2t0+/JvKRSWef1vHRHflRq6zcunNv6moy/a7d0PNvpROZ+NeqHSmfYl5+L5bK+n8Ul+VmJHcbOEfILBY2IEEtoRIRYQiMixJJL41hw5s7QBmAjOr9vPpuOlE46J78Xinnd+j0M5f8XXVAPpDbX4rLd1geC4Vi+Zhzojbkzl4Q3HeqQxdZULkIQgImd8z14kdx0Hx/WzTzjtpxb91TP7fhAHnpWK9rp4biyIq2XunjzHkfycLVS1E6Sq9vSAROB+iLjnnRQLJV0dVkvJe9Jf3BxddbPGj6JCLGERkSIJTQiQiy5NHsiE8rfsVGgC1KkYnnQNhnoPZGJ5MYpD/ZE3Z4sfuGOkr9LJn35t3xqRemUShUxXlu+onSWS7J7mwuKisx/p7keeH/n6MizSrP3UAd5TrtyT9GrnymdUU+u/Su3t5XOONgV4+PTUz2hc//bLJSH0tWSPpzdXJFBxy9ff1Xp9Nty7zgLJkonjGXQ8TTQh90vEj6JCLGERkSIJTQiQiyhERFiyaVxLGScucjqjM7SrBSkY2Ew0hHJ8Vw3CSfU1wkH8m1HJrlWy9GezPq8tqM3yKvlm2Kc93VLRyeUh4Sepw8941DOY9jTB7vnefSedJDUT/R3YmomHSujls6oHfdkxdZCQW/MU548XHXjiw9bCxl5YBtNQFT+3K3ZWK0onVlV3s9hf6B04lg6T3I5XcH1RcInESGW0IgIsYRGRIgll2ZPlJvLNs2BrM6MJ6fr5nTlnWAuy3Q21t8TvpH7k0mQHLA47srX9OOa0il4c4erY12lp9+T+7eUq3VmU/n7vzO4+Pbk4pfEeMnvKZ3pQFZEcgOwHo7snLFc0vumclHu4Qaji4M8J2MZTVo/e650mp48kL26rQ+pSxU5l8jRB9CTuXWaorldTZyqNXwSEWIJjYgQS2hEhFhCIyLEkkvjWIjn0ksd0H6+3ZCZm+m8znKM5kpcjQc66td357JMQ90e/mPy6TUpmOqo8OlQHu656bLSSTtyY57xQYZmVh5QZr2Lo5FfuvJ1Mb62pr8TT/fluJ3tK52jfalUKemPxUpVRq87zfaFc3Nn0pHihnpuwVzk/rCvr5nKyGzf6VSXEe515qL7h/qem9eTZmoPn0SEWEIjIsQSGhEhllyaPVEwF1c4BjGh3b48TFyu6iBOby5wtQ+SQwtz7VHiCw5bt9buiHHKqSqdTlP+/p/4+npOPBcYG4OJzZVA/rQmhs0juYfa2bmldNKbci6es6t0ej2ZIvv8SB8EvzqV77tY1Bm+51nbkvu5ckV/1ILhXJUkR+9l+hOZiRvHOug4V6yIcTqt5/8i4ZOIEEtoRIRYQiMixBIaESGWOPGn7V4JIRfCJxEhltCICLHk0pwT/aN//BtinMrozgSFooyRKi3rMxvjz7UnXFlVKo8/krFivYGOx/rP//bbxhhj/sW/+VdCnsvpuLjpRJ5dZDO6JWI6JeP1JmNd4TWYO686q7eVzn/849//5N9/8C//u/jbaKxbYe7clkVTnuy/q3Q++PCvxPi1L+p1fe1L8kyusKxjG//B69/55N9/8t1fl/oZHXO4XNoUY8/RO4tpcCzGRdA+tLYkC8X4rtb5Qu2fKNlnBZ9EhFhCIyLEEhoRIZZcmj3RN77+TTH2U7roop+WslxB//Y1rtyfFMp6DzMey7e9/+xI6Xzyt49kW/jikm7nFs3k9YpFvd+ZhTLu7+REd2dIp+a6wuXBnu8cB3uy6GJ3LgbOGGMOTr4vxqNYt7nvD2WeVkenHBnjXhfDerMBlH7KBx++L8YhyNmqlmVXiNWajsfzXRlUWcjqwozxptxvrtV+ts8GPokIsYRGRIglNCJCLKEREWLJpXEs3Lghk988XydfRXNhfl5aT38+ZyuV1wla07HsaLC3+zhxXq0zediHNsi9vkwmKy3pSqQb67IEZ62qDx+jWBZZcczFVUbbXbm5D2N92NppS2eDX9BdFYZT+f8d1/V7DCNZnXQ2u3huniffy+Ghdt606vKQO5jo113Ky3vec7TXIw5kkuX84bcxxnyxkjhVa/gkIsQSGhEhltCICLHk0uyJSkvyoM3VjQnMNJR7j9jVv8vnZZ6rDz4HfdmB7uhwN3FevZ48iCwU9WFfuyX3JidHx0pnPm1rtabb3E/HsnjJ3kHyvIwxptWRB6fjQB+2Tv0DMfYDvV8LZvJ1Z5Fe/NOGvHboXtzFb+em7Cg4AvudyUB+h7uePmB3fbluYajv+dlcIcnhUN9z82rSTO3hk4gQS2hEhFhCIyLEEhoRIZZcGsdCbOSmMop0luMslIdzUaw3q0Eso6w9nYBpxoO2HA91RPXH+I68nhPpw8rpuCnGx0dtpTMYyP9vdU1HU6c8eWjYaFwcKd3qS6fBSeNDpeNkT8XYL+go9Ft3d8T49S+8onSCuCPGswgs7Dnmb9/6pm4l6UYyCh9FaEezuQNkT9/zlCcP1GegSuqLhE8iQiyhERFiCY2IEEsuzZ5o0J/bewDznkzlvmIa6GDE4UgeAjpp/Rt6PG6LcT6X/Bu6WJQTOTjQ+46DfRlcOQ3BSbGRrxFF+kCwsiw7KcziodI5T6P1VIxTeb3fyc7FuQZGVza6slUT461rukPf2VwXu3J1Wemcp9WVe6hqdUPpbNRklZ4o1PvgxqkMDg4DPf/cXCUh39XdQl4kfBIRYgmNiBBLaESEWEIjIsSSS+NYcObK/zqO3nin5spoRbH+DohiGentRdqxsFItiPHn7l5PnNdqTR4A9nv6YHa5IjeymYwu0zUJ5VL7vn5/riMPMNud5FJexhjTHcq/f+WNl5TOKJZRz/2xPiyu1GTJqRGI9A7n1jGdLiqd86RSc2uSziudQl6uE3LvTEZSZ9jXc5tvQTkN9D1/kfBJRIglNCJCLKEREWLJpdkTzWbyEM0BWavGCed09G/fKJIHjj4IUq2U5O/1dEofBH7M65+7Lcav3X0ZzEvumxotfUj6zrvykLY70HuizFxhosHo4gDUjSsygPPzb95SOvefyCzbrNGll+/M7QnLqwWlUxzJ++NlQPbo+dfJyWsMxxOl02zKA9lSUc8tP1cqGjV2nHTn9n29iw+pP2v4JCLEEhoRIZbQiAixhEZEiCVOjHZqhJCF4ZOIEEtoRIRYcmnOif78e7INfONMV/MsLaXFeKWqOz50u7IAyFljX+ncu/f/5Gv/xVt6Pm/95AzjD/7w14R8qVxTusOx/EXsZypK563/83/FOJXV8XXVqizm8Wf/+38qndH+T2PHvvBNGSv3+pevzqub3cMfyNfY0N0obt9+TYyPzp4rnWxRxvUtr+rYuT/+1l9/8u8//F9/W/yt3RqZeTJGrmV1Wa9JpSLH5ZKOwXMDKaufNpXOP/21/6pknxV8EhFiCY2IEEtoRIRYcmn2RH5Oxj/lizq7xPVl/NXJqe6+8PDBu2J8erqrdEYD+Zt5Y1XvEz7m0f0fi3GxpAt0NNoyVuvKts7ruX1btpvvgdi53lwRyWz+4tOHK1tyD+H7uohHOjO3XwMxiXv7suDJD+/pYiy1udold17T7/E8jVP5XmZhWulk8/I7/PC5vp/7u3Jv/OUvvaF0rm9dE+Ozs4s7VnzW8ElEiCU0IkIsoRERYgmNiBBLLo1jYb7oRqOjN5kmlAd201FbqZzW98T4rH6gdKJQFutwZnpD/jH1E7mxjUHl0l5LVmLtlSpK5+4vfUWM37//VOn0+9LhcXVnRemIa74qK4jOXH3ImOvMtXR0tGMhmMpExvKS/lisr8qD0Xzu4kIljUZbjFtnOlEun5LrNuh3lU4YyM/Btau6Ouv6XOvObpuOBUJ+oaAREWIJjYgQSy7Nnqh+9kSMz5qgjX04V+wi1EGNUSR/Z3uO3u+4c4Uhc1nd+v1j7tySQZ3pjA6AzM7JalVdcGN7W+5vdp8/UzqzI7lXC6e6UOF5ep26GDsZre+58r2223rfNJrK918uVpTO2prci8Thxd3orqzJw+X64QOl82RX7gsLOX0ftrbk4XY41QVPjg7lWrrexUVUPmv4JCLEEhoRIZbQiAixhEZEiCWXxrHQG8tDUTejN5CptIxIDkdAx5cVT3NZ/T1RmOvaUCrq9vAf88bnZNbn7oHu1DAcS+dFJgVec27TfOP6ltJpD2RF0B+9v6d0ztPpSifByrqOlL5+XUY4d0FXhY/2ZMR1MNFVY0d9udZ+5uLv3/JSRYxXa2tK59njR2Icg24OpTtVMW43dVeO/txh9/qaXtsXCZ9EhFhCIyLEEhoRIZZcmj3RYCwDPX1Xt7Gfl2RzSsXcuCX3N/FEV+cJBvKQNpwGSudjxiN5ANoBv8k7bfmbfObpQ8MPH94TYzerD23vvCIDSlfWdXeG89y6Kfc7XmasdCqrci6Zgl60a9fkXqTeaiud2JNr5ujbIxj25Fy2N7eVztG6DBRtNHQXjEFPrm0U6CBVJ5LvcW1l9eLJfcbwSUSIJTQiQiyhERFiCY2IEEsujWMh9uRG1AGOhelEHvi5rp5+ba6klZ/PKJ2jjjxwPH5+qHQ+5vCZPASehfpA0HHkIbAb6yjikyP5Gl5ROw1iXx6W3n75RuK8jDEmm5XvbTjWm+52U77XslNVOtubO2K8AZwAJ2cyUro71K91niiSa1Kp6Ne9e1ceZNfruuRzqSQPsuO5KH1jjPFduW7ez/jRwCcRIZbQiAixhEZEiCWXZk9UrshDwAI4SR215e/w5tGp0hk05GGoE+jqNt26vE6vl5xBGs1V96nM9/owxrgp2eJl46pucZKZ+/8OwMHie+/IEsiN1jWlY37np/88OZLBsLmS3otFc/vI/X297wimcq03r+4oneVluadxPmXjMZ3KvUyzCSoRZeS63dy5rnQiIzOc2y19r2aBfK1WS7/Wi4RPIkIsoRERYgmNiBBLaESEWOLEcXxxExxCyIXwSUSIJTQiQiyhERFiCY2IEEtoRIRYQiMixBIaESGW0IgIsYRGRIgl/x/2D0nYR5kiggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x400 with 33 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, ch, img_size, slice_width, slices, slice_flat, batch_size, label = img_data(\n",
    "    Im_tr_loader, 2)\n",
    "img_plot(img, slice_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the VIT Model : Embedding &rarr; Transformer Encoder &rarr; MLP_Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, slice_input_size, slice_embed_size, img_slices, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_to_embed = nn.Linear(slice_input_size, slice_embed_size)\n",
    "        self.cls_to_embed = nn.Parameter(torch.rand(1, slice_embed_size))\n",
    "        self.pos_to_embed = nn.Parameter(\n",
    "            torch.rand(1, img_slices + 1, slice_embed_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, flattened_imgs):\n",
    "        # the input is a batch of images each has been sliced to patches and each slice\n",
    "        # has been flattened. i.e. the shape of the input is:\n",
    "        # [no. of images ,no. of slices per image, size of the flattened image slice]\n",
    "        #\n",
    "        img_embedding = self.img_to_embed(flattened_imgs)\n",
    "\n",
    "        cls_embedding = self.cls_to_embed.repeat(img_embedding.size(0), 1, 1)\n",
    "        img_embedding = torch.concat([cls_embedding, img_embedding], dim=1)\n",
    "\n",
    "        position_embedding = self.pos_to_embed.repeat(\n",
    "            img_embedding.size(0), 1, 1)\n",
    "\n",
    "        img_and_pos_embedding = img_embedding + position_embedding\n",
    "        embedding = self.dropout(img_and_pos_embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:   torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 17, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ImageEmbedding (ImageEmbedding)          [16, 16, 192]        [16, 17, 192]        3,456                True\n",
       "├─Linear (img_to_embed)                  [16, 16, 192]        [16, 16, 192]        37,056               True\n",
       "├─Dropout (dropout)                      [16, 17, 192]        [16, 17, 192]        --                   --\n",
       "========================================================================================================================\n",
       "Total params: 40,512\n",
       "Trainable params: 40,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.59\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 0.15\n",
       "Estimated Total Size (MB): 0.74\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, tr_img_t, _ = next(iter(Im_tr_loader))\n",
    "\n",
    "slice_embed = slice_flat\n",
    "\n",
    "embedding_layer = ImageEmbedding(slice_input_size=slice_flat,\n",
    "                                 slice_embed_size=slice_embed,\n",
    "                                 img_slices=slices,\n",
    "                                 dropout_ratio=0.2)\n",
    "\n",
    "embedding_output = embedding_layer(tr_img_t)\n",
    "\n",
    "print(\"Input Shape:  \", tr_img_t.size())\n",
    "print(\"Output Shape: \", embedding_output.size())\n",
    "# print(embedding_output)\n",
    "\n",
    "summary(model=embedding_layer,\n",
    "        input_size=tr_img_t.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_attention = nn.LayerNorm(size)\n",
    "        self.query = nn.Linear(size, size)\n",
    "        self.key = nn.Linear(size, size)\n",
    "        self.value = nn.Linear(size, size)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.norm_feed_forward = nn.LayerNorm(size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(size, 4 * size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * size, size),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        attn_input = self.norm_attention(input_tensor)\n",
    "        query = self.query(attn_input)\n",
    "        key = self.key(attn_input)\n",
    "        value = self.value(attn_input)\n",
    "        attn_output, _ = self.attention(query=query, key=key, value=value)\n",
    "\n",
    "        attn_plus_norm = input_tensor + attn_output\n",
    "\n",
    "        mlp_input = self.norm_feed_forward(attn_plus_norm)\n",
    "        output = attn_plus_norm + self.feed_forward(mlp_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  torch.Size([16, 17, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Encoder (Encoder)                        [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "├─LayerNorm (norm_attention)             [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "├─Linear (query)                         [16, 17, 192]        [16, 17, 192]        37,056               True\n",
       "├─Linear (key)                           [16, 17, 192]        [16, 17, 192]        37,056               True\n",
       "├─Linear (value)                         [16, 17, 192]        [16, 17, 192]        37,056               True\n",
       "├─MultiheadAttention (attention)         --                   [16, 17, 192]        148,224              True\n",
       "├─LayerNorm (norm_feed_forward)          [16, 17, 192]        [16, 17, 192]        384                  True\n",
       "├─Sequential (feed_forward)              [16, 17, 192]        [16, 17, 192]        --                   True\n",
       "│    └─Linear (0)                        [16, 17, 192]        [16, 17, 768]        148,224              True\n",
       "│    └─Dropout (1)                       [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "│    └─GELU (2)                          [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "│    └─Linear (3)                        [16, 17, 768]        [16, 17, 192]        147,648              True\n",
       "│    └─Dropout (4)                       [16, 17, 192]        [16, 17, 192]        --                   --\n",
       "========================================================================================================================\n",
       "Total params: 556,032\n",
       "Trainable params: 556,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.52\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.21\n",
       "Forward/backward pass size (MB): 4.18\n",
       "Params size (MB): 1.63\n",
       "Estimated Total Size (MB): 6.02\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = Encoder(size=slice_flat, num_heads=12, dropout=0.1)\n",
    "encoder_output = encoder_layer(embedding_output)\n",
    "\n",
    "print(\"Output Shape: \", encoder_output.size())\n",
    "\n",
    "summary(model=encoder_layer,\n",
    "        input_size=embedding_output.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 slice_input_size,\n",
    "                 slice_embed_size,\n",
    "                 img_slices,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_encoders=1,\n",
    "                 emb_dropout=0.1,\n",
    "                 enc_dropout=0.1,\n",
    "                 lr=1e-4, min_lr=4e-5,\n",
    "                 weight_decay=0.1,\n",
    "                 epochs=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ImageEmbedding(\n",
    "            slice_input_size=slice_input_size,\n",
    "            slice_embed_size=slice_embed_size,\n",
    "            img_slices=img_slices,\n",
    "            dropout_ratio=emb_dropout)\n",
    "        \n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(slice_embed_size, num_heads, dropout=enc_dropout) for _ in range(num_encoders)],)\n",
    "        \n",
    "        self.mlp_head = nn.Linear(slice_embed_size, num_classes)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.min_lr = min_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, flattened_img):\n",
    "        emb = self.embedding(flattened_img)\n",
    "        attn = self.encoders(emb)\n",
    "        output = torch.round(self.sigmoid(self.mlp_head(attn[:, 0, :])))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([16, 16, 192])\n",
      "Output Shape:  torch.Size([16, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "VIT (VIT)                                          [16, 16, 192]        [16, 1]              --                   True\n",
       "├─ImageEmbedding (embedding)                       [16, 16, 192]        [16, 17, 768]        13,824               True\n",
       "│    └─Linear (img_to_embed)                       [16, 16, 192]        [16, 16, 768]        148,224              True\n",
       "│    └─Dropout (dropout)                           [16, 17, 768]        [16, 17, 768]        --                   --\n",
       "├─Sequential (encoders)                            [16, 17, 768]        [16, 17, 768]        --                   True\n",
       "│    └─Encoder (0)                                 [16, 17, 768]        [16, 17, 768]        --                   True\n",
       "│    │    └─LayerNorm (norm_attention)             [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─Linear (query)                         [16, 17, 768]        [16, 17, 768]        590,592              True\n",
       "│    │    └─Linear (key)                           [16, 17, 768]        [16, 17, 768]        590,592              True\n",
       "│    │    └─Linear (value)                         [16, 17, 768]        [16, 17, 768]        590,592              True\n",
       "│    │    └─MultiheadAttention (attention)         --                   [16, 17, 768]        2,362,368            True\n",
       "│    │    └─LayerNorm (norm_feed_forward)          [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─Sequential (feed_forward)              [16, 17, 768]        [16, 17, 768]        4,722,432            True\n",
       "│    └─Encoder (1)                                 [16, 17, 768]        [16, 17, 768]        --                   True\n",
       "│    │    └─LayerNorm (norm_attention)             [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─Linear (query)                         [16, 17, 768]        [16, 17, 768]        590,592              True\n",
       "│    │    └─Linear (key)                           [16, 17, 768]        [16, 17, 768]        590,592              True\n",
       "│    │    └─Linear (value)                         [16, 17, 768]        [16, 17, 768]        590,592              True\n",
       "│    │    └─MultiheadAttention (attention)         --                   [16, 17, 768]        2,362,368            True\n",
       "│    │    └─LayerNorm (norm_feed_forward)          [16, 17, 768]        [16, 17, 768]        1,536                True\n",
       "│    │    └─Sequential (feed_forward)              [16, 17, 768]        [16, 17, 768]        4,722,432            True\n",
       "├─Linear (mlp_head)                                [16, 768]            [16, 1]              769                  True\n",
       "├─Sigmoid (sigmoid)                                [16, 1]              [16, 1]              --                   --\n",
       "==================================================================================================================================\n",
       "Total params: 17,882,113\n",
       "Trainable params: 17,882,113\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 210.30\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 35.00\n",
       "Params size (MB): 52.57\n",
       "Estimated Total Size (MB): 87.77\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_embed = slice_flat*4\n",
    "\n",
    "vit_model = VIT(slice_input_size=slice_flat,\n",
    "                slice_embed_size=slice_embed,\n",
    "                img_slices=slices,\n",
    "                num_classes=1,\n",
    "                num_heads=12,\n",
    "                num_encoders=2,\n",
    "                emb_dropout=0.1,\n",
    "                enc_dropout=0.1,\n",
    "                lr=1e-4, min_lr=4e-5,\n",
    "                weight_decay=0.1,\n",
    "                epochs=200)\n",
    "\n",
    "vit_output = vit_model(tr_img_t)\n",
    "\n",
    "print(\"Input Shape: \", tr_img_t.size())\n",
    "print(\"Output Shape: \", vit_output.size())\n",
    "\n",
    "summary(model=vit_model,\n",
    "        input_size=tr_img_t.size(),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_epochs = 30         # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "vit_optimizer = Adam(vit_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "criterion = CrossEntropyLoss() # returns the mean loss for the batch\n",
    "scheduler = lr_scheduler.LinearLR(vit_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: 100%|██████████| 6250/6250 [01:17<00:00, 80.38it/s, acc=50, loss=22.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/30]: 100%|██████████| 6250/6250 [01:23<00:00, 75.03it/s, acc=49.9, loss=22.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/30]: 100%|██████████| 6250/6250 [01:23<00:00, 74.90it/s, acc=50, loss=22.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/30]: 100%|██████████| 6250/6250 [01:22<00:00, 75.95it/s, acc=49.9, loss=22.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/30]: 100%|██████████| 6250/6250 [01:19<00:00, 78.57it/s, acc=50, loss=22.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/30]: 100%|██████████| 6250/6250 [01:23<00:00, 75.00it/s, acc=49.9, loss=22.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/30]: 100%|██████████| 6250/6250 [01:25<00:00, 73.16it/s, acc=49.9, loss=22.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/30]: 100%|██████████| 6250/6250 [01:24<00:00, 74.31it/s, acc=50, loss=22.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 22.18 | Validation accuracy: 50.00\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/30]: 100%|█████████▉| 6240/6250 [01:28<00:00, 70.44it/s, acc=49.9, loss=22.4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     vit_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m     loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvit_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_postfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavg_tr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavg_tr_accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m tr_loss_history\u001b[38;5;241m.\u001b[39mappend(avg_tr_loss)\n\u001b[1;32m     47\u001b[0m tr_accuracy_history\u001b[38;5;241m.\u001b[39mappend(avg_tr_accuracy)\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:1431\u001b[0m, in \u001b[0;36mtqdm.set_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostfix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m postfix[key]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1429\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m postfix\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[1;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[1;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[0;32m--> 453\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/tqdm/utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/pytVenv/lib/python3.11/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vit_model.to(device)\n",
    "\n",
    "for epoch in range(vit_epochs):\n",
    "\n",
    "    # Training Loop\n",
    "    vit_model.train()\n",
    "\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_tr_accuracy = 0\n",
    "    accumulated_val_loss = 0\n",
    "    accumulated_val_accuracy = 0\n",
    "\n",
    "    tr_loss_history = []\n",
    "    tr_accuracy_history = []\n",
    "    val_loss_history = []\n",
    "    val_accuracy_history = []\n",
    "\n",
    "    loop = tqdm(enumerate(Im_tr_loader), total=len(Im_tr_loader))\n",
    "    for batch, (_, inputs, targets) in loop:\n",
    "\n",
    "        # Put inputs and labels in device cuda\n",
    "        tr_images = inputs.to(device)\n",
    "        tr_labels = targets.to(device)\n",
    "\n",
    "        # Forward Path\n",
    "        tr_outputs = vit_model(tr_images).squeeze(1)\n",
    "\n",
    "        loss = criterion(tr_outputs,tr_labels)\n",
    "\n",
    "        accumulated_tr_loss += loss.item()\n",
    "        accumulated_tr_accuracy += (tr_labels == tr_outputs).sum().item()/tr_labels.size(0)\n",
    "        \n",
    "        avg_tr_loss = accumulated_tr_loss / (batch+1)\n",
    "        avg_tr_accuracy = 100*accumulated_tr_accuracy / (batch+1)\n",
    "\n",
    "        # Backward path (Gradient)\n",
    "        vit_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer(Adam) Step\n",
    "        vit_optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{vit_epochs}]\")\n",
    "        loop.set_postfix(loss=avg_tr_loss, acc=avg_tr_accuracy)\n",
    "    \n",
    "    tr_loss_history.append(avg_tr_loss)\n",
    "    tr_accuracy_history.append(avg_tr_accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# Validation Loop\n",
    "    vit_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch, (_, inputs, targets) in enumerate(Im_val_loader):\n",
    "\n",
    "            # Put inputs and labels in device cuda\n",
    "            val_images = inputs.to(device)\n",
    "            val_labels = targets.to(device)\n",
    "\n",
    "            val_outputs = vit_model(val_images).squeeze(1)\n",
    "\n",
    "            loss = criterion(val_outputs,val_labels)\n",
    "\n",
    "            accumulated_val_loss += loss.item()\n",
    "            accumulated_val_accuracy += (val_labels == val_outputs).sum().item()/val_labels.size(0)\n",
    "\n",
    "            avg_val_loss = accumulated_val_loss / (batch+1)\n",
    "            avg_val_accuracy = 100*accumulated_val_accuracy / (batch+1)\n",
    "\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_accuracy_history.append(avg_val_accuracy)\n",
    "\n",
    "        print(f'Validation loss: {avg_val_loss:.2f} | Validation accuracy: {avg_val_accuracy:.2f}')\n",
    "        print(\"-------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "axs[0].plot(range(len(tr_loss_history )), tr_loss_history , label='Train Loss')\n",
    "axs[1].plot(range(len(tr_accuracy_history)), tr_accuracy_history, label='Validation Loss')\n",
    "axs[0].plot(range(len(val_loss_history)), val_loss_history, label='Train Accuracy')\n",
    "axs[1].plot(range(len(val_accuracy_history)), val_accuracy_history, label='Validation Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
